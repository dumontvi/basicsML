{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsVmcyPpKD0Y",
        "colab_type": "code",
        "outputId": "68e7566b-812c-48dc-c9de-ecb688febb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhR-8CiMKh72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "def loadData():\n",
        "    with np.load(\"notMNIST.npz\") as data:\n",
        "        Data, Target = data[\"images\"], data[\"labels\"]\n",
        "        np.random.seed(521)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data = Data[randIndx] / 255.0\n",
        "        Target = Target[randIndx]\n",
        "        trainData, trainTarget = Data[:15000], Target[:15000]\n",
        "        validData, validTarget = Data[15000:16000], Target[15000:16000]\n",
        "        testData, testTarget = Data[16000:], Target[16000:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n",
        "\n",
        "# Implementation of a neural network using only Numpy - trained using gradient descent with momentum\n",
        "def convertOneHot(trainTarget, validTarget, testTarget):\n",
        "    newtrain = np.zeros((trainTarget.shape[0], 10))\n",
        "    newvalid = np.zeros((validTarget.shape[0], 10))\n",
        "    newtest = np.zeros((testTarget.shape[0], 10))\n",
        "\n",
        "    for item in range(0, trainTarget.shape[0]):\n",
        "        newtrain[item][trainTarget[item]] = 1\n",
        "    for item in range(0, validTarget.shape[0]):\n",
        "        newvalid[item][validTarget[item]] = 1\n",
        "    for item in range(0, testTarget.shape[0]):\n",
        "        newtest[item][testTarget[item]] = 1\n",
        "    return newtrain, newvalid, newtest\n",
        "\n",
        "\n",
        "def shuffle(trainData, trainTarget):\n",
        "    np.random.seed(421)\n",
        "    randIndx = np.arange(len(trainData))\n",
        "    target = trainTarget\n",
        "    np.random.shuffle(randIndx)\n",
        "    data, target = trainData[randIndx], target[randIndx]\n",
        "    return data, target\n",
        "\n",
        "\n",
        "# def relu(x):\n",
        "#     # TODO\n",
        "\n",
        "# def softmax(x):\n",
        "#     # TODO\n",
        "\n",
        "\n",
        "# def computeLayer(X, W, b):\n",
        "#     # TODO\n",
        "\n",
        "# def CE(target, prediction):\n",
        "\n",
        "#     # TODO\n",
        "\n",
        "# def gradCE(target, prediction):\n",
        "\n",
        "#     # TODO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anFdDL1OLiui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX, validX, testX, trainY, validY, testY = loadData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yb4164dLvkb",
        "colab_type": "code",
        "outputId": "60ff67b0-8f84-4c46-9378-e211d15d1074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(0,10): # Visualize the data\n",
        "    plt.figure()\n",
        "    plt.imshow(trainX[i], cmap='Greys')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method NpzFile.__del__ of <numpy.lib.npyio.NpzFile object at 0x7f3843f972b0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 230, in __del__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 221, in close\n",
            "    if self.zip is not None:\n",
            "AttributeError: 'NpzFile' object has no attribute 'zip'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3rmDfDsXZt0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhB1I4v8BUJcuAI6cvDcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+H9OrCRTeqojHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnGDYiZxg2Imc+H/ceMoqXr3OGwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMnUlEQVR4nO3dX4gd9RnG8efJ2mCwQZL+icEE26pU\npdK0LKFgLKm2RXMTCxqSixJBur2o0kIvKhZRL4pS+8deSGGtYiytpVDFXEgxDQUplOAqqa7aNlZS\nTFiTBoVaENts3l7sKGvcmTk5M+fMJO/3A4dzzvzmz8tkn8w585s5P0eEAJz5lnVdAIDxIOxAEoQd\nSIKwA0kQdiCJs8a5Mduc+sfYnHvuuaVt69atq1z27LPPbrucsTh48KCOHTvmpdoahd32NZJ+JmlC\n0i8i4p4m6wMWs5f8m31PXbfxpk2bStvuvffeymUvvfTSyvY6dbVVtS9bNvwH7snJyfL1DrtS2xOS\n7pd0raTLJO2wfdmw6wMwWk2+s2+U9EpEvBoR/5X0G0lb2ykLQNuahP18Sa8ten+omPY+tqdsz9ie\nabAtAA2N/ARdRExLmpY4QQd0qcmR/bCk9YveryumAeihJmF/RtLFtj9pe7mk7ZJ2t1MWgLa5yV1v\ntrdIuk8LXW8PRcQPaubnYzx6YWJiorJ9+/btle133HFHZftFF11U2V7Vrdik227jxo2amZlpv589\nIp6U9GSTdQAYDy6XBZIg7EAShB1IgrADSRB2IAnCDiTRqJ/9lDdGP/tI1N0K2qUuf7246lbRJn3Z\ndeuWpG3btlW2V/XTX3LJJZXLVpmcnCztZ+fIDiRB2IEkCDuQBGEHkiDsQBKEHUiCrjekVNddWde1\nVpebEydODL39uttrb7/99tK2G264QbOzs3S9AZkRdiAJwg4kQdiBJAg7kARhB5Ig7EAS9LOfAZqM\n+jlqdf3NZ6q6n6qu2i9NMxkR9LMDmRF2IAnCDiRB2IEkCDuQBGEHkiDsQBL0s58G6u69PnToUGnb\nypUrK5edn5+vbK/7+1ixYkVl++bNm0vb9u3bV7ls3fUDZ2offtM++rJ+9kZDNts+KOktSfOSjkfE\nZJP1ARidRmEvfCkijrWwHgAjxHd2IImmYQ9JT9l+1vbUUjPYnrI9Y3um4bYANND0Y/ymiDhs++OS\n9tj+a0Q8vXiGiJiWNC1xgg7oUqMje0QcLp6PSnpc0sY2igLQvqHDbvsc2yvffS3pq5Jm2yoMQLua\nfIxfI+nxog/4LEm/jojft1IVTsmqVatK2+r6wUc9dHHd9vFBddc+DGvosEfEq5I+22ItAEaIrjcg\nCcIOJEHYgSQIO5AEYQeSaONGGHSsyW3KTbve6pypt6GejjiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig\n7EAS9LOfAep+anpUyw6iz8NJZ8O/BJAEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig\n7EAShB1IgrADSRB2IAnCDiRB2IEkasNu+yHbR23PLpq22vYe2weK5/IBwgH0wiBH9oclXXPStFsl\n7Y2IiyXtLd4D6LHasEfE05LeOGnyVkm7ite7JF3Xcl0AWjbsb9CtiYi54vXrktaUzWh7StLUkNsB\n0JLGPzgZEWG7dPS/iJiWNC1JVfMBGK1hz8Yfsb1Wkorno+2VBGAUhg37bkk7i9c7JT3RTjkARmWQ\nrrdHJf1Z0qdtH7J9k6R7JH3F9gFJXy7eA+ix2u/sEbGjpOnqlmsBMEJcQQckQdiBJAg7kARhB5Ig\n7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS\nIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IYpDx2R+yfdT27KJpd9o+bHt/8dgy\n2jIBNDXIkf1hSdcsMf2nEbGheDzZblkA2lYb9oh4WtIbY6gFwAg1+c5+s+3ni4/5q8pmsj1le8b2\nTINtAWho2LD/XNKFkjZImpP047IZI2I6IiYjYnLIbQFowVBhj4gjETEfESckPSBpY7tlAWjbUGG3\nvXbR269Jmi2bF0A/nFU3g+1HJW2W9FHbhyTdIWmz7Q2SQtJBSd8cYY0AWlAb9ojYscTkB0dQC4AR\n4go6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKw\nA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkqgdxRX9FxEj\nW7bJuiXpxIkTjZZHe2qP7LbX2/6j7Zdsv2j728X01bb32D5QPK8afbkAhjXIx/jjkr4bEZdJ+oKk\nb9m+TNKtkvZGxMWS9hbvAfRUbdgjYi4initevyXpZUnnS9oqaVcx2y5J142qSADNndJ3dtufkPQ5\nSfskrYmIuaLpdUlrSpaZkjQ1fIkA2jDw2XjbH5b0O0nfiYh/L26LhbM4S57JiYjpiJiMiMlGlQJo\nZKCw2/6QFoL+q4h4rJh8xPbaon2tpKOjKRFAGwY5G29JD0p6OSJ+sqhpt6Sdxeudkp5ovzwMwnbp\no8mygzzqLFu2rPSB4Qz77zHId/YrJH1d0gu29xfTbpN0j6Tf2r5J0j8lbWtQP4ARqw17RPxJUtl/\nGVe3Ww6AUeGzFJAEYQeSIOxAEoQdSIKwA0lwi+sZ4M033yxtO378eOWy8/Pzle11t7iuWLGisv3t\nt9+ubM+qqk+87hqEun+z0vUOtRSA0w5hB5Ig7EAShB1IgrADSRB2IAnCDiThpj8VfEobs8e3sUT6\nfG941p+SnpiYqGyv2i91mbzgggtK2+bm5vTOO+8s2Ynf378SAK0i7EAShB1IgrADSRB2IAnCDiRB\n2IEkuJ/9DJC1L3uU6q5dqPuN9rp7zpcvX17adtddd1Uue8stt5S2XXnllaVtHNmBJAg7kARhB5Ig\n7EAShB1IgrADSRB2IInafnbb6yU9ImmNpJA0HRE/s32npG9I+lcx620R8eSoCkW5QcZJ78o4fy/h\nZFX7pW6f1V27ULf8jTfeWNl+9913l7add955lctW7dOq6wMGuajmuKTvRsRztldKetb2nqLtpxHx\nowHWAaBjg4zPPidprnj9lu2XJZ0/6sIAtOuUvrPb/oSkz0naV0y62fbzth+yvapkmSnbM7ZnGlUK\noJGBw277w5J+J+k7EfFvST+XdKGkDVo48v94qeUiYjoiJiNisoV6AQxpoLDb/pAWgv6riHhMkiLi\nSETMR8QJSQ9I2ji6MgE0VRt2L5x2fFDSyxHxk0XT1y6a7WuSZtsvD0BbBjkbf4Wkr0t6wfb+Ytpt\nknbY3qCF7riDkr45kgpRq8vurdNVXdfaVVddVdl+3333VbZffvnlp1zTu5p2+5UZ5Gz8nyQttXb6\n1IHTCFfQAUkQdiAJwg4kQdiBJAg7kARhB5Lgp6TRW3X9yXXXF6xevbq07f77769c9vrrr69sbzIk\nc51RDcHNkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkvA474W2/S9J/1w06aOSjo2tgFPT19r6WpdE\nbcNqs7YLIuJjSzWMNewf2Lg909ffputrbX2tS6K2YY2rNj7GA0kQdiCJrsM+3fH2q/S1tr7WJVHb\nsMZSW6ff2QGMT9dHdgBjQtiBJDoJu+1rbP/N9iu2b+2ihjK2D9p+wfb+rsenK8bQO2p7dtG01bb3\n2D5QPC85xl5Htd1p+3Cx7/bb3tJRbett/9H2S7ZftP3tYnqn+66irrHst7F/Z7c9Ienvkr4i6ZCk\nZyTtiIiXxlpICdsHJU1GROcXYNj+oqT/SHokIj5TTPuhpDci4p7iP8pVEfG9ntR2p6T/dD2MdzFa\n0drFw4xLuk7Sjepw31XUtU1j2G9dHNk3SnolIl6NiP9K+o2krR3U0XsR8bSkN06avFXSruL1Li38\nsYxdSW29EBFzEfFc8fotSe8OM97pvquoayy6CPv5kl5b9P6Q+jXee0h6yvaztqe6LmYJayJirnj9\nuqQ1XRazhNphvMfppGHGe7Pvhhn+vClO0H3Qpoj4vKRrJX2r+LjaS7HwHaxPfacDDeM9LksMM/6e\nLvfdsMOfN9VF2A9LWr/o/bpiWi9ExOHi+aikx9W/oaiPvDuCbvF8tON63tOnYbyXGmZcPdh3XQ5/\n3kXYn5F0se1P2l4uabuk3R3U8QG2zylOnMj2OZK+qv4NRb1b0s7i9U5JT3RYy/v0ZRjvsmHG1fG+\n63z484gY+0PSFi2ckf+HpO93UUNJXZ+S9Jfi8WLXtUl6VAsf6/6nhXMbN0n6iKS9kg5I+oOk1T2q\n7ZeSXpD0vBaCtbaj2jZp4SP685L2F48tXe+7irrGst+4XBZIghN0QBKEHUiCsANJEHYgCcIOJEHY\ngSQIO5DE/wETAAXBj282MQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpILi1zvM1FR",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: NN using Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtPAJ290M4Yx",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XhzROTpghA-n",
        "colab": {}
      },
      "source": [
        "def relu(x):  \n",
        "    return np.maximum(x, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQbAy47N9JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "  maxVal = np.amax(x, axis = 1)\n",
        "  newX = x - maxVal[:, None]\n",
        "  sumX = np.sum(np.exp(newX), axis = 1, keepdims=True)\n",
        "  result = np.exp(newX)/sumX\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l23ZhFxyQYnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeLayer(X, W, b):\n",
        "  return np.matmul(X,W) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfWuTDFHKc4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CE(target, prediction):\n",
        "  summation = np.sum(np.sum(target * np.log(prediction + 1e-15), axis = 1))\n",
        "  return -1 * summation/target.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xusbxNqGc1Xv",
        "colab": {}
      },
      "source": [
        "def softmaxDerivative(i, j, xi, xj):\n",
        "  if (i==j):\n",
        "    return softmax(xi)*(1-softmax(xj))\n",
        "  \n",
        "  else:\n",
        "    return -1*softmax(xi) * softmax(xj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXp98axbMQ6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradCE(target, predictionInput):\n",
        "  derivativeOfInput = predictionInput - target # [0.1, 0.1, 0.1, 0.9 ... ] - [0, 0, 0, 1, ....]\n",
        "  return derivativeOfInput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xbv2mdtLyzT",
        "colab_type": "code",
        "outputId": "7b398516-0350-4731-9675-a5c0614baebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "Y = np.array([[0 , 1, 0], [1, 0 , 0]])\n",
        "YPred = np.array([[0.05 , 0.9, 0.05], [0.1, 0.8 , 0.1]])\n",
        "gradCE(Y, YPred) # Works"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.05, -0.1 ,  0.05],\n",
              "       [-0.9 ,  0.8 ,  0.1 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg7DLqpZ8P6L",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 and 1.3: Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcgfDKpKNAVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relusign(matrix):\n",
        "  reluMatrix = relu(matrix)\n",
        "  reluMatrix[reluMatrix<=0] = 0\n",
        "  reluMatrix[reluMatrix>0] = 1\n",
        "  return reluMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYjGdRklfZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forwardPropagation(X, W1, W2, b1, b2):\n",
        "  z=computeLayer(X, W1, b1)\n",
        "  z1=relu(z)\n",
        "\n",
        "  z2=computeLayer(z1, W2, b2)\n",
        "  y_hat=softmax(z2)\n",
        "  \n",
        "  return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvMrrNS7tfoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_pred, y):\n",
        "    max_pred=np.argmax(y_pred, axis=1)\n",
        "    max_targ=np.argmax(y, axis=1)\n",
        "\n",
        "    diff = max_targ - max_pred\n",
        "    correctPred = np.count_nonzero(diff == 0)\n",
        "\n",
        "    return correctPred/(y_pred.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TTYXqJ6FPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generatePlot(x_val, y_val_train, y_val_valid, y_val_test, title, xlabel, ylabel, yupper):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "   \n",
        "    plt.plot(x_val,y_val_train, label=\"training\")\n",
        "    plt.plot(x_val,y_val_test, label=\"test\")\n",
        "    plt.plot(x_val,y_val_valid, label=\"validation\")\n",
        "    plt.ylim(0, yupper)    \n",
        "    plt.legend(loc='upper right')        \n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7tuC-x9Yik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(iterations, numhiddenunits):\n",
        "  W1 = np.random.normal(0, np.sqrt(2/(784+numhiddenunits)), (784, numhiddenunits)) #should be 1000 outputs going into the hidden layer\n",
        "  W2 = np.random.normal(0, np.sqrt(2/(10+numhiddenunits)), (numhiddenunits, 10)) #10 outputs for output layer\n",
        "\n",
        "  b1=np.zeros(numhiddenunits)\n",
        "  b2=np.zeros(10)\n",
        "\n",
        "  alpha=1e-5\n",
        "  gamma=0.99\n",
        "  v1=np.full((784, numhiddenunits), 1e-5)\n",
        "  v2=np.full((numhiddenunits, 10), 1e-5)\n",
        "  oneHotTrainY, oneHotValidY, oneHotTestY=convertOneHot(trainY, validY, testY)\n",
        "  minloss=3\n",
        "  minlossepoch=0\n",
        "  X=[]\n",
        "  X_v=[]\n",
        "  X_t=[]\n",
        "  for j in range(0, trainX.shape[0]):\n",
        "    X.insert(j,trainX[j].flatten())\n",
        "\n",
        "  for j in range(0, validX.shape[0]):\n",
        "    X_v.insert(j,validX[j].flatten())\n",
        "\n",
        "  for j in range(0, testX.shape[0]):\n",
        "    X_t.insert(j,testX[j].flatten())\n",
        "\n",
        "  X=np.array(X)\n",
        "  X_v=np.array(X_v)\n",
        "  X_t=np.array(X_t)\n",
        "  \n",
        "  acc_train_list=[]\n",
        "  acc_test_list=[]\n",
        "  acc_val_list=[]\n",
        "\n",
        "  loss_train_list=[]\n",
        "  loss_test_list=[]\n",
        "  loss_val_list=[]\n",
        "\n",
        "  for step in range(0, iterations):\n",
        "\n",
        "    y_h=forwardPropagation(X, W1, W2, b1, b2)\n",
        "    y_v=forwardPropagation(X_v, W1, W2, b1, b2)\n",
        "    y_t=forwardPropagation(X_t, W1, W2, b1, b2)\n",
        "    print(\"**************************************************************\", step)\n",
        " \n",
        "    acc_train=accuracy(y_h, oneHotTrainY)\n",
        "    acc_valid=accuracy(y_v, oneHotValidY)\n",
        "    acc_test=accuracy(y_t, oneHotTestY)\n",
        "    print(\"what is the tr accuracy: \", acc_train)\n",
        "    print(\"what is the v accuracy: \", acc_valid)\n",
        "    print(\"what is the t accuracy: \", acc_test)\n",
        "\n",
        "\n",
        "    loss_train=CE(oneHotTrainY, y_h)\n",
        "    loss_valid=CE(oneHotValidY, y_v)\n",
        "    loss_test=CE(oneHotTestY, y_t)\n",
        "    print('what is the loss ',loss_train)\n",
        "    print('what is the loss v ',loss_valid)\n",
        "    print('what is the loss test',loss_test)\n",
        "\n",
        "    if (loss_test < minloss):\n",
        "      minloss=loss_test\n",
        "      minlossepoch=step\n",
        "\n",
        "    acc_train_list.append(acc_train)\n",
        "    acc_val_list.append(acc_valid)\n",
        "    acc_test_list.append(acc_test)\n",
        "    loss_train_list.append(loss_train)\n",
        "    loss_test_list.append(loss_test)\n",
        "    loss_val_list.append(loss_valid)\n",
        "\n",
        "    z1=computeLayer(X, W1, b1)\n",
        "    sign=relusign(z1)\n",
        "    gradient=gradCE(oneHotTrainY, y_h)\n",
        "    derivativeW1=np.matmul(gradient, W2.T)\n",
        "\n",
        "    derivativeW1=(np.multiply(derivativeW1, sign)).T.dot(X) #784x1000\n",
        "\n",
        "    derivativeb1=np.multiply(gradient.dot(W2.T), sign)\n",
        "    derivativeb1=np.sum(derivativeb1, axis=0) #1x1000\n",
        "    \n",
        "    derivativeb2= np.sum(gradient, axis=0) #1x10\n",
        "\n",
        "    derivativeW2=(gradient.T).dot(relu(z1))\n",
        "\n",
        "    v1=gamma*v1+alpha*derivativeW1.T\n",
        "\n",
        "    v2=gamma*v2+alpha*derivativeW2.T\n",
        "\n",
        "    W1-=v1\n",
        "    W2-=v2\n",
        "\n",
        "    b1=b1-alpha*derivativeb1\n",
        "    b2=b2-alpha*derivativeb2\n",
        "\n",
        "  print(\"min loss is \"+str(minloss)+\" at epoch \"+str(minlossepoch))\n",
        "  return acc_train_list, acc_val_list, acc_test_list, loss_train_list, loss_val_list, loss_test_list\n",
        "\n",
        "#acc_train, acc_val, acc_test, loss_train, loss_val, loss_test = gradient_descent(200, 1000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heYb2jR3-3lz",
        "colab_type": "code",
        "outputId": "61e6cbce-0b2f-4341-ba92-f55460939e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "generatePlot(range(200), loss_train, loss_val, loss_test, \"Loss over 200 epochs\", \"Number of epochs\", \"Loss\", 3)\n",
        "generatePlot(range(200), acc_train, acc_val, acc_test, \"Accuracy over 200 epochs\", \"Number of epochs\", \"Accuracy\", 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3xcZ532/893imbUu1zkJvfe00l1\nOklIgXTYhIVAfvDLhmWzkCwQyC5saHmAJ5CQQCgBQhqkhxTiVEixHccldlziJluW1XuZ0dzPH2cs\ny7Zkybak0cTXm9e8NDPnzJnvjIMu3efcxZxziIiISPLxJboAEREROTQKcRERkSSlEBcREUlSCnER\nEZEkpRAXERFJUgpxERGRJKUQF5Ejkpl928z+kOg6RA6HQlykj8xss5mdnug6BpOZ/cjM1ptZg5mt\nNbPP7LN9rpktNbPm+M+5XbaZmX3fzKrit++bmQ3+pxD56FKIiwgAZubv5ukm4HwgG/gX4Kdmdnx8\n/xTgceAPQC7wO+Dx+PMA1wEXAnOA2fHjfGEgP4PIkUYhLtIPzOzzZrbBzKrN7AkzGxl/3szs/5jZ\nLjOrN7OVZjYzvu1cM3s/3srdbmb/0cOxfWb2DTPbEj/O780sO77tWTP78j77v2dmF8fvTzWzF+J1\nfWBml3bZ77dmdpeZPWNmTcCp+763c+5W59xa51zMOfcW8BpwXHzzKUAA+Ilzrs059zPAgNPi2/8F\n+LFzrtQ5tx34MXDNAb7D88xsuZnVmtk/zGx2l22bzezm+PdVY2a/MbNwb99/fNuMLt9BuZnd0uVt\nU+LfZ4OZrTazhV1e97X4v0tD/Ltb1FPtIomiEBc5TGZ2GvC/wKXACGAL8Of45jOBk4DJeK3ZS4Gq\n+LZfA19wzmUCM4GXeniLa+K3U4HxQAZwZ3zbA8AVXWqZDowFnjazdOAF4E9AEXA58Iv4PrtdCXwX\nyARe7+VzpgJHAavjT80AVri9525eEX9+9/b3umx7r8u2fY89D7gPr6WeD/wSeMLMQl12uwo4C5iA\n931+I/7aHr9/M8sEXgT+BowEJgJ/73LMC+L75gBPEP9ezWwK8GXgqPi/z1nA5p6/HZHEUIiLHL6r\ngPucc8ucc23AzcBxZjYOiOAF5FTAnHNrnHNl8ddFgOlmluWcq3HOLTvA8e9wzn3onGuMH/9yMwsA\nfwXmmtnYLvv+JV7HecBm59xvnHNR59y7wKPAp7oc+3Hn3BvxlnZrL5/zbrwgfi7+OAOo22efuvjn\n7W57HZDRw3Xx64BfOufecs51OOd+B7QBx3bZ507n3DbnXDXeHx67/3g50Pd/HrDTOfdj51yrc64h\nfkZht9edc8845zqA+/FO/QN0ACG8f5+gc26zc25jL9+PyKBTiIscvpF4rT8A4kFbBRQ7517Ca939\nHNhlZveYWVZ810uAc4EtZvaKmR1H9/Y6fvx+ABjmnGsAnsZrZYMXbH+M3x8LHBM/PV1rZrV4gTe8\ny7G29eUDmtkP8c4WXNql5d0IZO2zaxbQ0MP2LKBxn5b7bmOBr+5T62i8z95drVu6bOvx+48f40Dh\nu7PL/WYgbGYB59wG4Ebg23j/bn/ueopeZKhQiIscvh14IQRA/DR2PrAdwDn3M+fcAmA63mngm+LP\nv+Oc+wTeqe7HgIf6cnxgDBAFyuOPHwCuiP8REAYWx5/fBrzinMvpcstwzl3f5Vi9LmNoZt8BzgHO\ndM7Vd9m0Gpi9T8t6NntOt69mT8uW+P3VdG8b8N19ak1zzj3QZZ/RXe6Pwfte4MDf/za8SxAHzTn3\nJ+fcx+LHdsD3D+U4IgNJIS5ycIJmFu5yC+CF6LXmDbcKAd8D3nLObTazo8zsGDML4vX0bgViZpZi\nZleZWbZzLgLUA7Ee3vMB4CtmVmJmGfHjP+ici8a3P4MXNLfFn999nKeAyWb2aTMLxm9Hmdm0vn5Y\nM7sZ77r56c65qn02v4x32vkGMwt16WC3+9r+74F/N7PieCv2q8Bve3ire4Evxr8rM7N0M/t4/Jr2\nbl8ys1Fmlgf8F/Bgl++n2+8//h2MMLMb4zVmmtkxffjcU8zstPjxWoEWev73EUkYhbjIwXkG7xf6\n7tu3nXMvAt/Eu95chtfxavfp7Sy8gKrBO+VbBfwwvu3TwGYzqwe+iHequzv34V2vfRXYhBcq///u\njfHrwH8BTsfrxLb7+Qa8jnWX47VWd+K1Jrt2FuvN9/BavRvMrDF+uyV+/Ha8IWSfAWqBzwIXxp8H\nr3Pak8BKYBXeaf9fdvcmzrklwOfxLj3UABvYvyf7n4DngQ/xTpH/T/y1PX7/8e/gDLzhbTuB9XTT\nC78bIeB2oDL+uiK8a+0iQ4p1f3lKRGToMLPNwOfigS0icWqJi4iIJKkBC/H49cK3zZt4YnW8c8y+\n+4TM7MH4JA1vxYeEiIiISB8EBvDYbcBpzrnGeKee183sWefcm132+Vegxjk30cwux7ted9kA1iQi\nScg5Ny7RNYgMRQPWEneexvjDYPy27wX4T+DNtwzwCLCoh4kgREREZB8Dek3czPxmthzYBbywz0xJ\n4E3GsA0gPlymDm98p4iIiPRiIE+nE5/KcK6Z5QB/NbOZzrlVB3scM7sOb1pG0tPTF0ydOrWfKxUR\nERm6li5dWumcK9z3+QEN8d2cc7Vmthg4G2+86G7b8WZhKo1PmpHNnsUhur7+HuAegIULF7olS5YM\nfNEiIiJDhJlt6e75geydXhhvge9e/egMYO0+uz2Bt1whwCeBl3qYV1lERET2MZAt8RHA78zMj/fH\nwkPOuafM7DZgiXPuCbylGO83sw1ANXtmuRIREZFeDFiIO+dWAPO6ef5bXe63sveyiCIiItJHg3JN\nXEREPnoikQilpaW0tva2FL30VTgcZtSoUQSDwT7trxAXEZFDUlpaSmZmJuPGjUNTfBw+5xxVVVWU\nlpZSUlLSp9do7nQRETkkra2t5OfnK8D7iZmRn59/UGc2FOIiInLIFOD962C/T4W4iIgkpdraWn7x\ni18c9OvOPfdcamtrD7jPt771LV58ceivfKsQFxGRpNRTiEej0QO+7plnniEnJ+eA+9x2222cfvrp\nh1XfYFCIi4hIUvr617/Oxo0bmTt3LkcddRQnnngiF1xwAdOnTwfgwgsvZMGCBcyYMYN77rmn83Xj\nxo2jsrKSzZs3M23aND7/+c8zY8YMzjzzTFpaWgC45ppreOSRRzr3v/XWW5k/fz6zZs1i7Vpv3rKK\nigrOOOMMZsyYwec+9znGjh1LZWXloH4H6p0uIiKH7TtPrub9HfX9eszpI7O49fwZPW6//fbbWbVq\nFcuXL+fll1/m4x//OKtWrers2X3fffeRl5dHS0sLRx11FJdccgn5+XuvsbV+/XoeeOAB7r33Xi69\n9FIeffRRrr766v3eq6CggGXLlvGLX/yCH/3oR/zqV7/iO9/5Dqeddho333wzf/vb3/j1r3/dr5+/\nL9QSFxGRj4Sjjz56r6FZP/vZz5gzZw7HHnss27ZtY/369fu9pqSkhLlz5wKwYMECNm/e3O2xL774\n4v32ef3117n8cm+i0bPPPpvc3Nx+/DR9o5a4iIgctgO1mAdLenp65/2XX36ZF198kX/+85+kpaVx\nyimndDt0KxQKdd73+/2dp9N72s/v9/d6zX0wqSUuIiJJKTMzk4aGhm631dXVkZubS1paGmvXruXN\nN9/s9/c/4YQTeOihhwB4/vnnqamp6ff36I1a4iIikpTy8/M54YQTmDlzJqmpqQwbNqxz29lnn83d\nd9/NtGnTmDJlCscee2y/v/+tt97KFVdcwf33389xxx3H8OHDyczM7Pf3ORBLtpU/tZ64iMjQsGbN\nGqZNm5boMhKmra0Nv99PIBDgn//8J9dffz3Lly8/7ON2972a2VLn3MJ991VLXERE5BBs3bqVSy+9\nlFgsRkpKCvfee++g16AQFxEROQSTJk3i3XffTWgN6tgmIiKSpBTiIiIiSUohLiIikqQU4iIiIklK\nIS4iIknpUJciBfjJT35Cc3NzP1c0+BTiIiKSlBTiGmImIiJJqutSpGeccQZFRUU89NBDtLW1cdFF\nF/Gd73yHpqYmLr30UkpLS+no6OCb3/wm5eXl7Nixg1NPPZWCggIWL16c6I9yyBTiIiJy+J79Ouxc\n2b/HHD4Lzrm9x81dlyJ9/vnneeSRR3j77bdxznHBBRfw6quvUlFRwciRI3n66acBb0717Oxs7rjj\nDhYvXkxBQUH/1jzIdDpdRESS3vPPP8/zzz/PvHnzmD9/PmvXrmX9+vXMmjWLF154ga997Wu89tpr\nZGdnJ7rUfqWWuIiIHL4DtJgHg3OOm2++mS984Qv7bVu2bBnPPPMM3/jGN1i0aBHf+ta3ElDhwFBL\nXEREklLXpUjPOuss7rvvPhobGwHYvn07u3btYseOHaSlpXH11Vdz0003sWzZsv1em8zUEhcRkaTU\ndSnSc845hyuvvJLjjjsOgIyMDP7whz+wYcMGbrrpJnw+H8FgkLvuuguA6667jrPPPpuRI0cmdcc2\nLUUqIiKH5EhfinSgHMxSpDqdLiIikqQU4iIiIklKIS4iIpKkFOIiIiJJSiEuIiKSpBTiIiIiSUoh\nLiIiR4yMjAwAduzYwSc/+clu9znllFPobSjzvqugnXvuudTW1vZfoX2kEBcRkSPOyJEjeeSRRw75\n9fuG+DPPPENOTk5/lHZQFOIiIpK0vv71r/Pzn/+88/G3v/1t/ud//odFixYxf/58Zs2axeOPP77f\n6zZv3szMmTMBaGlp4fLLL2fatGlcdNFFtLS0dO53/fXXs3DhQmbMmMGtt94KwM9+9rPOpUxPPfVU\nAMaNG0dlZSUAd9xxBzNnzmTmzJn85Cc/6Xy/adOm8fnPf54ZM2Zw5pln7vU+h0rTroqIyGH7/tvf\nZ2312n495tS8qXzt6K8dcJ/LLruMG2+8kS996UsAPPTQQzz33HPccMMNZGVlUVlZybHHHssFF1yA\nmXV7jLvuuou0tDTWrFnDihUrmD9/fue27373u+Tl5dHR0cGiRYtYsWIFN9xwQ49LmS5dupTf/OY3\nvPXWWzjnOOaYYzj55JPJzc1l/fr1PPDAA9x7771ceumlPProo1x99dWH9R2pJS4iIklr3rx5nQud\nvPfee+Tm5jJ8+HBuueUWZs+ezemnn8727dspLy/v8RivvvpqZ5jOnj2b2bNnd2576KGHmD9/PvPm\nzWP16tW8//77B6zn9ddf56KLLiI9PZ2MjAwuvvhiXnvtNQBKSkqYO3cuAAsWLGDz5s2H+enVEhcR\nkX7QW4t5IH3qU5/ikUceYefOnVx22WX88Y9/pKKigqVLlxIMBhk3bhytra0HfdxNmzbxox/9iHfe\neYfc3FyuueaaQzrObqFQqPO+3+/vl9PpaomLiEhSu+yyy/jzn//MI488wqc+9Snq6uooKioiGAyy\nePFitmzZcsDXn3TSSfzpT38CYNWqVaxYsQKA+vp60tPTyc7Opry8nGeffbbzNT0tZXriiSfy2GOP\n0dzcTFNTE3/961858cQT+/HT7k0tcRERSWozZsygoaGB4uJiRowYwVVXXcX555/PrFmzWLhwIVOn\nTj3g66+//nquvfZapk2bxrRp01iwYAEAc+bMYd68eUydOpXRo0dzwgkndL6mp6VM58+fzzXXXMPR\nRx8NwOc+9znmzZvXL6fOu6OlSEVE5JBoKdKBoaVIRUREjgAKcRERkSSlEBcREUlSAxbiZjbazBab\n2ftmttrM/q2bfU4xszozWx6/fWug6hERkf6XbP2qhrqD/T4Hsnd6FPiqc26ZmWUCS83sBefcviPl\nX3POnTeAdYiIyAAIh8NUVVWRn5/f42xo0nfOOaqqqgiHw31+zYCFuHOuDCiL328wszVAMXDg6W5E\nRCQpjBo1itLSUioqKhJdykdGOBxm1KhRfd5/UMaJm9k4YB7wVjebjzOz94AdwH8451Z38/rrgOsA\nxowZ06+1tUdjOCI8v+V5zhp3Fin+lH49vojIR1UwGKSkpCTRZRzRBjzEzSwDeBS40TlXv8/mZcBY\n51yjmZ0LPAZM2vcYzrl7gHvAGyfeX7W9vr6Sm+7/Ows+tpRXyp4lP5zP8cXH99fhRUREBtSA9k43\nsyBegP/ROfeXfbc75+qdc43x+88AQTMr2He/gTK96S2uy7mJV8q8qfRaooc/j62IiMhgGcje6Qb8\nGljjnLujh32Gx/fDzI6O11M1UDXta11OiJ/kZTEqkgpAe6x9sN5aRETksA3k6fQTgE8DK81sefy5\nW4AxAM65u4FPAtebWRRoAS53gzheYdqIozi9PZcrd23mmtE5tHcoxEVEJHkMZO/014EDjjlwzt0J\n3DlQNfQmO5TNtVO/SmH5Z4Ec2jraElWKiIjIQTviZ2ybeMy51MSyAYjEIgmuRkREpO+O+BAPh1JY\nmXkGAO1NlQmuRkREpO+O+BAHCE0+E4DqXZsSXImIiEjfKcSBCSOGYc5R09qc6FJERET6TCEOhMJp\nhJwjoo5tIiKSRBTiQCAlTNBBREPMREQkiSjEgZRQmBTnNE5cRESSikIcCIbC3ul0DTETEZEkohAH\ngimppDhHxCnERUQkeSjEgVAoTBCFuIiIJBeFOHuuiUdj0USXIiIi0mcKccDv9xGMQYSORJciIiLS\nZwrxuIAzok4tcRERSR4K8bgARoRYossQERHpM4V4XNAZHTqdLiIiSUQhHud3aomLiEhyUYjHBfAR\nVYiLiEgSUYjH+Z2PiLlElyEiItJnCvE4ryWuEBcRkeShEI/z41dLXEREkopCPC6An6iBcwpyERFJ\nDgrxOD8BAK1kJiIiSUMhHheIh7jWFBcRkWShEI/z+/wAtHW0JbgSERGRvlGIxwUJAjqdLiIiyUMh\nHhfwpQA6nS4iIslDIR7nN68lrtPpIiKSLBTiccHdLfGYWuIiIpIcFOJxQb8X4i0RtcRFRCQ5KMTj\nAr4QAM1tLQmuREREpG8U4nFBvxfiTa0NCa5ERESkbxTicUF/GICW9qYEVyIiItI3CvG4lGAqAC2t\nzQmuREREpG8U4nFBvxfirZHGBFciIiLSNwrxuPDulni7OraJiEhyUIjH7T6d3hZViIuISHJQiMeF\ng2kAtEdaE1yJiIhI3yjE48JhL8TbogpxERFJDgrxuFAgA3OO9g6FuIiIJAeFeFwwnEqKc7RHNe2q\niIgkB4V4XCAYIsVBJKYQFxGR5KAQjwuGQqQ4R0TriYuISJJQiMcFUlJJwRHRUqQiIpIkFOJxwZSw\n1xJXiIuISJJQiMelhFLjIR5NdCkiIiJ9MmAhbmajzWyxmb1vZqvN7N+62cfM7GdmtsHMVpjZ/IGq\npze7r4lHXSRRJYiIiByUwAAeOwp81Tm3zMwygaVm9oJz7v0u+5wDTIrfjgHuiv8cdBbwTqc3ObXE\nRUQkOQxYS9w5V+acWxa/3wCsAYr32e0TwO+d500gx8xGDFRNB+RP8YaY0ZGQtxcRETlYg3JN3MzG\nAfOAt/bZVAxs6/K4lP2DHjO7zsyWmNmSioqKgSnS5yfoHFG1xEVEJEkMeIibWQbwKHCjc67+UI7h\nnLvHObfQObewsLCwfwvsIuCMqFriIiKSJAY0xM0siBfgf3TO/aWbXbYDo7s8HhV/LiG8EI8l6u1F\nREQOykD2Tjfg18Aa59wdPez2BPCZeC/1Y4E651zZQNXUm4DzKcRFRCRpDGTv9BOATwMrzWx5/Llb\ngDEAzrm7gWeAc4ENQDNw7QDW06sARtQU4iIikhwGLMSdc68D1ss+DvjSQNVwsAL4iOISXYaIiEif\naMa2LgLOR0QtcRERSRIK8S78+IkaeCcIREREhjaFeBcB8wMQ1fzpIiKSBBTiXfjjXQTaOtoSXImI\niEjvFOJdBOIh3q7lSEVEJAkoxLsIWDzEOxTiIiIy9CnEu9gd4pEOLUcqIiJDn0K8i4AFAV0TFxGR\n5KAQ7yLgSwEU4iIikhwU4l0E4yHe1K4QFxGRoU8h3kXAHw/xSGuCKxEREemdQryLFH8IgJaIWuIi\nIjL0KcS7CMZDvKGtKcGViIiI9E4h3kVxII+MWIxXSl9IdCkiIiK9Uoh3kRpM55KGRt4of5myxrJE\nlyMiInJACvEufMEQV9Y34Bzc//4fE12OiIjIASnEu/AFQ4yMdjApNI/fr3qQ59dsTXRJIiIiPVKI\nd+ELhgHYuX4C5m/lte2vJbgiERGRninEu/DHQzwUSQegsb0+keWIiIgckEK8i4y0VAA+s2A0AM1R\nTfoiIiJDl0K8i4LsLACumFsMQGukJZHliIiIHJBCvKuAN+1qSiyGcz5aO9QSFxGRoUsh3lV8xjaL\ntWMuqBAXEZEhTSHeVbwlTulSAs5Pu0JcRESGMIV4V+mFgMHi/2FYrJ609m2JrkhERKRHCvGucsbA\nV1bDFQ+S6mK4WHOiKxIREelRINEFDDnZxRDKIDXmaPS1J7oaERGRHqkl3p1gOmHniKIQFxGRoUsh\n3h1/gJCDCNFEVyIiItIjhXgPQs5H1DoSXYaIiEiPFOI9COJXiIuIyJCmEO9BCn4iFkt0GSIiIj1S\niPcgxQK0K8RFRGQIU4j3IMWCRHzQFlXnNhERGZoU4j1IMW8e9fpWTfgiIiJDk0K8Byk+L8RrFeIi\nIjJEKcR7EPKHAahraUpwJSIiIt1TiPcgHEgFoK5NLXERERmaFOI9CAfSAKhXiIuIyBClEO9BKJgO\nQENLXYIrERER6V6fQtzMJph53bXN7BQzu8HMcga2tMRKC2UC0Nxak+BKREREutfXlvijQIeZTQTu\nAUYDfxqwqoaAtBQvxFta1RIXEZGhqa8hHnPORYGLgP/rnLsJGDFwZSVeejgLgJa2+gRXIiIi0r2+\nhnjEzK4A/gV4Kv5ccGBKGhrS07IBaGtvSHAlIiIi3etriF8LHAd81zm3ycxKgPsHrqzEy0jLA6At\n0pjgSkRERLrXpxB3zr3vnLvBOfeAmeUCmc657x/oNWZ2n5ntMrNVPWw/xczqzGx5/PatQ6h/wOSk\nx0M8qsleRERkaOpr7/SXzSzLzPKAZcC9ZnZHLy/7LXB2L/u85pybG7/d1pdaBks4NRu/c0SiGicu\nIiJDU19Pp2c75+qBi4HfO+eOAU4/0Aucc68C1YdZX8JYKIOwc7THWhJdioiISLf6GuIBMxsBXMqe\njm394Tgze8/MnjWzGf143MMXTCccc0RibYmuREREpFt9DfHbgOeAjc65d8xsPLD+MN97GTDWOTcH\n+L/AYz3taGbXmdkSM1tSUVFxmG/bRylppLoY0Vjr4LyfiIjIQeprx7aHnXOznXPXxx9/6Jy75HDe\n2DlX75xrjN9/BgiaWUEP+97jnFvonFtYWFh4OG/bd4FUws4Rce2D834iIiIHqa8d20aZ2V/jvc13\nmdmjZjbqcN7YzIabmcXvHx2vpepwjtmvfD5CMSNCJNGViIiIdCvQx/1+gzfN6qfij6+OP3dGTy8w\nsweAU4ACMysFbiU+QYxz7m7gk8D1ZhYFWoDLnXPuED7DgEnBaCGa6DJERES61dcQL3TO/abL49+a\n2Y0HeoFz7opett8J3NnH90+IFOejXSEuIiJDVF87tlWZ2dVm5o/frmYonfoeICnOT9RiiS5DRESk\nW30N8c/iDS/bCZThnQq/ZoBqGjKC5qddIS4iIkNUX3unb3HOXeCcK3TOFTnnLgQOq3d6MkghQEQh\nLiIiQ1RfW+Ld+fd+q2KISrEU2izRVYiIiHTvcEL8Ix9vKZZCuw86Yh2JLkVERGQ/hxPiQ2o42EAI\n+kIANLRp/nQRERl6DjjEzMwa6D6sDUgdkIqGkBR/GIDa1iZyUjMSXI2IiMjeDhjizrnMwSpkKAr5\nU8FBbUsj5A5LdDkiIiJ7OZzT6R954UAaAPWNNQmuREREZH8K8QMIBdMBaGhJ2mXRRUTkI0whfgDh\nFO86eHNzbYIrERER2Z9C/ADSQl6XgOZWhbiIiAw9CvEDSA9lA9DcVp/gSkRERPanED+AtFQvxFva\nGxJciYiIyP4U4geQkZYLQJtCXEREhiCF+AHkZBQA0BJpTHAlIiIi+1OIH0BOVjzEo00JrkRERGR/\nB5yx7UiXlpZLaixGq2tOdCkiIiL7UYgfgIUySYs52mlNdCkiIiL7UYgfiD9AqnO0KcRFRGQI0jXx\nXqTGjDbXnugyRERE9qMQ70XIGe0oxEVEZOhRiPci5Py005HoMkRERPajEO9FiABtphAXEZGhRyHe\nixBB2iyW6DJERET2oxDvRYoFafG5RJchIiKyH4V4L0IWptXAOQW5iIgMLQrxXoR9YZwZTe2atU1E\nRIYWhXgvQoE0AKqb6xJciYiIyN4U4r0IBzIBqK7fleBKRERE9qYQ70U4mAFArUJcRESGGIV4L9JD\n2QDUNlcmuBIREZG9KcR7kR7OAaCpqSrBlYiIiOxNId6LjLRcABpaaxNciYiIyN4U4r3ISi8EoLmt\nPsGViIiI7E0h3oucjAIAmtsV4iIiMrQoxHuRmzUMgJZIY4IrERER2Vsg0QUMdZlZ+aTEHK1oxjYR\nERla1BLvRSg1g3QXo62jJdGliIiI7EUt8V6Yz0c4Bm20JroUERGRvSjE+yDsjDbaE12GiIjIXnQ6\nvQ9CMaONSKLLEBER2YtCvA9Czk870USXISIisheFeB+E8NNqHTjn+NXKX7GpblOiSxIREVGI90UK\nQVrNUd5czk+X/ZSnP3w60SWJiIgoxPsihRTazLGhdgMA9Zq9TUREhoABC3Ezu8/MdpnZqh62m5n9\nzMw2mNkKM5s/ULUcrpAvTLMP1tesBxTiIiIyNAxkS/y3wNkH2H4OMCl+uw64awBrOSwpvjAdZqyu\nWg1ATUtdgisSEREZwBB3zr0KVB9gl08Av3eeN4EcMxsxUPUcjpA/DYB3y5cDsKmyLJHliIiIAIm9\nJl4MbOvyuDT+3H7M7DozW2JmSyoqKgaluK5CgQwAdrWUA9DRvnPQaxAREdlXUnRsc87d45xb6Jxb\nWFhYOOjvHw5mdt5PjcVocZq9TUREEi+RIb4dGN3l8aj4c0NOWkpW5/15rW00WwTnXAIrEhERSWyI\nPwF8Jt5L/Vigzjk3JC82p2h6YXYAACAASURBVIVyAPA5x9y2NjoMWqJa1UxERBJrwBZAMbMHgFOA\nAjMrBW4FggDOubuBZ4BzgQ1AM3DtQNVyuDLT8gAYE4mSF40B3jCztGBaIssSEZEj3ICFuHPuil62\nO+BLA/X+/SkzPR+Aie0RGn3FQAt1bXUMTx+e2MJEROSIlhQd2xItO2MYABmRbFLDXsc6TfgiIiKJ\nphDvg8K8cdy2q4ZxdiLpKdkAVDXXJrgqERE50inE+yAzK5efVn+bhmlfID3Fuz5e3ujNY/Ovz/0r\nf1n/l0SWJyIiRyiFeB9kpwX5r0+fz2dPmkR2WgEAFfU7aY408/bOt1lavjTBFYqIyJFowDq2fdSc\nMd27Lp6ZPgxfo6OmqYKyJm9EXFVLVSJLExGRI5Ra4gcpNTOPzFiM+pYqtjd6c9NUtlQmuCoRETkS\nKcQPUmpWPlmxGA1ttZQ1ei3xipbBn89dREREIX6QUrPyyI7FaIw0sKNpBwDVrTVEY9EEVyYiIkca\nhfhBSs/KI6sjRnNHE1vrS+PPOmpaaxJal4iIHHkU4gfJl5pDVixGS6yFLXXbcc4AXRcXEZHBpxA/\nWKEssmKOFtopby4j1u7N4KYQFxGRwaYQP1g+H6kxP80WoT5STazFW01VIS4iIoNNIX4Iwi5I/Cw6\nV7evAaCscVcCKxIRkSORQvwQpFqo8/5ZHZsIdxhb68oTWJGIiByJFOKHIGypnfeLo1FyOmBnk1ri\nIiIyuBTihyDNnwmAOaMo2kFRR4RKTb0qIiKDTCF+CNKCWQCkR4MEgBEd7dS1KcRFRGRwKcQPQUYo\nF4CC+CRthR1RmqLVCaxIRESGjFhs0N5Kq5gdguzUAmiBMdE2YsEM8jtiRGijOdJMWjAt0eWJiMhA\ninVA/Xao3drltg1qt3j3G8rg69sgGB7wUhTihyAjvYDChihHtzdAyUkU7HgFgKrWKoW4iMhHQVsj\nVH8I1RuhehPUbPZCumYL1G2DfdfLyBwBOWNg1FHez452hfhQFcrM46klZYScw3fCKeRvWwx4E76M\nzhyd4OpERKRP2puhZhNUbfTCumqjF9xVG6Fx5977phVA7lgong8zLvLu54yBnLGQPQoCoe7fY4Ap\nxA9BODOPNOe8B4VTSbNsACqatCSpiMiQ0hH1gnrX+1C1Id663uT9bCjbe9/0QsibABMXQd54yJ8I\n+RMgtwRCGYmpvxcK8UOQlpW/50H+RNJDw4AattZrwhcRkYRwDupKYdca2LU6/vN9qFgHHW179ksv\n8gJ6/Knez7ySeGBPgHB24uo/RArxQxDKyAOgw5+KP2skaZlj8LlqzdomIjIY2hpgx3IvpMt3B/Ya\naG/Ys09WMRRNg/GnQNF0KJwKBZMglJmoqgeEQvxQxP9a8xdOBDOCuWPJq17Gjn1PzYiIyOFrKIeN\nL8G2t6D0HS+8XXwYV2oeDJsBc6/wQnt3YKfmJLbmQaIQPxS7T7nkTwQgtbCEUbui7Grcsmef13/i\n/UcXCMGcy2HmJQkoVEQkCTkHO1fCuudg3bOwfan3fCgLRi2Eqed5vcCHz4KMIjBLbL0JpBA/FCnp\nkDHc+48IyBhWwqj3omxq69Kb8R8/A18QIi3QWq8QFxE5kJYa2PyG1/hZ9xzUl3rPFy+AU78Bk8+E\nYbPApznKulKIHwozuGEZBLwxgIG8cRRHotTH6ol0RAi21kFzFZz1PShbAVveSHDBIiJDSCzm9Rjf\nudJrZW96FcreAxwE02DCaXDK12HSmZA5LNHVDmkK8UOVkr7nflYxo6JRnDnKmsoYU70NgCd3ZJJS\n52Nh407yYx3g8yeoWBGRBOiIehOkVK6Hyg+gch3sWutd0440e/v4gt5ZzZO/BuNP9lreCRpznYwU\n4v0hGCbXeWMIt9aXMqbyAyLAtze9QaTgn5yan81PG3ZCdnFi6xQR6U/OeWcdd89mVlcKddu9nzWb\nvHHZHe179k8vgsIpMP9fvM5ow2d6ndCCqT2+hRyYQryfZGVOATazonwj80pX87WiItrT/0HQBdkU\nDHrz7CrERSRZxWJQ9i5sfcs7BV75AVRv3ntYF3idz7JHebOZTTzdC+2Cyd7wrtTchJT+UaYQ7ydF\no44nWLmJD8rWUFu2mtfTwpxefDHVu8p5371GR802/KOPTnSZIiJ911wNG/4OG16ADS96rW6Ij8Ge\nDmOO82YzyyvxQjt7VFJOmJLMFOL9pGDaiRS/9DvKa9byYdOHdKSFOHvCiTzfuozWqjfYXv4BYxJd\npIhIb6o3wQfPwNqnYes/vfHYafleq3riGTDuY5A1ItFVSpxCvJ+ERs+nONrB1vbtbPA3AyEWDp/H\n2rI6qIItlRsU4iIy9DgHZcu90F77jDdlKUDRDDjxqzD5bBg5Tx1zhyiFeH9JSSPHpbHCGlkeCpHj\nsshPzWdq4RhYDzsbtwNww5P/SXVbFX/45K8TXLCIHLGibbDpNW8ilQ+e9frsmA/GHO8NjZ1yrneK\nXIY8hXg/ykoZRYN/K2+nhpmcMRWA2cO9/yNUtnvXktaUP0d5oIO15duYOkzLlorIIHDOW7Vryz9g\n/XOw4SWINO0Zk33aN2DSWZCe3/uxZEhRiPejorwZ0LCVRp+PhWM+BsDwjHyCMaPaNVLfWEN5oANn\nxr0v/pgfX/WTBFcsIh8pzdV7ltnseqta782IBl6ntDmXweRzoOREDe9KcgrxfjR54snw7rMAnDb+\nWADMjKxYmCpfE++8+yjODHOO9Q2vU98aISscTGTJIpJsmqu9yVP2DerqD6G1du99s0Z5p8WnXQDF\n82H0Md647CN4rvGPGoV4P5oz6WPwLoRixpS8SZ3PZ5JNWaCWzRueBB+cHMvmlXAdv3n5Vf7t7EUJ\nrFhEhrTGXXtW76r4wLs1V+7Zbj7IHu2thz3zkvj62PFb7li1so8ACvF+lJ2aTZqlMSarhIBvz1eb\nHSyklO1UNb5PIDPEl0/4Oi+/eQvvrrmb1kWnEA6q16eIxLXWwerH4L0HvCFe4I29LpwKU87ZM3lK\n3gRvbHYgJbH1SkIpxPvZNz/2TUamj9zrufz0MbzXuJJ1wQhF0TBTJp/HxDe+QWPqWh5duo2rjh2X\nmGJFZGiItnsTqqx4ED74G3S0Qf4kOOUWb/Wu4XO0epd0SyHez84bf95+z43ImwyNT7M0HGKqGwlm\nnD3iOO7c9QYvv/pnLj/6a/h9ukYlckSJxWDbm15wr37Mu56dVgAL/gVmX+YtBKJr19ILhfggmFQ0\nAbZC1IxhOXMBOPvor3DnU29Q5HucZ1d9hvNmj+zlKCKS9JzzVvBa+QisfBjqtnnDvKaeB7MvhfGn\ngF+dXaXvFOKDYMawsZ33Z8bnTx+bP4WpwRw2Z5bz7l9e4K/LpvOxSQVcfexYgn6dNhP5yHDOWyv7\n/ce9W/VGML83PnvRt7yJVUIZia5SkpRCfBBMyC0GBxicPG5W5/NnTbqYn75/H/8n+0GeLz+TX67N\n4+Elpfzgk7OZWaxFBESSTkfEG6dduc4bm719GZS+Aw1lXnCXnAjHfxmmng8ZhYmuVj4CBjTEzexs\n4KeAH/iVc+72fbZfA/wQ2B5/6k7n3K8GsqZECPqD+F0OHTQzIW/PLG1nTf0kP33/Pra7Ffyg5Q0I\nwzu1M/nFvefx41v+k9QU9VoXGVJiMWir88ZqN1d5Y7MrPvBCu3Kd9zgW3bN/zlhvwZCSk2HqxyEt\nL3G1y0fSgIW4mfmBnwNnAKXAO2b2hHPu/X12fdA59+WBqmOoGJ05mg4XxWe+vZ6bkT+D+1JK2TJz\nNsdEjWNXv8xPG3/IK++ey+nHzOnct27Jw/DKD8j6/17AUnMS8RFEPtqcg7pS2PEuVG2Ams3QVAkt\n1XtCu6UGXMfer/MFvHHZBZO9a9sFk+O3iVqW8wgU6YjQEGkgN5SLDULHxIFsiR8NbHDOfQhgZn8G\nPgHsG+JHhDvP/N9un7/5mJu56727eHbXuzwcaSRzZCafqahnxJv3wTE/BcC11OKeuYmcWA1lL9/D\niHP+czBLF/noqtkCm16BD1+Bza9D484929IKIGOY13oumgqped79tPw993PHeetpa6x2UnPOEYlF\naIm20NbRRmu0lZZoC83RZhraG2hob6CxvZGGSEPn44b2hs7Hje2Nnc+1drQC8NaVb5EWTBvw2gcy\nxIuBbV0elwLHdLPfJWZ2ErAO+Ipzbtu+O5jZdcB1AGPGJOeCnmOzxnb7/JzCOdx9+t10xDp4p/wd\nfrPqN/w82sTvtj5JffPtZKWlsuGRW5nQUctGN4LCZffAGTfql4bIoYjFYPsSb0jXB89AzSbv+Yxh\nUHKSNy1p8XyvJR3KTGytclhqW2vZ0rCFbQ3b2NawjdKGUmrbamlsb6Qp0kRjpJGWaEtncMdcrE/H\nDfqCZKZkereg93NY2jCyUrLITMkkI5hBZkrmXmddB1KiO7Y9CTzgnGszsy8AvwNO23cn59w9wD0A\nCxcudINb4uDw+/wcO+JYxmSO4axHz2JZRoSWxQ8zY/Jkxm28n4cyTua5rCLuK3sIt/ov2JzLE12y\nSHJwDkqXwKpHYc0T3rKb/hRvONcxX4TxJ2s+8SQWjUXZUr+FD6o/4IMa77a+ej27WnbttV9RWhH5\n4XwyUjIYmTGSjGAGacE0Qv4Q4UCY1EBq5/2wP9z5XFZKFhkpGZ0hHfKHEvRJuzeQIb4d6LrW5ij2\ndGADwDlX1eXhr4AfDGA9SWFkxkjmFs7hyfZlXPLOf5H1dgM7LZM/jOtgS/ObPBss5rSXfkC4vdGb\n5WnbW951vEt/B9mjEl2+yNBRsQ5WPuSNx67ZDP4QTDwdFt0KU87W9eokFI1F2VC7gRUVK1hVuYq1\n1WvZWLuR9lg7AAFfgPHZ4zlmxDFMyZvCuKxxjM4cTXFm8ZAL3/4ykCH+DjDJzErwwvty4MquO5jZ\nCOdcWfzhBcCaAawnaZxdcg63V7zHtkCMd7Kv4oUpWWwpewrDuDttKufW/R2e/qq3c9YoaKqAv98G\nF9+T2MJFEqW9CSrWwq41Xqt706vx8dg+7zT5Sf8J086HcFaiK5WDsLNpJysrV7KyYiXvVbzHmuo1\ntERbAMgJ5TA1bypXTL2CKXlTmJw7mfHZ4wkeYZPlDFiIO+eiZvZl4Dm8IWb3OedWm9ltwBLn3BPA\nDWZ2ARAFqoFrBqqeZHLWuLP4wTs/4KlFN5CXls3flv+cT0//NB9Uf8BKtnNK8x945LNzKMgMU205\nND/7LUatuAuO/gKMWpDo8kUOnXPQ3ugtAtLR7j3eLdoKrfVe57OaLVC7FWq37OlJvltKJow7AY6+\nDmZcCJnDB/1jyMFpibZQ1lTGupp1rKtex7qadaypWtN5SjzoCzI1byoXT7qYWQWzmF0wm1GZowal\n9/dQZ84l1yXmhQsXuiVLliS6jAH3uec+x1s73wLgtNGn8cOTf8jD6x7m9rdvp3njf+DvKGLemByW\nba0hpaOZpVn/SXjYJPjs3/Zc24tfC4xufo3ArE96Kx6JJFqkxWs1V2/aE8S1W71grtvmhXVfhHO8\n5TbzxkPRdCia5v3MHQc+zbEwVDS0N7C9cTulDaWUNpSys3knlS2VVLZUUtVSRWVLJY2Rxs79AxZg\nXPY4JudOZnbhbGYVzGJq3lRS/Ed2Z14zW+qcW7jf8wrxoentsrd5ZP0jXDn1SuYWefOt72jcwVmP\nnsW1U79Mddnx/GNjFSdNzuONbcs5uWoV3+j4JZz4H7Dom7DtHXjsizzUvpMf5uVwZUsH//bx3+Ab\ne1yCP5kcUWIdsGM5bH7Nm3q0fLU3k1nXnsCped4fmLtvGcO8096BcHyH+B+lgRTvOnZagRfeuqY9\nJERiEXY27qS0sdS7xcN6e+N2ShtLqWur22v/jGAGBakFe93yU/MZljaMSbmTGJ89/ogP7O4oxD8i\nLn3yUkL+EPefez/17fV8ZfFXeHvn27Tt/AQPpm9hXuUTMOMiomuf5ta8Qp7I8BNozyKaUs+ZTS18\n95QfE57+iUR/DEkmbQ1eJ7HGcm+lrVAWpBd4Ld6MYXv36o60Qvkqb6rRTa95Y693/xLPGQPDZsHw\nmTBsBuRPhOzRuk6dJOra6lhfs571tetZX7OeLfVb2N64nbKmsr2GZwV8AYoziinOKGZUxihGZY7y\n7sd/Zof0x9eh6CnEEz3ETA7SojGLuHP5nVz65KU0R5vZ3ridiTkT+ZBnufzDL/NKiY/hq//KT3On\n8ERGC9PTzufy2V/ktr//hOcLXyB98Ve5Lb0Qxh5/cG/cEfXG12aPhuzigflwMnic8+b5jrZAtM07\nxb37mnP9dm/60J0roGyF10GsJ4FUSM2FlHRvNrPmKryFAvBCfsaF3hCucSdprvAkUddWx6a6Td6t\nfhMbajawrmYd5c3lnftkpmRSkl3CnMI5fHz8xzvDelTGKIrSivDrcsagUYgnmWtmXoOZ8WbZm7RE\nW7j79LspyS7h4scvxj/mEU5cdy2fHD6bJzP+zIT0efz5ku9iZqQGb+JrzzXx14J/cPxfPs3ZF/3e\n6/wD3i/0VY/Cy//r/RJPL4BhM71fvpEWbxjbhhe9X9KhbLjiT9580AOhJt5Rqa3eW90p8NEcFtKv\nmqu94G2tB5x3GjoQhmCq9z1Wb/JCuWZT/P4mr3NYb5Nb5IyB4bNhzuVeyzlzBKTmeO/TVOF1JqvZ\n7LXO25u8a9QZw7x9i+dryONhiHREaIw00uE6BmxssnOO7Y3bWVO9hjVVa3i/+n3WVq2lqnXPyN+g\nL0hJdglHDT+KSbmTmJQziUm5kxiWNkydyoYInU7/iPj71r9z4+IbCTOcxsYsUjI28tiFf6Uku6Rz\nn9ufXcWfN/874fAOHt5exujZV3lTR254EXau9E51Fs/3fkGXvuP9BMgY7gX6hEXw2o+9MPjEz2HW\np/Y+lVpXCo27oHCK1zI7GPU74JmbYO1Te54rmAKfuBPiy7cKXuu5cr3377PtLe9WtaFvr80YDnkl\nXkewzBEQDHst6UDIC/xAGFIyIGukF+Cao3/AxVyMDbUbWFW5ivU169lQu4ENtRuobKnca7/MYCYl\nOSVMyJ7AhJwJjM8ez4ScCQxPH96nmcFiLsbW+q17BfaaqjXUt9cDXmeyCTkTmJo3lUm5kyjJLqEk\nq4SRGSPVqh4idE38CPBW2Vt84/VvsLN5J5+d8a98ZeGNe21vj8b4+F1PUJ7+PUYFjfs3byDfASPm\nwoJrYO6Ve3r1xmJQ+QEE07xf6LvDurka/nwlbP0nTPk4zP6U9wfAxpe8hSMAMK+n8JRzvDWT88Z7\nAeLr5pdNrAPe+RX8/b+91Z8+9hWvld9SA89+zWthnnIznHRT96/vD+1N3gpUvgAUTgN/P56gcg7K\nlntzc+9Y5n1eM+/7yB619y1j+J73bqn1Wrm1W7yWc8Va71pzxQfe0CvwOoSNOdb7IydvgjdNqPm8\n0+K7T48H07zvP3ec1qweIto62nhj+xu8sOUF/rHjH1S3VgMQ9oeZkDOBiTkTGZ05moyUDAIWoL69\nnvLmcjbVbWJj7ca9WsqpgVTGZ49nTNYYwv4wQV+QgC+Az3w0RhqpbatlR+MOtjVs6xxfHfQFmZw7\nmWn505iWN43p+dOZlDvpIzsZykeFQvwIUd9ez9+3/J1zx5/b7f8p15TVc+Gv/0ho1L1Myh7D7Sf8\nNyUF0yhvLmdFxQqyU7KZVTiLzJQ980Zvqd/Ckxuf5NlNzzIlbwo/OOF7BN7+JSz+nhcUvoD3h8C0\n87zA2LXW64285R97Vnzyh7w/BvInQPFCr0VYv92bw3rHMi/sP36H9/xubQ3epDYrHvRm2jrmizBy\nntfir1znXYvNGQsFk3qeMrN2q9dqLV/tdboKhLya2pu94Uy71nj77L6OG0zzJgc59npv+ciux60v\ng3XPeq8Br5f0jAu9Mw/7inV4ZxVeu8MLcfAWykhJ9/5YaSjzxkJ3ZT5vOlDMu1bdVeYI7zT1sBlQ\nNAOKF3jfpU5pJoXa1lreLHuTl7a+xCulr9AcbSY7lM2JxSdy7IhjmVs0l1EZo/rU6q1treXDug/Z\nWLeRD2s/ZGPtRrY1bKM91k40FiUSixBzMdKD6WSlZHV2KpuSO4Xp+dMZnzOeoO/ImhDlo0AhLp0W\nr93F9Y/+kWDxb8Fi+MxPrMvyioYxIWcCswpmsbF2IysqV2AYMwtmsrJyJRdOvJDbjr8Nayjzwqho\nhndqdl/N1bB9GdRu9q5112yOr738wZ59sorhjNtg5iXdB5JzsOQ+eO6WnscPFy+EM76z5zp9tA02\nLvZa+Bte8J7zBbxTx9EWML9Xb+ZIb3Wqwmnez46IF/irHvUuJWSPgeJ5XrBvX7an7lCWF7ht9d51\n5eGzvbWixxwb7529Epb+Huq2en/UHPdlb/u+k47s7kRWV+rd6nd4n9HFvH1zxsZXydJwqoG2q3kX\nqypX8WHdh5Q2lFLfXk9LtIWwP0x6MJ2C1AKK0oooSiuiMK2w8w9kiw9/i8QitEZbvRWwOlqpa6uj\normCzfWb+aDmAzbUbMDhyA3lsmjsIs4YewZHDT9KYSp9phCXvbz8wS6+8MDzdKRsxhfaiYtm42sb\nR9SaSEnfysiiXbT4NjMiYxjnjz+fc0rOISdUwL0r7+KXK37JtTOv5Svzv3JonVtaar3wyh7V93Bq\nb/ICtmyF97qi6V6Ilr0Hr/8f73hpBd6wpfLV0N7gdbJacC1MPddb4KKvneQird582xte9FrRkRbv\nDMDoo73OdrsXy2goh1WPeGcTSt+hszUPXmv+qM9560vrmuKQU9pQyuJti3mv4j3eq3iPnU17liDN\nD+eTHcomHAjT3tFOQ3sDVS1VRF30oN9nWNowJudOZlbhLI4feTwz8mcQ8Kk/sRw8hbjsp7qpnc1V\nTVQ1tjMqN5XJwzJ5f0c9f3p7C48sLcVnxoyRWQR8PsobWtla3czCcblMn/kif9nwMNfNvo4vz/0y\nZoZzjjXVa6hormBkxkhGZ44mHOimdd4L5xxLypfwjx3/4MTiE5lXNK/3PxQiLfDeA/HW8noonOyF\n5/hTB2/J1qZK75p1KNM79Z01cnDeV/qsrq2O5zY/x1MfPsW7u7z+GyPTRzK7cDZzCucwq3AWE7In\nkJGyf9+BmItR3VpNRXMFFS0VRGIRcODi/0vxpRAKhAj7w4T8IbJD2eSn5us6s/QbhbgclG3Vzdz9\nyka2VDUT6YiRn5FCUWaY+9/cwtElOUyY9jce3/hXjhl+DEF/kHU169jVvGfpP7/5mZw7men50xmZ\nMZLx2eM5fuTxpAXTenzPFRUr+O83/5u11Ws7n5uYM5FbjrmFo4Yftde+zjkNcUkSzjlq22qpaauh\nob2h89YUaaI50kxztJmWaEvn/ZA/REFqASXZJcwrmsfw9EOf+7yto41XS1/lqY1P8er2V4nGoozP\nHs/5E7yzS8UZmvNAkoNCXPrFX5aV8u8Pvcex43OZMfM13q18ixRfClmBInytMyBSwLC8FkJp5Wys\nf5/1NeupaasBvJ60p485nS/O+SJjsvaex/3hdQ/zv2/9LwWpBVw3+zoWjVnE4m2L+dXKX1HaUMpV\n067i5NEnE/aHeWzDYzz94dN0uA6yQ9lcO+NaPj3904cc6s2RZh7f+DhVLVVMyp3EgmELKEgtOOzv\nCrxrrc9vfp7SxlKqW6uZnDuZeUXzmF04+7Cuh7Z3tLO5fjMbarwhSc3RZvzmJzMlk2FpwxiePpzh\n6cMJ+UM0tDdQ315PQ3sD0Vi0s6U4LmscOeH+H0a2q3kX7+56l+W7lrOiYgWb6jbREGk44GtSfCmk\nBdNIDaTS1tFGTWsNLn55ojijmPlF85k3bB5zCucwIXvCATuANUeaWVK+hJe2vsTzW56nob2BgtQC\nzi05l/PGn8fUvKn6A1CSjkJc+s2jS0v5r8dWkp4S4Izpw1iypYYNuxrxGWSEAtS3RvH7jAvmjOQL\nJ49nbEGQVZWreHbTszz14VNEOiJ8YuInWDBsAX7z88DaB1hesZwTRp7A90/6/l7TMjZHmrlj6R08\n+MGDnc+F/CHOHnc2eal5rK1ayz/L/sk5487h28d/+4At/X01R5r5/fu/5/73vSlsfeYj5mKkBlL5\n0twvcdW0qw75+uWHdR/y21W/5ckPnyQai3b2FC5r8lbezQhmcPzI45ldOJspeVPICGbgMx9NkSbq\n2+qpb6+nrq2u25+1bbXsbNpJR7wzot/8pAXTiMaincOI+ionlPP/2rvz4DiqO4Hj399MzylpRpct\n67LlCxvjcJjgQEKALFkCWQIhCZBjQw5q2WzuTYUkW9nNZlOV2pBUtnYDCbk4E3ZDyLFxsllgSQgk\nEMDGBzb4tmTJwrItaXSO5up++0e3hpEs2bKxNBr596mamu43Pd3vTc/Mr9/r7vdoibXQEm+hJdZC\nY0UjUSuK5bMYyY3ka8gjuRGSuSQZO0PIHyJqRYkEIoT8IYazwyRSCXYndrO9dzudQ52Ae8vUWbVn\nsbxyOQtjC6kJ11ARrKAiWEEsGKMsUEYkECFiRY46oMk6WXYldrHx0EY2Hd7E84eez9+KFfAFaCxv\npK6sjlgwRsSKkHNy+YE22gfayZlc/qDx6iVXs7Z+rZ6LViVNg7g6pXYfGuTTD26mrXuY17ZUc8kZ\n83jbOfXUloXYfXiIhzZ08MCz7Yxkbc5pinP9a5t555omhu0E39n8HX6999ekbPdq86byJm466yZu\nOOOGSWtYXcNddAx2kEglWLtgbb4GaYzhrm13cfum26mN1HLrBbdyxaIr8h1gOMbBGINPfPlz920D\nbTz98tPcve1uDicPc1nzZdy8+mbOrDmT3Ynd3LnlTp488CSL44u5ccWNXFR/EUPZIdoG2ljftZ7W\n/laMMUQCEc6udUdZaihvIOALsLV7K79v/z2PdzxO0B/kumXX8f5V76e5ohkRIZFKsPHQRp448ARP\nvfzUmFMQE/GLn3goQs585wAAFOhJREFUTiwYIxaKuc/ebUPLq5aztHIpLbGW/IARaTvN4eHDdCW7\n6BruIutkiQVj+eBp+SwydobeVC+t/a20DbS5z/1tY+4/nowl1qQXeC2KLWJl9UrOrj2b8+afx8qa\nlafs6mtjDB2DHWw5soXdfbs5MHiAw8nDDGQGSOfSWD6LiBWhqaKJJfElrK1fy7nzzj2p6zKUmo00\niKtp4TgGn2/ipsnEcIZfbOrkoQ0d7OgapCoa4AOvb+GWS5YQtKCtv41EOsGa+Wteda9QW45s4avP\nfJXtvduxfBZ10bp8s2xhjVVEyDluEFpds5pbL7iVNXVrxqzLGMNj7Y9xz7Z72Nq9dcxr8VCclVUr\nsXwWvalediV25dc/qjJUyY0rbuS9Z76X6nD1MfPdPdLN3r69pHIpbGNTFijLB+14KE7Uis5Y0+9A\nZoCu4S5SuRRZJ0vEihC1okQDUbfmbUXw+/z5Gn8y69bMo4EosWCMgF9vl1JqumgQV0VjjGHD/gTf\ne2Ifj20/REM8zOevWsmVqxcQstzgvb9nmN+8cJBn9vUQsvzMqwjx0cuW0lw99eZx27F5uO1hdid2\n05XsIuwPUx2uJuAP4BgH27GxjU1jeSMX1l+Yrx0fy/ae7ezp20M8FGdB2QKWVS4b081lMptkT98e\nN/jZKVZVu51pTKUrTKWUmioN4mpWWN/Wyz/99zZ2dA1SHrJY3RijtXuYQwNpAM6sd4elbOsexvIJ\nX33Ha7jmHL1dSyl1etMgrmaNnO3wxz3dPLKti+0HB1g6v5zXNMa54qwFNFZGAGjvSfKpBzexqb2P\nd53fxJevOYvykF6YpJQ6PWkQVyUnZzt863e7uePxPTRWRXjD0lri0QBbOvrY1N5HQ2WEs5vi3HLJ\nEs5q0G5JlVJzlwZxVbKea+3l6w/voK0nSe9wmjPrY7x2URVdAymebe0lmbH50tWreN/rFur9v0qp\nOUmDuJoTxl8N3zOU5jM/3cITu45wTlOcD1+8mObqKIOpHAcSSfb3JAn6fdRXhjm3uZJV9TEN9Eqp\nkqNBXM1ZjmN4cEMHP3hyH/u6h8e8FrJ85ByD7bjf80U1Ud60Yj4XLqmmqSqKiDsSlQg0VEaIR/Q2\nKaXU7KNBXM15jmN4trWXVM6mImTRUBlhQSyMYwxdAyme3NXNwy928VxrD6msc9T7g5aPt5y1gA+9\noYU1C6uKUAKllJqYBnGlPJmcw9bOPo4MZgCDMeAYeK61h19u6mQgleMDFy3ic1eupGzcFfHGGNI5\nh3BAhxdVSs0cDeJKTcFwOsc3HtnJfX9uozIS4KrX1POaxjhd/Sl2dg3yXFsvvcMZKqMBVtRV8MnL\nl/OGZadmsBSllJqMBnGlTsDz+3u59+n9PPbSIUayNiLQVBVhbUsNLTVRDg2m+P32w7zcn+KNy2v5\n/JUrWd2ot7kppaaHBnGlTkIyk6NnKENdLEzQGtuVaipr8+Nn9nPH43voS2b5q7PrueacBi5eVptv\nhs/knPw5er0qXil1sjSIKzVN+keyfO+Jvfzomf0MptzBVSyf4PcJ6Zx7AV3I8tFQGeHCJdVcesY8\n3rCsloqwXgmvlJoaDeJKTbOs7bC+tZeN7QlGsjY521AesggH/HQPpdnXPcwze3sYTOewfML5i6q4\ndMU8Lj1jHivqKrD8OmiKUmpiGsSVmgWytsPG/Qn+sOsIf9h5hO0HBwAI+n0sri2jsSpCXSxEXSzM\ngliY5XUVrKqPEQnq1fBKnc40iCs1Cx0aSPH03m52dA2y9/AQB/tTHBpI0T2UyS/jE1g2v5zVDXFW\nNcRYVR9jUW0Z8ytCBLT2rtRpYbIgrsNCKVVEdbEw153XdFR6JudwaCDFjq5Btnb282JnP3/a080v\nNnXmlxGB2vIQ9fEwK+oqeP2yGi5aUsuCeHgmi6CUKiKtiStVQrqH0uw4OMiBRDJfa+/sG2FrZz99\nySwAS2rLuGhpDa9fWsuKBeWELD8ikM45CBCPBIhHAnoOXqkSojVxpeaA2vIQFy8PHZXuOIbtXQP8\neW8PT+/t4VebX+aBZ9snXU/I8rG6Mc45TZWcu7CS85oraaqK6G1wSpUYrYkrNQflbIetnf10JEZI\nZ22MgVDAh2MMAyM52nuTbOnoY2tnf/42uNryEOc2x1k6r5yFNVFqykLUlAepLgtSUxYkFg6MGUFO\nKTVztCau1GnE8vs4b2EV5x1nIJes7bCza5BNHX1s2p9g28v9PLmrm4x99AAxfp8wvyLE6sY45y+q\nYs3CKs5uims/8koVkdbElVJj2I7h8GCKnqEMvcPuo2c4Q+9wmpf7Umzu6KPVG/LV8gmrGmKc21xJ\nXSxMZTRAVTRIZTRATVmI6rIgVVE9/67Uq6U1caXUlPh9Qn08Qn08MukyPUNpNrX3sbE9wcb2BL/Y\n2MlQOjfhsiJQGQnQXB1lcW3ZUQ/tuU6pk6c1caXUKZHK2vSPZEkkMySGs14tPk33UIbuoTTtvUla\nu4fp7Buh8G+nImRRWxGitjxITVmI2opg/ta55uoozVVR6uNhrc2r05rWxJVS0yoc8BMO+KmLHfs+\n9VTWzgf01u5huvpTdA+l6R5Ks+fIEM+0pvO3y42yfEJDZYTm6ggN8QjzYyHmlYeYHwtTFwvTXBWh\ntjykF96p044GcaXUjAoH/JxRV8EZdRWTLpPJOXT1p+hIJOnoTdLem6QjMUJ7b5Indx+heyiD7Yxt\nRQxaPhorIzRVuY/6eITqsqB3Xt57LnPP2WtPd2qu0CCulJp1gpaPhTVRFtZEJ3zdcQy9yQxHBtMc\n7B+hMzHCgfwjyaMvD9AznJnwveA24VeVBakqC1IdDXjP3rwX9KuiAS/wB6nUznHULKVBXClVcnw+\nobY8RG15iDPrYxMuk87Z9CXdc/OJ4Qy9Sfc5MZqWdK+8PzKUZtehIXqHM4xk7Um3GQtb+aBeHQ1S\nGQ1SHvLnTyNEgn7ikUC+9q/316uZoEFcKTUnhSw/dbHjn6MvlMra+QCfGM4WBP7RA4EsieEMXQMp\nth8cYDhjM5K1yeSOvq9+lN8n+Vr92EeI6miAymiQeDRAZSRARThAOOAj4h0UhC2/HgCoY9IgrpRS\nnnDAT0NlhIbKyW+vm4jtmPzV+YX31fcOZ73nV+6539E1SGI4Q99IlqncHBS0fJQF/WOb/L3neCRA\n0PIR9AsBv899jJ/3+whaQsgabTXw5VsPwpZPTxOUuGkN4iJyJfAfgB/4oTHma+NeDwH3A+cDPcCN\nxpi26cyTUkqdan6fUBayKAtZUz4AyNkOA6kcfUk3oPcnswyksqSzDiNZt4af8p6HUrn8qYEOr8vc\nRDJD1n71twgH/ELY8hPyAnwkMDbYh6zRVgHf0QcBo/MFBwiRwCvrct//ysGE5ReCfh+WT/D7RPvq\nPwWmLYiLiB/4NvCXwAFgvYisM8a8VLDYzUDCGLNMRN4N3AbcOF15Ukqp2cLy+/JN6yfDGMNI1iab\nM2Rsh2zBI5Mz5JxXpjO2Q8o7KEhnHVI5m5GMTcqbdl97ZZnR+aF0jiODadI5Z0x6KmdPqRXheIJe\nYPf7xAvsPvw+sHy+gjTJtzCE/D4CljsfzLc6uAcFVsHBQcDvritQsO7C10e3lZ/3y9ht+sduuzAv\nls/nLf/KOq2C9NH3+IQZOUiZzpr4WmCPMWYfgIj8BLgWKAzi1wJf9qZ/BtwhImJKrQcapZSaYSJC\nNGjByR0DvCrGGNI5J39AMBrcR8YdBKRz7kFD1nHI5hxyjiFrG7K2Q852yNiGnO1gG4PtGHKOwbbN\nmPnc6IGJbcjmHFJZh8FUjkzOIWM7ZHLOmGVzjvde2z2QcYoUTbb88xXEI9PfG+F0BvFGoKNg/gDw\nusmWMcbkRKQfqAG6pzFfSimlXgURyTenx5nd3eY6owcHjhvU7YL5rD12Pme/slxu3LztHYDkX/em\ns2Nec/LLhAMzc61BSVzYJiK3ALd4s0MisvMUrr6WuXPQoGWZnbQss5OWZXaaE2X5lPt0KsuyaKLE\n6QzinUBzwXyTlzbRMgdExALiuBe4jWGM+T7w/enIpIhsmKg/2lKkZZmdtCyzk5ZldtKynJjprO+v\nB5aLyGIRCQLvBtaNW2Yd8AFv+l3A7/V8uFJKKTU101YT985xfxx4BPcWs7uNMS+KyFeADcaYdcBd\nwI9EZA/QixvolVJKKTUF03pO3BjzW+C349K+VDCdAq6fzjxMwbQ00xeJlmV20rLMTlqW2UnLcgJK\nbjxxpZRSSrm0vz2llFKqRJ3WQVxErhSRnSKyR0S+UOz8nAgRaRaRx0XkJRF5UUQ+5aV/WUQ6RWSz\n93hrsfM6FSLSJiJbvTxv8NKqReT/RGS391xV7Hwej4isKPjsN4vIgIh8ulT2i4jcLSKHRWRbQdqE\n+0Fc3/J+Py+IyJri5fxok5TlGyKyw8vvL0Wk0ktvEZGRgv3z3eLl/GiTlGXS75SI/IO3X3aKyFuK\nk+uJTVKWBwvK0SYim7302b5fJvsfnrnfjDHmtHzgXmy3F1iC2+fRFmBVsfN1AvmvB9Z40xXALmAV\nbg94ny12/k6iPG1A7bi0rwNf8Ka/ANxW7HyeYJn8QBfu/Z0lsV+AS4A1wLbj7QfgrcD/AgJcCDxb\n7PxPoSxXAJY3fVtBWVoKl5ttj0nKMuF3yvsf2AKEgMXe/5y/2GU4VlnGvf5N4Eslsl8m+x+esd/M\n6VwTz3cLa4zJAKPdwpYEY8xBY8xGb3oQ2I7bA95cci1wnzd9H/D2IublZFwO7DXG7C92RqbKGPMk\n7p0ihSbbD9cC9xvXM0CliNTPTE6Pb6KyGGMeNcbkvNlncPuvmPUm2S+TuRb4iTEmbYxpBfbg/t/N\nCscqi4gIcAPwXzOaqZN0jP/hGfvNnM5BfKJuYUsyCIpIC3Ae8KyX9HGvqebuUmiC9hjgURF5Xtwe\n+gDqjDEHvekuoK44WTtp72bsn1Ep7heYfD+U+m/ow7i1olGLRWSTiDwhIm8sVqZO0ETfqVLeL28E\nDhljdheklcR+Gfc/PGO/mdM5iM8JIlIO/Bz4tDFmALgTWAqcCxzEbZoqBRcbY9YAVwEfE5FLCl80\nbltUydxKIW4HR9cAD3lJpbpfxii1/TAZEfkikAMe8JIOAguNMecBnwH+U0RixcrfFM2J79Q472Hs\ngW9J7JcJ/ofzpvs3czoH8al0CzuriUgA94vzgDHmFwDGmEPGGNsY4wA/YBY1ox2LMabTez4M/BI3\n34dGm5q858PFy+EJuwrYaIw5BKW7XzyT7YeS/A2JyAeBq4H3eX+weE3PPd7087jnkc8oWian4Bjf\nqVLdLxbwDuDB0bRS2C8T/Q8zg7+Z0zmIT6Vb2FnLO3d0F7DdGPNvBemF51euA7aNf+9sIyJlIlIx\nOo178dE2xnbL+wHgV8XJ4UkZU6Moxf1SYLL9sA64ybvi9kKgv6AJcVYSkSuBzwHXGGOSBenzRMTv\nTS8BlgP7ipPLqTnGd2od8G4RCYnIYtyyPDfT+TsJbwZ2GGMOjCbM9v0y2f8wM/mbKfbVfcV84F4p\nuAv36O6Lxc7PCeb9YtwmmheAzd7jrcCPgK1e+jqgvth5nUJZluBeTbsFeHF0X+AOS/s7YDfwGFBd\n7LxOsTxluAP5xAvSSmK/4B54HASyuOfrbp5sP+BeYftt7/ezFXhtsfM/hbLswT0nOfqb+a637Du9\n795mYCPwtmLnfwplmfQ7BXzR2y87gauKnf/jlcVLvxf4yLhlZ/t+mex/eMZ+M9pjm1JKKVWiTufm\ndKWUUqqkaRBXSimlSpQGcaWUUqpEaRBXSimlSpQGcaWUUqpEaRBXqkhExIjINwvmPysiXz5F675X\nRN51KtZ1nO1cLyLbReTx6d7WuO1+UETumMltKjUbaRBXqnjSwDtEpLbYGSnk9Zw1VTcDf2OMedN0\n5UcpNTkN4koVTw74PvD3418YX5MWkSHv+TJvIIhficg+EfmaiLxPRJ4Tdzz2pQWrebOIbBCRXSJy\ntfd+v7hjaq/3Bs7424L1/lFE1gEvTZCf93jr3yYit3lpX8Lt7OIuEfnGBO+5tWA7/+KltYg7nvcD\nXg3+ZyIS9V673BvoYqs3oEfIS79ARJ4WkS1eOSu8TTSIyMPijtn89YLy3evlc6uIHPXZKjWXnMgR\nt1Lq1Ps28MJoEJqic4AzcYdz3Af80BizVkQ+BXwC+LS3XAtuf9pLgcdFZBlwE25Xjxd4QfIpEXnU\nW34NsNq4w1fmiUgD7tjb5wMJ3NHm3m6M+YqI/AXumNYbxr3nCtwuMtfi9lK1zhvUph1YgdtL11Mi\ncjfwUa9p/F7gcmPMLhG5H/g7EfkObl/aNxpj1os7+MWIt5lzcUeNSgM7ReR2YD7QaIxZ7eWj8gQ+\nV6VKjtbElSoi4454dD/wyRN423rjjmOcxu2+cTQIb8UN3KN+aoxxjDus4z5gJW6/9DeJyGbcIRNr\ncIMtwHPjA7jnAuAPxpgjxh2L+wHgkgmWK3SF99iE213myoLtdBhjnvKmf4xbm18BtBpjdnnp93nb\nWAEcNMasB/fzMq+MB/47Y0y/MSaF23qwyCvnEhG53esnfcyIUkrNNVoTV6r4/h030N1TkJbDO8gW\nER8QLHgtXTDtFMw7jP1Nj+9T2eDWij9hjHmk8AURuQwYPrnsT0iAfzXGfG/cdlomydfJKPwcbMAy\nxiRE5BzgLcBHgBtwxw1Xak7SmrhSRWaM6QV+inuR2Kg23OZrcMclD5zEqq8XEZ93nnwJ7mAYj+A2\nUwcAROQMb+S4Y3kOuFREar0Rpd4DPHGc9zwCfFjccZYRkUYRme+9tlBELvKm3wv8yctbi9fkD/B+\nbxs7gXoRucBbT8WxLrzzLhL0GWN+Dvwj7ikCpeYsrYkrNTt8E/h4wfwPgF+JyBbgYU6ultyOG4Bj\nuKNDpUTkh7hN7hu9YRSPAG8/1kqMMQdF5AvA47g17P8xxhxzWFhjzKMicibwZ3czDAF/jVtj3gl8\nzDsf/hJwp5e3DwEPeUF6Pe4IYxkRuRG4XUQiuOfD33yMTTcC93itFwD/cKx8KlXqdBQzpdSM8ZrT\nfzN64ZlS6tXR5nSllFKqRGlNXCmllCpRWhNXSimlSpQGcaWUUqpEaRBXSimlSpQGcaWUUqpEaRBX\nSimlSpQGcaWUUqpE/T9+QrjhYfL+pgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FChTbWud8Zqg",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H5WISDr_uvx",
        "colab_type": "code",
        "outputId": "922cb864-7b05-4412-abfc-29fec8f3bc48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "acc_train_100, acc_val_100, acc_test_100, loss_train_100, loss_val_100, loss_test_100 = gradient_descent(200, 100)\n",
        "acc_train_500, acc_val_500, acc_test_500, loss_train_500, loss_val_500, loss_test_500 = gradient_descent(200, 500)\n",
        "acc_train_2000, acc_val_2000, acc_test_2000, loss_train_2000, loss_val_2000, loss_test_2000 = gradient_descent(200, 2000)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************************************************************** 0\n",
            "what is the tr accuracy:  0.11553333333333334\n",
            "what is the v accuracy:  0.134\n",
            "what is the t accuracy:  0.1130690161527166\n",
            "what is the loss  2.58915010814696\n",
            "what is the loss v  2.554886024173392\n",
            "what is the loss test 2.6103203336535787\n",
            "************************************************************** 0\n",
            "what is the tr accuracy:  0.11553333333333334\n",
            "what is the v accuracy:  0.134\n",
            "what is the t accuracy:  0.1130690161527166\n",
            "what is the loss  2.58915010814696\n",
            "what is the loss v  2.554886024173392\n",
            "what is the loss test 2.6103203336535787\n",
            "************************************************************** 1\n",
            "what is the tr accuracy:  0.24373333333333333\n",
            "what is the v accuracy:  0.26\n",
            "what is the t accuracy:  0.24192364170337738\n",
            "what is the loss  2.1419081900638\n",
            "what is the loss v  2.1516103473391386\n",
            "what is the loss test 2.1352502444759853\n",
            "************************************************************** 1\n",
            "what is the tr accuracy:  0.24373333333333333\n",
            "what is the v accuracy:  0.26\n",
            "what is the t accuracy:  0.24192364170337738\n",
            "what is the loss  2.1419081900638\n",
            "what is the loss v  2.1516103473391386\n",
            "what is the loss test 2.1352502444759853\n",
            "************************************************************** 2\n",
            "what is the tr accuracy:  0.4368\n",
            "what is the v accuracy:  0.429\n",
            "what is the t accuracy:  0.4423641703377386\n",
            "what is the loss  1.742195356370411\n",
            "what is the loss v  1.7769039078850335\n",
            "what is the loss test 1.7201001945294998\n",
            "************************************************************** 2\n",
            "what is the tr accuracy:  0.4368\n",
            "what is the v accuracy:  0.429\n",
            "what is the t accuracy:  0.4423641703377386\n",
            "what is the loss  1.742195356370411\n",
            "what is the loss v  1.7769039078850335\n",
            "what is the loss test 1.7201001945294998\n",
            "************************************************************** 3\n",
            "what is the tr accuracy:  0.5844\n",
            "what is the v accuracy:  0.602\n",
            "what is the t accuracy:  0.6031571218795888\n",
            "what is the loss  1.3103494477797912\n",
            "what is the loss v  1.3265321814006212\n",
            "what is the loss test 1.295846292324076\n",
            "************************************************************** 3\n",
            "what is the tr accuracy:  0.5844\n",
            "what is the v accuracy:  0.602\n",
            "what is the t accuracy:  0.6031571218795888\n",
            "what is the loss  1.3103494477797912\n",
            "what is the loss v  1.3265321814006212\n",
            "what is the loss test 1.295846292324076\n",
            "************************************************************** 4\n",
            "what is the tr accuracy:  0.6696\n",
            "what is the v accuracy:  0.669\n",
            "what is the t accuracy:  0.6740088105726872\n",
            "what is the loss  1.0476176013516711\n",
            "what is the loss v  1.0221363029377677\n",
            "what is the loss test 1.0350904634058913\n",
            "************************************************************** 4\n",
            "what is the tr accuracy:  0.6696\n",
            "what is the v accuracy:  0.669\n",
            "what is the t accuracy:  0.6740088105726872\n",
            "what is the loss  1.0476176013516711\n",
            "what is the loss v  1.0221363029377677\n",
            "what is the loss test 1.0350904634058913\n",
            "************************************************************** 5\n",
            "what is the tr accuracy:  0.7306\n",
            "what is the v accuracy:  0.746\n",
            "what is the t accuracy:  0.7338472834067548\n",
            "what is the loss  0.9152576258055422\n",
            "what is the loss v  0.8816451070543464\n",
            "what is the loss test 0.8951785757404411\n",
            "************************************************************** 5\n",
            "what is the tr accuracy:  0.7306\n",
            "what is the v accuracy:  0.746\n",
            "what is the t accuracy:  0.7338472834067548\n",
            "what is the loss  0.9152576258055422\n",
            "what is the loss v  0.8816451070543464\n",
            "what is the loss test 0.8951785757404411\n",
            "************************************************************** 6\n",
            "what is the tr accuracy:  0.7571333333333333\n",
            "what is the v accuracy:  0.751\n",
            "what is the t accuracy:  0.7624816446402349\n",
            "what is the loss  0.7924270833386458\n",
            "what is the loss v  0.8032552611401611\n",
            "what is the loss test 0.7801598519667287\n",
            "************************************************************** 6\n",
            "what is the tr accuracy:  0.7571333333333333\n",
            "what is the v accuracy:  0.751\n",
            "what is the t accuracy:  0.7624816446402349\n",
            "what is the loss  0.7924270833386458\n",
            "what is the loss v  0.8032552611401611\n",
            "what is the loss test 0.7801598519667287\n",
            "************************************************************** 7\n",
            "what is the tr accuracy:  0.7637333333333334\n",
            "what is the v accuracy:  0.755\n",
            "what is the t accuracy:  0.7767988252569751\n",
            "what is the loss  0.7947883550382056\n",
            "what is the loss v  0.8290657652970052\n",
            "what is the loss test 0.7670948784352171\n",
            "************************************************************** 7\n",
            "what is the tr accuracy:  0.7637333333333334\n",
            "what is the v accuracy:  0.755\n",
            "what is the t accuracy:  0.7767988252569751\n",
            "what is the loss  0.7947883550382056\n",
            "what is the loss v  0.8290657652970052\n",
            "what is the loss test 0.7670948784352171\n",
            "************************************************************** 8\n",
            "what is the tr accuracy:  0.7361333333333333\n",
            "what is the v accuracy:  0.739\n",
            "what is the t accuracy:  0.7426578560939795\n",
            "what is the loss  0.9313068385204354\n",
            "what is the loss v  0.9508358114649138\n",
            "what is the loss test 0.9016910942938808\n",
            "************************************************************** 8\n",
            "what is the tr accuracy:  0.7361333333333333\n",
            "what is the v accuracy:  0.739\n",
            "what is the t accuracy:  0.7426578560939795\n",
            "what is the loss  0.9313068385204354\n",
            "what is the loss v  0.9508358114649138\n",
            "what is the loss test 0.9016910942938808\n",
            "************************************************************** 9\n",
            "what is the tr accuracy:  0.8248666666666666\n",
            "what is the v accuracy:  0.821\n",
            "what is the t accuracy:  0.830029368575624\n",
            "what is the loss  0.7185997350128741\n",
            "what is the loss v  0.7050909717459808\n",
            "what is the loss test 0.7015544013301862\n",
            "************************************************************** 9\n",
            "what is the tr accuracy:  0.8248666666666666\n",
            "what is the v accuracy:  0.821\n",
            "what is the t accuracy:  0.830029368575624\n",
            "what is the loss  0.7185997350128741\n",
            "what is the loss v  0.7050909717459808\n",
            "what is the loss test 0.7015544013301862\n",
            "************************************************************** 10\n",
            "what is the tr accuracy:  0.7898666666666667\n",
            "what is the v accuracy:  0.785\n",
            "what is the t accuracy:  0.8013950073421439\n",
            "what is the loss  0.8438378357612737\n",
            "what is the loss v  0.8172103100456473\n",
            "what is the loss test 0.8111374729844596\n",
            "************************************************************** 10\n",
            "what is the tr accuracy:  0.7898666666666667\n",
            "what is the v accuracy:  0.785\n",
            "what is the t accuracy:  0.8013950073421439\n",
            "what is the loss  0.8438378357612737\n",
            "what is the loss v  0.8172103100456473\n",
            "what is the loss test 0.8111374729844596\n",
            "************************************************************** 11\n",
            "what is the tr accuracy:  0.8082666666666667\n",
            "what is the v accuracy:  0.805\n",
            "what is the t accuracy:  0.816079295154185\n",
            "what is the loss  0.8599957935922862\n",
            "what is the loss v  0.8789883550624205\n",
            "what is the loss test 0.8439039744202871\n",
            "************************************************************** 11\n",
            "what is the tr accuracy:  0.8082666666666667\n",
            "what is the v accuracy:  0.805\n",
            "what is the t accuracy:  0.816079295154185\n",
            "what is the loss  0.8599957935922862\n",
            "what is the loss v  0.8789883550624205\n",
            "what is the loss test 0.8439039744202871\n",
            "************************************************************** 12\n",
            "what is the tr accuracy:  0.8176\n",
            "what is the v accuracy:  0.81\n",
            "what is the t accuracy:  0.82856093979442\n",
            "what is the loss  0.9508558388573051\n",
            "what is the loss v  1.0082330616596629\n",
            "what is the loss test 0.9337752935181481\n",
            "************************************************************** 12\n",
            "what is the tr accuracy:  0.8176\n",
            "what is the v accuracy:  0.81\n",
            "what is the t accuracy:  0.82856093979442\n",
            "what is the loss  0.9508558388573051\n",
            "what is the loss v  1.0082330616596629\n",
            "what is the loss test 0.9337752935181481\n",
            "************************************************************** 13\n",
            "what is the tr accuracy:  0.8305333333333333\n",
            "what is the v accuracy:  0.827\n",
            "what is the t accuracy:  0.8340675477239354\n",
            "what is the loss  0.8429556990836727\n",
            "what is the loss v  0.8459206173954458\n",
            "what is the loss test 0.8380676172832797\n",
            "************************************************************** 13\n",
            "what is the tr accuracy:  0.8305333333333333\n",
            "what is the v accuracy:  0.827\n",
            "what is the t accuracy:  0.8340675477239354\n",
            "what is the loss  0.8429556990836727\n",
            "what is the loss v  0.8459206173954458\n",
            "what is the loss test 0.8380676172832797\n",
            "************************************************************** 14\n",
            "what is the tr accuracy:  0.8177333333333333\n",
            "what is the v accuracy:  0.82\n",
            "what is the t accuracy:  0.8270925110132159\n",
            "what is the loss  0.921121079247085\n",
            "what is the loss v  0.9001375785713938\n",
            "what is the loss test 0.8904852789230137\n",
            "************************************************************** 14\n",
            "what is the tr accuracy:  0.8177333333333333\n",
            "what is the v accuracy:  0.82\n",
            "what is the t accuracy:  0.8270925110132159\n",
            "what is the loss  0.921121079247085\n",
            "what is the loss v  0.9001375785713938\n",
            "what is the loss test 0.8904852789230137\n",
            "************************************************************** 15\n",
            "what is the tr accuracy:  0.8518666666666667\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.856093979441997\n",
            "what is the loss  0.7918252347241391\n",
            "what is the loss v  0.7283253308799178\n",
            "what is the loss test 0.7590922226799689\n",
            "************************************************************** 15\n",
            "what is the tr accuracy:  0.8518666666666667\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.856093979441997\n",
            "what is the loss  0.7918252347241391\n",
            "what is the loss v  0.7283253308799178\n",
            "what is the loss test 0.7590922226799689\n",
            "************************************************************** 16\n",
            "what is the tr accuracy:  0.8338666666666666\n",
            "what is the v accuracy:  0.829\n",
            "what is the t accuracy:  0.8366372980910426\n",
            "what is the loss  0.8893074951765232\n",
            "what is the loss v  0.8292150714681403\n",
            "what is the loss test 0.8689195476381822\n",
            "************************************************************** 16\n",
            "what is the tr accuracy:  0.8338666666666666\n",
            "what is the v accuracy:  0.829\n",
            "what is the t accuracy:  0.8366372980910426\n",
            "what is the loss  0.8893074951765232\n",
            "what is the loss v  0.8292150714681403\n",
            "what is the loss test 0.8689195476381822\n",
            "************************************************************** 17\n",
            "what is the tr accuracy:  0.8312\n",
            "what is the v accuracy:  0.825\n",
            "what is the t accuracy:  0.8355359765051396\n",
            "what is the loss  0.8878154095141101\n",
            "what is the loss v  0.8361903514754513\n",
            "what is the loss test 0.8741041586426177\n",
            "************************************************************** 17\n",
            "what is the tr accuracy:  0.8312\n",
            "what is the v accuracy:  0.825\n",
            "what is the t accuracy:  0.8355359765051396\n",
            "what is the loss  0.8878154095141101\n",
            "what is the loss v  0.8361903514754513\n",
            "what is the loss test 0.8741041586426177\n",
            "************************************************************** 18\n",
            "what is the tr accuracy:  0.846\n",
            "what is the v accuracy:  0.84\n",
            "what is the t accuracy:  0.8509544787077826\n",
            "what is the loss  0.784038863533174\n",
            "what is the loss v  0.7450728318144786\n",
            "what is the loss test 0.7784866486570581\n",
            "************************************************************** 18\n",
            "what is the tr accuracy:  0.846\n",
            "what is the v accuracy:  0.84\n",
            "what is the t accuracy:  0.8509544787077826\n",
            "what is the loss  0.784038863533174\n",
            "what is the loss v  0.7450728318144786\n",
            "what is the loss test 0.7784866486570581\n",
            "************************************************************** 19\n",
            "what is the tr accuracy:  0.8461333333333333\n",
            "what is the v accuracy:  0.845\n",
            "what is the t accuracy:  0.855359765051395\n",
            "what is the loss  0.7085662720068316\n",
            "what is the loss v  0.6971155390684999\n",
            "what is the loss test 0.7091167876609981\n",
            "************************************************************** 19\n",
            "what is the tr accuracy:  0.8461333333333333\n",
            "what is the v accuracy:  0.845\n",
            "what is the t accuracy:  0.855359765051395\n",
            "what is the loss  0.7085662720068316\n",
            "what is the loss v  0.6971155390684999\n",
            "what is the loss test 0.7091167876609981\n",
            "************************************************************** 20\n",
            "what is the tr accuracy:  0.8336666666666667\n",
            "what is the v accuracy:  0.839\n",
            "what is the t accuracy:  0.841042584434655\n",
            "what is the loss  0.7522465138757642\n",
            "what is the loss v  0.7596467740323717\n",
            "what is the loss test 0.7450196952437611\n",
            "************************************************************** 20\n",
            "what is the tr accuracy:  0.8336666666666667\n",
            "what is the v accuracy:  0.839\n",
            "what is the t accuracy:  0.841042584434655\n",
            "what is the loss  0.7522465138757642\n",
            "what is the loss v  0.7596467740323717\n",
            "what is the loss test 0.7450196952437611\n",
            "************************************************************** 21\n",
            "what is the tr accuracy:  0.8546666666666667\n",
            "what is the v accuracy:  0.854\n",
            "what is the t accuracy:  0.8535242290748899\n",
            "what is the loss  0.7135342314469745\n",
            "what is the loss v  0.7256558049181309\n",
            "what is the loss test 0.711838382073094\n",
            "************************************************************** 21\n",
            "what is the tr accuracy:  0.8546666666666667\n",
            "what is the v accuracy:  0.854\n",
            "what is the t accuracy:  0.8535242290748899\n",
            "what is the loss  0.7135342314469745\n",
            "what is the loss v  0.7256558049181309\n",
            "what is the loss test 0.711838382073094\n",
            "************************************************************** 22\n",
            "what is the tr accuracy:  0.8622666666666666\n",
            "what is the v accuracy:  0.862\n",
            "what is the t accuracy:  0.8645374449339207\n",
            "what is the loss  0.6316101829546884\n",
            "what is the loss v  0.6360212011280543\n",
            "what is the loss test 0.6319498421211655\n",
            "************************************************************** 22\n",
            "what is the tr accuracy:  0.8622666666666666\n",
            "what is the v accuracy:  0.862\n",
            "what is the t accuracy:  0.8645374449339207\n",
            "what is the loss  0.6316101829546884\n",
            "what is the loss v  0.6360212011280543\n",
            "what is the loss test 0.6319498421211655\n",
            "************************************************************** 23\n",
            "what is the tr accuracy:  0.8536\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8575624082232012\n",
            "what is the loss  0.5898735117827938\n",
            "what is the loss v  0.5894550727272954\n",
            "what is the loss test 0.5928954308255869\n",
            "************************************************************** 23\n",
            "what is the tr accuracy:  0.8536\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8575624082232012\n",
            "what is the loss  0.5898735117827938\n",
            "what is the loss v  0.5894550727272954\n",
            "what is the loss test 0.5928954308255869\n",
            "************************************************************** 24\n",
            "what is the tr accuracy:  0.8409333333333333\n",
            "what is the v accuracy:  0.841\n",
            "what is the t accuracy:  0.8472834067547724\n",
            "what is the loss  0.5977131077921023\n",
            "what is the loss v  0.5895947232908652\n",
            "what is the loss test 0.5985738464596451\n",
            "************************************************************** 24\n",
            "what is the tr accuracy:  0.8409333333333333\n",
            "what is the v accuracy:  0.841\n",
            "what is the t accuracy:  0.8472834067547724\n",
            "what is the loss  0.5977131077921023\n",
            "what is the loss v  0.5895947232908652\n",
            "what is the loss test 0.5985738464596451\n",
            "************************************************************** 25\n",
            "what is the tr accuracy:  0.8232\n",
            "what is the v accuracy:  0.832\n",
            "what is the t accuracy:  0.8263582966226138\n",
            "what is the loss  0.6270584835242792\n",
            "what is the loss v  0.606708670964053\n",
            "what is the loss test 0.6260898866316841\n",
            "************************************************************** 25\n",
            "what is the tr accuracy:  0.8232\n",
            "what is the v accuracy:  0.832\n",
            "what is the t accuracy:  0.8263582966226138\n",
            "what is the loss  0.6270584835242792\n",
            "what is the loss v  0.606708670964053\n",
            "what is the loss test 0.6260898866316841\n",
            "************************************************************** 26\n",
            "what is the tr accuracy:  0.8327333333333333\n",
            "what is the v accuracy:  0.838\n",
            "what is the t accuracy:  0.8337004405286343\n",
            "what is the loss  0.5936752007689\n",
            "what is the loss v  0.5707975280016737\n",
            "what is the loss test 0.5965757410948253\n",
            "************************************************************** 26\n",
            "what is the tr accuracy:  0.8327333333333333\n",
            "what is the v accuracy:  0.838\n",
            "what is the t accuracy:  0.8337004405286343\n",
            "what is the loss  0.5936752007689\n",
            "what is the loss v  0.5707975280016737\n",
            "what is the loss test 0.5965757410948253\n",
            "************************************************************** 27\n",
            "what is the tr accuracy:  0.8306\n",
            "what is the v accuracy:  0.83\n",
            "what is the t accuracy:  0.8340675477239354\n",
            "what is the loss  0.5920112440869787\n",
            "what is the loss v  0.571732983371092\n",
            "what is the loss test 0.5982672035267942\n",
            "************************************************************** 27\n",
            "what is the tr accuracy:  0.8306\n",
            "what is the v accuracy:  0.83\n",
            "what is the t accuracy:  0.8340675477239354\n",
            "what is the loss  0.5920112440869787\n",
            "what is the loss v  0.571732983371092\n",
            "what is the loss test 0.5982672035267942\n",
            "************************************************************** 28\n",
            "what is the tr accuracy:  0.8304666666666667\n",
            "what is the v accuracy:  0.836\n",
            "what is the t accuracy:  0.8318649045521292\n",
            "what is the loss  0.5868304710864233\n",
            "what is the loss v  0.5713913900898222\n",
            "what is the loss test 0.5949388316388939\n",
            "************************************************************** 28\n",
            "what is the tr accuracy:  0.8304666666666667\n",
            "what is the v accuracy:  0.836\n",
            "what is the t accuracy:  0.8318649045521292\n",
            "what is the loss  0.5868304710864233\n",
            "what is the loss v  0.5713913900898222\n",
            "what is the loss test 0.5949388316388939\n",
            "************************************************************** 29\n",
            "what is the tr accuracy:  0.8406\n",
            "what is the v accuracy:  0.834\n",
            "what is the t accuracy:  0.8392070484581498\n",
            "what is the loss  0.5678495272227675\n",
            "what is the loss v  0.5627242314513284\n",
            "what is the loss test 0.5794928103583253\n",
            "************************************************************** 29\n",
            "what is the tr accuracy:  0.8406\n",
            "what is the v accuracy:  0.834\n",
            "what is the t accuracy:  0.8392070484581498\n",
            "what is the loss  0.5678495272227675\n",
            "what is the loss v  0.5627242314513284\n",
            "what is the loss test 0.5794928103583253\n",
            "************************************************************** 30\n",
            "what is the tr accuracy:  0.8436\n",
            "what is the v accuracy:  0.837\n",
            "what is the t accuracy:  0.8469162995594713\n",
            "what is the loss  0.5509452186062432\n",
            "what is the loss v  0.5561723899894293\n",
            "what is the loss test 0.5629820260530077\n",
            "************************************************************** 30\n",
            "what is the tr accuracy:  0.8436\n",
            "what is the v accuracy:  0.837\n",
            "what is the t accuracy:  0.8469162995594713\n",
            "what is the loss  0.5509452186062432\n",
            "what is the loss v  0.5561723899894293\n",
            "what is the loss test 0.5629820260530077\n",
            "************************************************************** 31\n",
            "what is the tr accuracy:  0.8493333333333334\n",
            "what is the v accuracy:  0.845\n",
            "what is the t accuracy:  0.8538913362701909\n",
            "what is the loss  0.5364441241995581\n",
            "what is the loss v  0.5513086538888342\n",
            "what is the loss test 0.5466696658516507\n",
            "************************************************************** 31\n",
            "what is the tr accuracy:  0.8493333333333334\n",
            "what is the v accuracy:  0.845\n",
            "what is the t accuracy:  0.8538913362701909\n",
            "what is the loss  0.5364441241995581\n",
            "what is the loss v  0.5513086538888342\n",
            "what is the loss test 0.5466696658516507\n",
            "************************************************************** 32\n",
            "what is the tr accuracy:  0.8568666666666667\n",
            "what is the v accuracy:  0.843\n",
            "what is the t accuracy:  0.8612334801762115\n",
            "what is the loss  0.5238928236792217\n",
            "what is the loss v  0.5482017696239535\n",
            "what is the loss test 0.5317274270324798\n",
            "************************************************************** 32\n",
            "what is the tr accuracy:  0.8568666666666667\n",
            "what is the v accuracy:  0.843\n",
            "what is the t accuracy:  0.8612334801762115\n",
            "what is the loss  0.5238928236792217\n",
            "what is the loss v  0.5482017696239535\n",
            "what is the loss test 0.5317274270324798\n",
            "************************************************************** 33\n",
            "what is the tr accuracy:  0.8591333333333333\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8660058737151248\n",
            "what is the loss  0.5183919650162387\n",
            "what is the loss v  0.5506412945938287\n",
            "what is the loss test 0.5250847243114477\n",
            "************************************************************** 33\n",
            "what is the tr accuracy:  0.8591333333333333\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8660058737151248\n",
            "what is the loss  0.5183919650162387\n",
            "what is the loss v  0.5506412945938287\n",
            "what is the loss test 0.5250847243114477\n",
            "************************************************************** 34\n",
            "what is the tr accuracy:  0.8584666666666667\n",
            "what is the v accuracy:  0.851\n",
            "what is the t accuracy:  0.8638032305433186\n",
            "what is the loss  0.5231379362455273\n",
            "what is the loss v  0.5603882308901043\n",
            "what is the loss test 0.5301939925783692\n",
            "************************************************************** 34\n",
            "what is the tr accuracy:  0.8584666666666667\n",
            "what is the v accuracy:  0.851\n",
            "what is the t accuracy:  0.8638032305433186\n",
            "what is the loss  0.5231379362455273\n",
            "what is the loss v  0.5603882308901043\n",
            "what is the loss test 0.5301939925783692\n",
            "************************************************************** 35\n",
            "what is the tr accuracy:  0.8562666666666666\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.8535242290748899\n",
            "what is the loss  0.5370959463160785\n",
            "what is the loss v  0.576225930078084\n",
            "what is the loss test 0.5448021888972918\n",
            "************************************************************** 35\n",
            "what is the tr accuracy:  0.8562666666666666\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.8535242290748899\n",
            "what is the loss  0.5370959463160785\n",
            "what is the loss v  0.576225930078084\n",
            "what is the loss test 0.5448021888972918\n",
            "************************************************************** 36\n",
            "what is the tr accuracy:  0.853\n",
            "what is the v accuracy:  0.843\n",
            "what is the t accuracy:  0.856093979441997\n",
            "what is the loss  0.5490447137121269\n",
            "what is the loss v  0.5880191037292318\n",
            "what is the loss test 0.5564001283309616\n",
            "************************************************************** 36\n",
            "what is the tr accuracy:  0.853\n",
            "what is the v accuracy:  0.843\n",
            "what is the t accuracy:  0.856093979441997\n",
            "what is the loss  0.5490447137121269\n",
            "what is the loss v  0.5880191037292318\n",
            "what is the loss test 0.5564001283309616\n",
            "************************************************************** 37\n",
            "what is the tr accuracy:  0.8576\n",
            "what is the v accuracy:  0.844\n",
            "what is the t accuracy:  0.8571953010279001\n",
            "what is the loss  0.5480591198173954\n",
            "what is the loss v  0.5846435693295464\n",
            "what is the loss test 0.5562340755384444\n",
            "************************************************************** 37\n",
            "what is the tr accuracy:  0.8576\n",
            "what is the v accuracy:  0.844\n",
            "what is the t accuracy:  0.8571953010279001\n",
            "what is the loss  0.5480591198173954\n",
            "what is the loss v  0.5846435693295464\n",
            "what is the loss test 0.5562340755384444\n",
            "************************************************************** 38\n",
            "what is the tr accuracy:  0.8617333333333334\n",
            "what is the v accuracy:  0.844\n",
            "what is the t accuracy:  0.8630690161527166\n",
            "what is the loss  0.535127215949438\n",
            "what is the loss v  0.5679402925118867\n",
            "what is the loss test 0.5469757591006796\n",
            "************************************************************** 38\n",
            "what is the tr accuracy:  0.8617333333333334\n",
            "what is the v accuracy:  0.844\n",
            "what is the t accuracy:  0.8630690161527166\n",
            "what is the loss  0.535127215949438\n",
            "what is the loss v  0.5679402925118867\n",
            "what is the loss test 0.5469757591006796\n",
            "************************************************************** 39\n",
            "what is the tr accuracy:  0.8652666666666666\n",
            "what is the v accuracy:  0.854\n",
            "what is the t accuracy:  0.8652716593245228\n",
            "what is the loss  0.5174738867386797\n",
            "what is the loss v  0.5464705429446742\n",
            "what is the loss test 0.5353715776781767\n",
            "************************************************************** 39\n",
            "what is the tr accuracy:  0.8652666666666666\n",
            "what is the v accuracy:  0.854\n",
            "what is the t accuracy:  0.8652716593245228\n",
            "what is the loss  0.5174738867386797\n",
            "what is the loss v  0.5464705429446742\n",
            "what is the loss test 0.5353715776781767\n",
            "************************************************************** 40\n",
            "what is the tr accuracy:  0.8694666666666667\n",
            "what is the v accuracy:  0.853\n",
            "what is the t accuracy:  0.8652716593245228\n",
            "what is the loss  0.5020959338014376\n",
            "what is the loss v  0.5263334226880584\n",
            "what is the loss test 0.5271220700968106\n",
            "************************************************************** 40\n",
            "what is the tr accuracy:  0.8694666666666667\n",
            "what is the v accuracy:  0.853\n",
            "what is the t accuracy:  0.8652716593245228\n",
            "what is the loss  0.5020959338014376\n",
            "what is the loss v  0.5263334226880584\n",
            "what is the loss test 0.5271220700968106\n",
            "************************************************************** 41\n",
            "what is the tr accuracy:  0.8724\n",
            "what is the v accuracy:  0.86\n",
            "what is the t accuracy:  0.8645374449339207\n",
            "what is the loss  0.4936097739680738\n",
            "what is the loss v  0.5102596322634941\n",
            "what is the loss test 0.5240081654835227\n",
            "************************************************************** 41\n",
            "what is the tr accuracy:  0.8724\n",
            "what is the v accuracy:  0.86\n",
            "what is the t accuracy:  0.8645374449339207\n",
            "what is the loss  0.4936097739680738\n",
            "what is the loss v  0.5102596322634941\n",
            "what is the loss test 0.5240081654835227\n",
            "************************************************************** 42\n",
            "what is the tr accuracy:  0.8750666666666667\n",
            "what is the v accuracy:  0.868\n",
            "what is the t accuracy:  0.8667400881057269\n",
            "what is the loss  0.49256826521811553\n",
            "what is the loss v  0.5026096853637109\n",
            "what is the loss test 0.527390038661395\n",
            "************************************************************** 42\n",
            "what is the tr accuracy:  0.8750666666666667\n",
            "what is the v accuracy:  0.868\n",
            "what is the t accuracy:  0.8667400881057269\n",
            "what is the loss  0.49256826521811553\n",
            "what is the loss v  0.5026096853637109\n",
            "what is the loss test 0.527390038661395\n",
            "************************************************************** 43\n",
            "what is the tr accuracy:  0.8743333333333333\n",
            "what is the v accuracy:  0.868\n",
            "what is the t accuracy:  0.8663729809104258\n",
            "what is the loss  0.4948735028086339\n",
            "what is the loss v  0.5003900955950866\n",
            "what is the loss test 0.5347888989791231\n",
            "************************************************************** 43\n",
            "what is the tr accuracy:  0.8743333333333333\n",
            "what is the v accuracy:  0.868\n",
            "what is the t accuracy:  0.8663729809104258\n",
            "what is the loss  0.4948735028086339\n",
            "what is the loss v  0.5003900955950866\n",
            "what is the loss test 0.5347888989791231\n",
            "************************************************************** 44\n",
            "what is the tr accuracy:  0.8735333333333334\n",
            "what is the v accuracy:  0.87\n",
            "what is the t accuracy:  0.8678414096916299\n",
            "what is the loss  0.49313328427404257\n",
            "what is the loss v  0.49727622447667313\n",
            "what is the loss test 0.5383609412172906\n",
            "************************************************************** 44\n",
            "what is the tr accuracy:  0.8735333333333334\n",
            "what is the v accuracy:  0.87\n",
            "what is the t accuracy:  0.8678414096916299\n",
            "what is the loss  0.49313328427404257\n",
            "what is the loss v  0.49727622447667313\n",
            "what is the loss test 0.5383609412172906\n",
            "************************************************************** 45\n",
            "what is the tr accuracy:  0.8772\n",
            "what is the v accuracy:  0.874\n",
            "what is the t accuracy:  0.8696769456681351\n",
            "what is the loss  0.48410768857009184\n",
            "what is the loss v  0.4894095382736548\n",
            "what is the loss test 0.5343722944347226\n",
            "************************************************************** 45\n",
            "what is the tr accuracy:  0.8772\n",
            "what is the v accuracy:  0.874\n",
            "what is the t accuracy:  0.8696769456681351\n",
            "what is the loss  0.48410768857009184\n",
            "what is the loss v  0.4894095382736548\n",
            "what is the loss test 0.5343722944347226\n",
            "************************************************************** 46\n",
            "what is the tr accuracy:  0.8778\n",
            "what is the v accuracy:  0.872\n",
            "what is the t accuracy:  0.8704111600587372\n",
            "what is the loss  0.4731153018751731\n",
            "what is the loss v  0.48132375701990293\n",
            "what is the loss test 0.5272359469970992\n",
            "************************************************************** 46\n",
            "what is the tr accuracy:  0.8778\n",
            "what is the v accuracy:  0.872\n",
            "what is the t accuracy:  0.8704111600587372\n",
            "what is the loss  0.4731153018751731\n",
            "what is the loss v  0.48132375701990293\n",
            "what is the loss test 0.5272359469970992\n",
            "************************************************************** 47\n",
            "what is the tr accuracy:  0.8771333333333333\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8722466960352423\n",
            "what is the loss  0.4675813994879512\n",
            "what is the loss v  0.47991998778935885\n",
            "what is the loss test 0.5250497999869681\n",
            "************************************************************** 47\n",
            "what is the tr accuracy:  0.8771333333333333\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8722466960352423\n",
            "what is the loss  0.4675813994879512\n",
            "what is the loss v  0.47991998778935885\n",
            "what is the loss test 0.5250497999869681\n",
            "************************************************************** 48\n",
            "what is the tr accuracy:  0.8764666666666666\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.867474302496329\n",
            "what is the loss  0.46777898190854417\n",
            "what is the loss v  0.4838207472747671\n",
            "what is the loss test 0.528062905059246\n",
            "************************************************************** 48\n",
            "what is the tr accuracy:  0.8764666666666666\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.867474302496329\n",
            "what is the loss  0.46777898190854417\n",
            "what is the loss v  0.4838207472747671\n",
            "what is the loss test 0.528062905059246\n",
            "************************************************************** 49\n",
            "what is the tr accuracy:  0.8742\n",
            "what is the v accuracy:  0.873\n",
            "what is the t accuracy:  0.869309838472834\n",
            "what is the loss  0.46829037481667973\n",
            "what is the loss v  0.485867857628797\n",
            "what is the loss test 0.5306834738178784\n",
            "************************************************************** 49\n",
            "what is the tr accuracy:  0.8742\n",
            "what is the v accuracy:  0.873\n",
            "what is the t accuracy:  0.869309838472834\n",
            "what is the loss  0.46829037481667973\n",
            "what is the loss v  0.485867857628797\n",
            "what is the loss test 0.5306834738178784\n",
            "************************************************************** 50\n",
            "what is the tr accuracy:  0.8754\n",
            "what is the v accuracy:  0.87\n",
            "what is the t accuracy:  0.8700440528634361\n",
            "what is the loss  0.465347503148429\n",
            "what is the loss v  0.48320451015367316\n",
            "what is the loss test 0.5294143166650357\n",
            "************************************************************** 50\n",
            "what is the tr accuracy:  0.8754\n",
            "what is the v accuracy:  0.87\n",
            "what is the t accuracy:  0.8700440528634361\n",
            "what is the loss  0.465347503148429\n",
            "what is the loss v  0.48320451015367316\n",
            "what is the loss test 0.5294143166650357\n",
            "************************************************************** 51\n",
            "what is the tr accuracy:  0.8768666666666667\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8711453744493393\n",
            "what is the loss  0.4573148617929819\n",
            "what is the loss v  0.4743881409345164\n",
            "what is the loss test 0.5228639721473748\n",
            "************************************************************** 51\n",
            "what is the tr accuracy:  0.8768666666666667\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8711453744493393\n",
            "what is the loss  0.4573148617929819\n",
            "what is the loss v  0.4743881409345164\n",
            "what is the loss test 0.5228639721473748\n",
            "************************************************************** 52\n",
            "what is the tr accuracy:  0.879\n",
            "what is the v accuracy:  0.88\n",
            "what is the t accuracy:  0.869309838472834\n",
            "what is the loss  0.4490881664031074\n",
            "what is the loss v  0.46456265012032355\n",
            "what is the loss test 0.5146711553745084\n",
            "************************************************************** 52\n",
            "what is the tr accuracy:  0.879\n",
            "what is the v accuracy:  0.88\n",
            "what is the t accuracy:  0.869309838472834\n",
            "what is the loss  0.4490881664031074\n",
            "what is the loss v  0.46456265012032355\n",
            "what is the loss test 0.5146711553745084\n",
            "************************************************************** 53\n",
            "what is the tr accuracy:  0.8798666666666667\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8715124816446402\n",
            "what is the loss  0.44292231162734785\n",
            "what is the loss v  0.4581073615636085\n",
            "what is the loss test 0.5071631627281394\n",
            "************************************************************** 53\n",
            "what is the tr accuracy:  0.8798666666666667\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8715124816446402\n",
            "what is the loss  0.44292231162734785\n",
            "what is the loss v  0.4581073615636085\n",
            "what is the loss test 0.5071631627281394\n",
            "************************************************************** 54\n",
            "what is the tr accuracy:  0.879\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8700440528634361\n",
            "what is the loss  0.4391501567918087\n",
            "what is the loss v  0.4575648422921976\n",
            "what is the loss test 0.5022271740953085\n",
            "************************************************************** 54\n",
            "what is the tr accuracy:  0.879\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8700440528634361\n",
            "what is the loss  0.4391501567918087\n",
            "what is the loss v  0.4575648422921976\n",
            "what is the loss test 0.5022271740953085\n",
            "************************************************************** 55\n",
            "what is the tr accuracy:  0.8790666666666667\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8696769456681351\n",
            "what is the loss  0.4394970446144539\n",
            "what is the loss v  0.46399028463587577\n",
            "what is the loss test 0.5015593388513218\n",
            "************************************************************** 55\n",
            "what is the tr accuracy:  0.8790666666666667\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8696769456681351\n",
            "what is the loss  0.4394970446144539\n",
            "what is the loss v  0.46399028463587577\n",
            "what is the loss test 0.5015593388513218\n",
            "************************************************************** 56\n",
            "what is the tr accuracy:  0.8788\n",
            "what is the v accuracy:  0.877\n",
            "what is the t accuracy:  0.8700440528634361\n",
            "what is the loss  0.4435858226847941\n",
            "what is the loss v  0.4726638147499448\n",
            "what is the loss test 0.5048395197520131\n",
            "************************************************************** 56\n",
            "what is the tr accuracy:  0.8788\n",
            "what is the v accuracy:  0.877\n",
            "what is the t accuracy:  0.8700440528634361\n",
            "what is the loss  0.4435858226847941\n",
            "what is the loss v  0.4726638147499448\n",
            "what is the loss test 0.5048395197520131\n",
            "************************************************************** 57\n",
            "what is the tr accuracy:  0.8792666666666666\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8707782672540382\n",
            "what is the loss  0.44624168325844604\n",
            "what is the loss v  0.4775186894411032\n",
            "what is the loss test 0.5076048369414704\n",
            "************************************************************** 57\n",
            "what is the tr accuracy:  0.8792666666666666\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8707782672540382\n",
            "what is the loss  0.44624168325844604\n",
            "what is the loss v  0.4775186894411032\n",
            "what is the loss test 0.5076048369414704\n",
            "************************************************************** 58\n",
            "what is the tr accuracy:  0.8796666666666667\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8711453744493393\n",
            "what is the loss  0.4432481387666376\n",
            "what is the loss v  0.47387239448238266\n",
            "what is the loss test 0.5061683302159944\n",
            "************************************************************** 58\n",
            "what is the tr accuracy:  0.8796666666666667\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8711453744493393\n",
            "what is the loss  0.4432481387666376\n",
            "what is the loss v  0.47387239448238266\n",
            "what is the loss test 0.5061683302159944\n",
            "************************************************************** 59\n",
            "what is the tr accuracy:  0.8799333333333333\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8737151248164464\n",
            "what is the loss  0.43695830978976036\n",
            "what is the loss v  0.46566138761215065\n",
            "what is the loss test 0.5027983387914139\n",
            "************************************************************** 59\n",
            "what is the tr accuracy:  0.8799333333333333\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8737151248164464\n",
            "what is the loss  0.43695830978976036\n",
            "what is the loss v  0.46566138761215065\n",
            "what is the loss test 0.5027983387914139\n",
            "************************************************************** 60\n",
            "what is the tr accuracy:  0.8811333333333333\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.8751835535976505\n",
            "what is the loss  0.43046433815382756\n",
            "what is the loss v  0.45605644037032506\n",
            "what is the loss test 0.5000302215964406\n",
            "************************************************************** 60\n",
            "what is the tr accuracy:  0.8811333333333333\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.8751835535976505\n",
            "what is the loss  0.43046433815382756\n",
            "what is the loss v  0.45605644037032506\n",
            "what is the loss test 0.5000302215964406\n",
            "************************************************************** 61\n",
            "what is the tr accuracy:  0.882\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8737151248164464\n",
            "what is the loss  0.42484871683125885\n",
            "what is the loss v  0.44916100308283274\n",
            "what is the loss test 0.4987577403519758\n",
            "************************************************************** 61\n",
            "what is the tr accuracy:  0.882\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8737151248164464\n",
            "what is the loss  0.42484871683125885\n",
            "what is the loss v  0.44916100308283274\n",
            "what is the loss test 0.4987577403519758\n",
            "************************************************************** 62\n",
            "what is the tr accuracy:  0.8845333333333333\n",
            "what is the v accuracy:  0.877\n",
            "what is the t accuracy:  0.8744493392070485\n",
            "what is the loss  0.42009321137167105\n",
            "what is the loss v  0.4463649869745776\n",
            "what is the loss test 0.49901800814039865\n",
            "************************************************************** 62\n",
            "what is the tr accuracy:  0.8845333333333333\n",
            "what is the v accuracy:  0.877\n",
            "what is the t accuracy:  0.8744493392070485\n",
            "what is the loss  0.42009321137167105\n",
            "what is the loss v  0.4463649869745776\n",
            "what is the loss test 0.49901800814039865\n",
            "************************************************************** 63\n",
            "what is the tr accuracy:  0.8853333333333333\n",
            "what is the v accuracy:  0.877\n",
            "what is the t accuracy:  0.8740822320117474\n",
            "what is the loss  0.41569931801158866\n",
            "what is the loss v  0.44584776556249894\n",
            "what is the loss test 0.49949508478565696\n",
            "************************************************************** 63\n",
            "what is the tr accuracy:  0.8853333333333333\n",
            "what is the v accuracy:  0.877\n",
            "what is the t accuracy:  0.8740822320117474\n",
            "what is the loss  0.41569931801158866\n",
            "what is the loss v  0.44584776556249894\n",
            "what is the loss test 0.49949508478565696\n",
            "************************************************************** 64\n",
            "what is the tr accuracy:  0.886\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8726138032305433\n",
            "what is the loss  0.41250808638351383\n",
            "what is the loss v  0.44574256985014327\n",
            "what is the loss test 0.49951462168338734\n",
            "************************************************************** 64\n",
            "what is the tr accuracy:  0.886\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8726138032305433\n",
            "what is the loss  0.41250808638351383\n",
            "what is the loss v  0.44574256985014327\n",
            "what is the loss test 0.49951462168338734\n",
            "************************************************************** 65\n",
            "what is the tr accuracy:  0.8868666666666667\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.8729809104258444\n",
            "what is the loss  0.4117756361939208\n",
            "what is the loss v  0.44729359721964884\n",
            "what is the loss test 0.5010989915402017\n",
            "************************************************************** 65\n",
            "what is the tr accuracy:  0.8868666666666667\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.8729809104258444\n",
            "what is the loss  0.4117756361939208\n",
            "what is the loss v  0.44729359721964884\n",
            "what is the loss test 0.5010989915402017\n",
            "************************************************************** 66\n",
            "what is the tr accuracy:  0.8855333333333333\n",
            "what is the v accuracy:  0.873\n",
            "what is the t accuracy:  0.8748164464023495\n",
            "what is the loss  0.4130143199513016\n",
            "what is the loss v  0.45012133751934064\n",
            "what is the loss test 0.5037268010199387\n",
            "************************************************************** 66\n",
            "what is the tr accuracy:  0.8855333333333333\n",
            "what is the v accuracy:  0.873\n",
            "what is the t accuracy:  0.8748164464023495\n",
            "what is the loss  0.4130143199513016\n",
            "what is the loss v  0.45012133751934064\n",
            "what is the loss test 0.5037268010199387\n",
            "************************************************************** 67\n",
            "what is the tr accuracy:  0.8852\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8755506607929515\n",
            "what is the loss  0.4129820350672099\n",
            "what is the loss v  0.44985080856725146\n",
            "what is the loss test 0.5046776296995716\n",
            "************************************************************** 67\n",
            "what is the tr accuracy:  0.8852\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8755506607929515\n",
            "what is the loss  0.4129820350672099\n",
            "what is the loss v  0.44985080856725146\n",
            "what is the loss test 0.5046776296995716\n",
            "************************************************************** 68\n",
            "what is the tr accuracy:  0.8858\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8744493392070485\n",
            "what is the loss  0.409179754580232\n",
            "what is the loss v  0.4449016358732056\n",
            "what is the loss test 0.5018648018270278\n",
            "************************************************************** 68\n",
            "what is the tr accuracy:  0.8858\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8744493392070485\n",
            "what is the loss  0.409179754580232\n",
            "what is the loss v  0.4449016358732056\n",
            "what is the loss test 0.5018648018270278\n",
            "************************************************************** 69\n",
            "what is the tr accuracy:  0.8872666666666666\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8762848751835536\n",
            "what is the loss  0.4027440317959378\n",
            "what is the loss v  0.43708908903520344\n",
            "what is the loss test 0.4959364803867378\n",
            "************************************************************** 69\n",
            "what is the tr accuracy:  0.8872666666666666\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8762848751835536\n",
            "what is the loss  0.4027440317959378\n",
            "what is the loss v  0.43708908903520344\n",
            "what is the loss test 0.4959364803867378\n",
            "************************************************************** 70\n",
            "what is the tr accuracy:  0.8894666666666666\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8792217327459618\n",
            "what is the loss  0.3964993030130469\n",
            "what is the loss v  0.43010281263622396\n",
            "what is the loss test 0.4897336336402983\n",
            "************************************************************** 70\n",
            "what is the tr accuracy:  0.8894666666666666\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8792217327459618\n",
            "what is the loss  0.3964993030130469\n",
            "what is the loss v  0.43010281263622396\n",
            "what is the loss test 0.4897336336402983\n",
            "************************************************************** 71\n",
            "what is the tr accuracy:  0.8909333333333334\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.881424375917768\n",
            "what is the loss  0.3923911418411278\n",
            "what is the loss v  0.42629304051222355\n",
            "what is the loss test 0.48555658443146255\n",
            "************************************************************** 71\n",
            "what is the tr accuracy:  0.8909333333333334\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.881424375917768\n",
            "what is the loss  0.3923911418411278\n",
            "what is the loss v  0.42629304051222355\n",
            "what is the loss test 0.48555658443146255\n",
            "************************************************************** 72\n",
            "what is the tr accuracy:  0.8914666666666666\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.3915660607345065\n",
            "what is the loss v  0.42525769703715227\n",
            "what is the loss test 0.4843709551099581\n",
            "************************************************************** 72\n",
            "what is the tr accuracy:  0.8914666666666666\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.3915660607345065\n",
            "what is the loss v  0.42525769703715227\n",
            "what is the loss test 0.4843709551099581\n",
            "************************************************************** 73\n",
            "what is the tr accuracy:  0.8912666666666667\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.881424375917768\n",
            "what is the loss  0.3940068602273689\n",
            "what is the loss v  0.4262064092655148\n",
            "what is the loss test 0.4860619501809617\n",
            "************************************************************** 73\n",
            "what is the tr accuracy:  0.8912666666666667\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.881424375917768\n",
            "what is the loss  0.3940068602273689\n",
            "what is the loss v  0.4262064092655148\n",
            "what is the loss test 0.4860619501809617\n",
            "************************************************************** 74\n",
            "what is the tr accuracy:  0.8912\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8792217327459618\n",
            "what is the loss  0.3980044905400783\n",
            "what is the loss v  0.42830885661603146\n",
            "what is the loss test 0.48967602677167277\n",
            "************************************************************** 74\n",
            "what is the tr accuracy:  0.8912\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8792217327459618\n",
            "what is the loss  0.3980044905400783\n",
            "what is the loss v  0.42830885661603146\n",
            "what is the loss test 0.48967602677167277\n",
            "************************************************************** 75\n",
            "what is the tr accuracy:  0.8900666666666667\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8781204111600588\n",
            "what is the loss  0.4007512032398018\n",
            "what is the loss v  0.42852131736430826\n",
            "what is the loss test 0.49261738960611334\n",
            "************************************************************** 75\n",
            "what is the tr accuracy:  0.8900666666666667\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8781204111600588\n",
            "what is the loss  0.4007512032398018\n",
            "what is the loss v  0.42852131736430826\n",
            "what is the loss test 0.49261738960611334\n",
            "************************************************************** 76\n",
            "what is the tr accuracy:  0.8903333333333333\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8773861967694567\n",
            "what is the loss  0.40096929501976764\n",
            "what is the loss v  0.42620308653921274\n",
            "what is the loss test 0.493695970537816\n",
            "************************************************************** 76\n",
            "what is the tr accuracy:  0.8903333333333333\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8773861967694567\n",
            "what is the loss  0.40096929501976764\n",
            "what is the loss v  0.42620308653921274\n",
            "what is the loss test 0.493695970537816\n",
            "************************************************************** 77\n",
            "what is the tr accuracy:  0.8911333333333333\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8788546255506607\n",
            "what is the loss  0.3995762543577833\n",
            "what is the loss v  0.4231445666223667\n",
            "what is the loss test 0.49410986635883186\n",
            "************************************************************** 77\n",
            "what is the tr accuracy:  0.8911333333333333\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8788546255506607\n",
            "what is the loss  0.3995762543577833\n",
            "what is the loss v  0.4231445666223667\n",
            "what is the loss test 0.49410986635883186\n",
            "************************************************************** 78\n",
            "what is the tr accuracy:  0.8926\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.8799559471365639\n",
            "what is the loss  0.39513208124480215\n",
            "what is the loss v  0.420335865467906\n",
            "what is the loss test 0.4931169792097458\n",
            "************************************************************** 78\n",
            "what is the tr accuracy:  0.8926\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.8799559471365639\n",
            "what is the loss  0.39513208124480215\n",
            "what is the loss v  0.420335865467906\n",
            "what is the loss test 0.4931169792097458\n",
            "************************************************************** 79\n",
            "what is the tr accuracy:  0.8944666666666666\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.3863322845672189\n",
            "what is the loss v  0.4166118204377444\n",
            "what is the loss test 0.48967212163369267\n",
            "************************************************************** 79\n",
            "what is the tr accuracy:  0.8944666666666666\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.3863322845672189\n",
            "what is the loss v  0.4166118204377444\n",
            "what is the loss test 0.48967212163369267\n",
            "************************************************************** 80\n",
            "what is the tr accuracy:  0.8961333333333333\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8861967694566814\n",
            "what is the loss  0.3779704490585117\n",
            "what is the loss v  0.41457653514329507\n",
            "what is the loss test 0.4879660914606031\n",
            "************************************************************** 80\n",
            "what is the tr accuracy:  0.8961333333333333\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8861967694566814\n",
            "what is the loss  0.3779704490585117\n",
            "what is the loss v  0.41457653514329507\n",
            "what is the loss test 0.4879660914606031\n",
            "************************************************************** 81\n",
            "what is the tr accuracy:  0.8978666666666667\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8869309838472834\n",
            "what is the loss  0.3727632291911523\n",
            "what is the loss v  0.41442200685179353\n",
            "what is the loss test 0.4899143588836762\n",
            "************************************************************** 81\n",
            "what is the tr accuracy:  0.8978666666666667\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8869309838472834\n",
            "what is the loss  0.3727632291911523\n",
            "what is the loss v  0.41442200685179353\n",
            "what is the loss test 0.4899143588836762\n",
            "************************************************************** 82\n",
            "what is the tr accuracy:  0.8982666666666667\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.37045942034714363\n",
            "what is the loss v  0.4151453498406155\n",
            "what is the loss test 0.49451607607768894\n",
            "************************************************************** 82\n",
            "what is the tr accuracy:  0.8982666666666667\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.37045942034714363\n",
            "what is the loss v  0.4151453498406155\n",
            "what is the loss test 0.49451607607768894\n",
            "************************************************************** 83\n",
            "what is the tr accuracy:  0.8994666666666666\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.3701560013529267\n",
            "what is the loss v  0.41665667595721756\n",
            "what is the loss test 0.5005646794362608\n",
            "************************************************************** 83\n",
            "what is the tr accuracy:  0.8994666666666666\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.3701560013529267\n",
            "what is the loss v  0.41665667595721756\n",
            "what is the loss test 0.5005646794362608\n",
            "************************************************************** 84\n",
            "what is the tr accuracy:  0.9002\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.3708544367518282\n",
            "what is the loss v  0.41861180033365497\n",
            "what is the loss test 0.5065760258078359\n",
            "************************************************************** 84\n",
            "what is the tr accuracy:  0.9002\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.3708544367518282\n",
            "what is the loss v  0.41861180033365497\n",
            "what is the loss test 0.5065760258078359\n",
            "************************************************************** 85\n",
            "what is the tr accuracy:  0.8993333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8869309838472834\n",
            "what is the loss  0.3715376579285118\n",
            "what is the loss v  0.4207369546152363\n",
            "what is the loss test 0.5115154124112727\n",
            "************************************************************** 85\n",
            "what is the tr accuracy:  0.8993333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8869309838472834\n",
            "what is the loss  0.3715376579285118\n",
            "what is the loss v  0.4207369546152363\n",
            "what is the loss test 0.5115154124112727\n",
            "************************************************************** 86\n",
            "what is the tr accuracy:  0.8981333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8843612334801763\n",
            "what is the loss  0.3717839993105851\n",
            "what is the loss v  0.4228848064557012\n",
            "what is the loss test 0.5154461177279437\n",
            "************************************************************** 86\n",
            "what is the tr accuracy:  0.8981333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8843612334801763\n",
            "what is the loss  0.3717839993105851\n",
            "what is the loss v  0.4228848064557012\n",
            "what is the loss test 0.5154461177279437\n",
            "************************************************************** 87\n",
            "what is the tr accuracy:  0.8982\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.371390523930016\n",
            "what is the loss v  0.4248034004302876\n",
            "what is the loss test 0.517736673790232\n",
            "************************************************************** 87\n",
            "what is the tr accuracy:  0.8982\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.371390523930016\n",
            "what is the loss v  0.4248034004302876\n",
            "what is the loss test 0.517736673790232\n",
            "************************************************************** 88\n",
            "what is the tr accuracy:  0.8982\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8832599118942731\n",
            "what is the loss  0.37007999303540295\n",
            "what is the loss v  0.4255710328428656\n",
            "what is the loss test 0.5180375701628069\n",
            "************************************************************** 88\n",
            "what is the tr accuracy:  0.8982\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8832599118942731\n",
            "what is the loss  0.37007999303540295\n",
            "what is the loss v  0.4255710328428656\n",
            "what is the loss test 0.5180375701628069\n",
            "************************************************************** 89\n",
            "what is the tr accuracy:  0.8992666666666667\n",
            "what is the v accuracy:  0.887\n",
            "what is the t accuracy:  0.8832599118942731\n",
            "what is the loss  0.36770338211427445\n",
            "what is the loss v  0.42439523425140496\n",
            "what is the loss test 0.5163092618778948\n",
            "************************************************************** 89\n",
            "what is the tr accuracy:  0.8992666666666667\n",
            "what is the v accuracy:  0.887\n",
            "what is the t accuracy:  0.8832599118942731\n",
            "what is the loss  0.36770338211427445\n",
            "what is the loss v  0.42439523425140496\n",
            "what is the loss test 0.5163092618778948\n",
            "************************************************************** 90\n",
            "what is the tr accuracy:  0.9004666666666666\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8843612334801763\n",
            "what is the loss  0.36448630734888293\n",
            "what is the loss v  0.42169304373626654\n",
            "what is the loss test 0.5127744146219056\n",
            "************************************************************** 90\n",
            "what is the tr accuracy:  0.9004666666666666\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8843612334801763\n",
            "what is the loss  0.36448630734888293\n",
            "what is the loss v  0.42169304373626654\n",
            "what is the loss test 0.5127744146219056\n",
            "************************************************************** 91\n",
            "what is the tr accuracy:  0.8993333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8861967694566814\n",
            "what is the loss  0.3608902278930081\n",
            "what is the loss v  0.4186974247431181\n",
            "what is the loss test 0.5084460176359666\n",
            "************************************************************** 91\n",
            "what is the tr accuracy:  0.8993333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8861967694566814\n",
            "what is the loss  0.3608902278930081\n",
            "what is the loss v  0.4186974247431181\n",
            "what is the loss test 0.5084460176359666\n",
            "************************************************************** 92\n",
            "what is the tr accuracy:  0.8993333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8854625550660793\n",
            "what is the loss  0.3574686483210675\n",
            "what is the loss v  0.4171320051381015\n",
            "what is the loss test 0.5042344125044335\n",
            "************************************************************** 92\n",
            "what is the tr accuracy:  0.8993333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8854625550660793\n",
            "what is the loss  0.3574686483210675\n",
            "what is the loss v  0.4171320051381015\n",
            "what is the loss test 0.5042344125044335\n",
            "************************************************************** 93\n",
            "what is the tr accuracy:  0.8997333333333334\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.3545589534771089\n",
            "what is the loss v  0.4187236494335283\n",
            "what is the loss test 0.5007311219874185\n",
            "************************************************************** 93\n",
            "what is the tr accuracy:  0.8997333333333334\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.3545589534771089\n",
            "what is the loss v  0.4187236494335283\n",
            "what is the loss test 0.5007311219874185\n",
            "************************************************************** 94\n",
            "what is the tr accuracy:  0.9003333333333333\n",
            "what is the v accuracy:  0.886\n",
            "what is the t accuracy:  0.8799559471365639\n",
            "what is the loss  0.35214399127869567\n",
            "what is the loss v  0.42373698813816935\n",
            "what is the loss test 0.4980653559272081\n",
            "************************************************************** 94\n",
            "what is the tr accuracy:  0.9003333333333333\n",
            "what is the v accuracy:  0.886\n",
            "what is the t accuracy:  0.8799559471365639\n",
            "what is the loss  0.35214399127869567\n",
            "what is the loss v  0.42373698813816935\n",
            "what is the loss test 0.4980653559272081\n",
            "************************************************************** 95\n",
            "what is the tr accuracy:  0.9005333333333333\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.3506310841777237\n",
            "what is the loss v  0.4309756355144542\n",
            "what is the loss test 0.4968235001696471\n",
            "************************************************************** 95\n",
            "what is the tr accuracy:  0.9005333333333333\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.3506310841777237\n",
            "what is the loss v  0.4309756355144542\n",
            "what is the loss test 0.4968235001696471\n",
            "************************************************************** 96\n",
            "what is the tr accuracy:  0.9014\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.3499757555705636\n",
            "what is the loss v  0.4386328487970033\n",
            "what is the loss test 0.49669685640024014\n",
            "************************************************************** 96\n",
            "what is the tr accuracy:  0.9014\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.3499757555705636\n",
            "what is the loss v  0.4386328487970033\n",
            "what is the loss test 0.49669685640024014\n",
            "************************************************************** 97\n",
            "what is the tr accuracy:  0.9020666666666667\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.3489204585970085\n",
            "what is the loss v  0.4445586464342226\n",
            "what is the loss test 0.4964635437447409\n",
            "************************************************************** 97\n",
            "what is the tr accuracy:  0.9020666666666667\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.3489204585970085\n",
            "what is the loss v  0.4445586464342226\n",
            "what is the loss test 0.4964635437447409\n",
            "************************************************************** 98\n",
            "what is the tr accuracy:  0.9028666666666667\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.3461924251187635\n",
            "what is the loss v  0.44684123383528335\n",
            "what is the loss test 0.4950875375519911\n",
            "************************************************************** 98\n",
            "what is the tr accuracy:  0.9028666666666667\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.3461924251187635\n",
            "what is the loss v  0.44684123383528335\n",
            "what is the loss test 0.4950875375519911\n",
            "************************************************************** 99\n",
            "what is the tr accuracy:  0.9042666666666667\n",
            "what is the v accuracy:  0.886\n",
            "what is the t accuracy:  0.8828928046989721\n",
            "what is the loss  0.34212194022547204\n",
            "what is the loss v  0.44554411907606234\n",
            "what is the loss test 0.4927167747550367\n",
            "************************************************************** 99\n",
            "what is the tr accuracy:  0.9042666666666667\n",
            "what is the v accuracy:  0.886\n",
            "what is the t accuracy:  0.8828928046989721\n",
            "what is the loss  0.34212194022547204\n",
            "what is the loss v  0.44554411907606234\n",
            "what is the loss test 0.4927167747550367\n",
            "************************************************************** 100\n",
            "what is the tr accuracy:  0.9053333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8858296622613803\n",
            "what is the loss  0.3380521453775659\n",
            "what is the loss v  0.4427396005533911\n",
            "what is the loss test 0.4908019108362192\n",
            "************************************************************** 100\n",
            "what is the tr accuracy:  0.9053333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8858296622613803\n",
            "what is the loss  0.3380521453775659\n",
            "what is the loss v  0.4427396005533911\n",
            "what is the loss test 0.4908019108362192\n",
            "************************************************************** 101\n",
            "what is the tr accuracy:  0.9060666666666667\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.3350369695605554\n",
            "what is the loss v  0.4407421018556887\n",
            "what is the loss test 0.4903478534301628\n",
            "************************************************************** 101\n",
            "what is the tr accuracy:  0.9060666666666667\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.3350369695605554\n",
            "what is the loss v  0.4407421018556887\n",
            "what is the loss test 0.4903478534301628\n",
            "************************************************************** 102\n",
            "what is the tr accuracy:  0.9062\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.33300596363526214\n",
            "what is the loss v  0.440402805743753\n",
            "what is the loss test 0.4909317369162485\n",
            "************************************************************** 102\n",
            "what is the tr accuracy:  0.9062\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.33300596363526214\n",
            "what is the loss v  0.440402805743753\n",
            "what is the loss test 0.4909317369162485\n",
            "************************************************************** 103\n",
            "what is the tr accuracy:  0.9063333333333333\n",
            "what is the v accuracy:  0.887\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.33106512871406973\n",
            "what is the loss v  0.4409543151998954\n",
            "what is the loss test 0.49139982633852386\n",
            "************************************************************** 103\n",
            "what is the tr accuracy:  0.9063333333333333\n",
            "what is the v accuracy:  0.887\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.33106512871406973\n",
            "what is the loss v  0.4409543151998954\n",
            "what is the loss test 0.49139982633852386\n",
            "************************************************************** 104\n",
            "what is the tr accuracy:  0.9067333333333333\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.8850954478707783\n",
            "what is the loss  0.32878137753904607\n",
            "what is the loss v  0.44220763350644815\n",
            "what is the loss test 0.49085369657865646\n",
            "************************************************************** 104\n",
            "what is the tr accuracy:  0.9067333333333333\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.8850954478707783\n",
            "what is the loss  0.32878137753904607\n",
            "what is the loss v  0.44220763350644815\n",
            "what is the loss test 0.49085369657865646\n",
            "************************************************************** 105\n",
            "what is the tr accuracy:  0.9078666666666667\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.32620348280758493\n",
            "what is the loss v  0.4432914262490883\n",
            "what is the loss test 0.48923968070416674\n",
            "************************************************************** 105\n",
            "what is the tr accuracy:  0.9078666666666667\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.32620348280758493\n",
            "what is the loss v  0.4432914262490883\n",
            "what is the loss test 0.48923968070416674\n",
            "************************************************************** 106\n",
            "what is the tr accuracy:  0.9085333333333333\n",
            "what is the v accuracy:  0.886\n",
            "what is the t accuracy:  0.8869309838472834\n",
            "what is the loss  0.32376527389304977\n",
            "what is the loss v  0.44442398171963093\n",
            "what is the loss test 0.48755292824813756\n",
            "************************************************************** 106\n",
            "what is the tr accuracy:  0.9085333333333333\n",
            "what is the v accuracy:  0.886\n",
            "what is the t accuracy:  0.8869309838472834\n",
            "what is the loss  0.32376527389304977\n",
            "what is the loss v  0.44442398171963093\n",
            "what is the loss test 0.48755292824813756\n",
            "************************************************************** 107\n",
            "what is the tr accuracy:  0.9084\n",
            "what is the v accuracy:  0.887\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.32155312005139264\n",
            "what is the loss v  0.44429329543008567\n",
            "what is the loss test 0.48543375416653983\n",
            "************************************************************** 107\n",
            "what is the tr accuracy:  0.9084\n",
            "what is the v accuracy:  0.887\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.32155312005139264\n",
            "what is the loss v  0.44429329543008567\n",
            "what is the loss test 0.48543375416653983\n",
            "************************************************************** 108\n",
            "what is the tr accuracy:  0.9098\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.319432009192264\n",
            "what is the loss v  0.4429219925407926\n",
            "what is the loss test 0.48262124579689675\n",
            "************************************************************** 108\n",
            "what is the tr accuracy:  0.9098\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.319432009192264\n",
            "what is the loss v  0.4429219925407926\n",
            "what is the loss test 0.48262124579689675\n",
            "************************************************************** 109\n",
            "what is the tr accuracy:  0.9102\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.3173379792251265\n",
            "what is the loss v  0.44046012667283213\n",
            "what is the loss test 0.47981627499913726\n",
            "************************************************************** 109\n",
            "what is the tr accuracy:  0.9102\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.3173379792251265\n",
            "what is the loss v  0.44046012667283213\n",
            "what is the loss test 0.47981627499913726\n",
            "************************************************************** 110\n",
            "what is the tr accuracy:  0.9104666666666666\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.3154839367253899\n",
            "what is the loss v  0.4371435419840276\n",
            "what is the loss test 0.47699448498892444\n",
            "************************************************************** 110\n",
            "what is the tr accuracy:  0.9104666666666666\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.3154839367253899\n",
            "what is the loss v  0.4371435419840276\n",
            "what is the loss test 0.47699448498892444\n",
            "************************************************************** 111\n",
            "what is the tr accuracy:  0.9106666666666666\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.31412517019472125\n",
            "what is the loss v  0.4339552715217085\n",
            "what is the loss test 0.4746529668732301\n",
            "************************************************************** 111\n",
            "what is the tr accuracy:  0.9106666666666666\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.31412517019472125\n",
            "what is the loss v  0.4339552715217085\n",
            "what is the loss test 0.4746529668732301\n",
            "************************************************************** 112\n",
            "what is the tr accuracy:  0.909\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.31327391760075757\n",
            "what is the loss v  0.4308879672832403\n",
            "what is the loss test 0.4732889391091325\n",
            "************************************************************** 112\n",
            "what is the tr accuracy:  0.909\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.31327391760075757\n",
            "what is the loss v  0.4308879672832403\n",
            "what is the loss test 0.4732889391091325\n",
            "************************************************************** 113\n",
            "what is the tr accuracy:  0.9093333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.31263575304157015\n",
            "what is the loss v  0.4279210934675996\n",
            "what is the loss test 0.4724122953690732\n",
            "************************************************************** 113\n",
            "what is the tr accuracy:  0.9093333333333333\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.31263575304157015\n",
            "what is the loss v  0.4279210934675996\n",
            "what is the loss test 0.4724122953690732\n",
            "************************************************************** 114\n",
            "what is the tr accuracy:  0.9084666666666666\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.31158011823021553\n",
            "what is the loss v  0.42494010147476263\n",
            "what is the loss test 0.47140302309599075\n",
            "************************************************************** 114\n",
            "what is the tr accuracy:  0.9084666666666666\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.31158011823021553\n",
            "what is the loss v  0.42494010147476263\n",
            "what is the loss test 0.47140302309599075\n",
            "************************************************************** 115\n",
            "what is the tr accuracy:  0.9078\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.30943622217156225\n",
            "what is the loss v  0.42198956393719894\n",
            "what is the loss test 0.46973365972415276\n",
            "************************************************************** 115\n",
            "what is the tr accuracy:  0.9078\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.30943622217156225\n",
            "what is the loss v  0.42198956393719894\n",
            "what is the loss test 0.46973365972415276\n",
            "************************************************************** 116\n",
            "what is the tr accuracy:  0.9092\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.3062182029827179\n",
            "what is the loss v  0.4190386473013705\n",
            "what is the loss test 0.4673271561438485\n",
            "************************************************************** 116\n",
            "what is the tr accuracy:  0.9092\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.3062182029827179\n",
            "what is the loss v  0.4190386473013705\n",
            "what is the loss test 0.4673271561438485\n",
            "************************************************************** 117\n",
            "what is the tr accuracy:  0.9104\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8906020558002937\n",
            "what is the loss  0.3024372666858851\n",
            "what is the loss v  0.41684599041635695\n",
            "what is the loss test 0.4648582060534538\n",
            "************************************************************** 117\n",
            "what is the tr accuracy:  0.9104\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8906020558002937\n",
            "what is the loss  0.3024372666858851\n",
            "what is the loss v  0.41684599041635695\n",
            "what is the loss test 0.4648582060534538\n",
            "************************************************************** 118\n",
            "what is the tr accuracy:  0.9117333333333333\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.29914242901320465\n",
            "what is the loss v  0.41635385560873445\n",
            "what is the loss test 0.4631167081199228\n",
            "************************************************************** 118\n",
            "what is the tr accuracy:  0.9117333333333333\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.29914242901320465\n",
            "what is the loss v  0.41635385560873445\n",
            "what is the loss test 0.4631167081199228\n",
            "************************************************************** 119\n",
            "what is the tr accuracy:  0.9123333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.29728248865335916\n",
            "what is the loss v  0.41833066444268474\n",
            "what is the loss test 0.4629195868444084\n",
            "************************************************************** 119\n",
            "what is the tr accuracy:  0.9123333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.29728248865335916\n",
            "what is the loss v  0.41833066444268474\n",
            "what is the loss test 0.4629195868444084\n",
            "************************************************************** 120\n",
            "what is the tr accuracy:  0.9126666666666666\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8924375917767988\n",
            "what is the loss  0.29697262353494425\n",
            "what is the loss v  0.4227955817940904\n",
            "what is the loss test 0.46451177843591296\n",
            "************************************************************** 120\n",
            "what is the tr accuracy:  0.9126666666666666\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8924375917767988\n",
            "what is the loss  0.29697262353494425\n",
            "what is the loss v  0.4227955817940904\n",
            "what is the loss test 0.46451177843591296\n",
            "************************************************************** 121\n",
            "what is the tr accuracy:  0.9126\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.2975199353230803\n",
            "what is the loss v  0.4282852780310507\n",
            "what is the loss test 0.46709223910073194\n",
            "************************************************************** 121\n",
            "what is the tr accuracy:  0.9126\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.2975199353230803\n",
            "what is the loss v  0.4282852780310507\n",
            "what is the loss test 0.46709223910073194\n",
            "************************************************************** 122\n",
            "what is the tr accuracy:  0.9120666666666667\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.29803250430279665\n",
            "what is the loss v  0.4332602749511659\n",
            "what is the loss test 0.46986361057821524\n",
            "************************************************************** 122\n",
            "what is the tr accuracy:  0.9120666666666667\n",
            "what is the v accuracy:  0.89\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.29803250430279665\n",
            "what is the loss v  0.4332602749511659\n",
            "what is the loss test 0.46986361057821524\n",
            "************************************************************** 123\n",
            "what is the tr accuracy:  0.9125333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.29767018766894154\n",
            "what is the loss v  0.4360737263425169\n",
            "what is the loss test 0.47176257350145806\n",
            "************************************************************** 123\n",
            "what is the tr accuracy:  0.9125333333333333\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.29767018766894154\n",
            "what is the loss v  0.4360737263425169\n",
            "what is the loss test 0.47176257350145806\n",
            "************************************************************** 124\n",
            "what is the tr accuracy:  0.9124\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.2960342091419333\n",
            "what is the loss v  0.4356678270553217\n",
            "what is the loss test 0.47221942509128095\n",
            "************************************************************** 124\n",
            "what is the tr accuracy:  0.9124\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.2960342091419333\n",
            "what is the loss v  0.4356678270553217\n",
            "what is the loss test 0.47221942509128095\n",
            "************************************************************** 125\n",
            "what is the tr accuracy:  0.9126666666666666\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.2938187658625849\n",
            "what is the loss v  0.43227940307382734\n",
            "what is the loss test 0.4716680132954263\n",
            "************************************************************** 125\n",
            "what is the tr accuracy:  0.9126666666666666\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.2938187658625849\n",
            "what is the loss v  0.43227940307382734\n",
            "what is the loss test 0.4716680132954263\n",
            "************************************************************** 126\n",
            "what is the tr accuracy:  0.9126666666666666\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.2917280305275976\n",
            "what is the loss v  0.4268803094480736\n",
            "what is the loss test 0.470954959764251\n",
            "************************************************************** 126\n",
            "what is the tr accuracy:  0.9126666666666666\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.2917280305275976\n",
            "what is the loss v  0.4268803094480736\n",
            "what is the loss test 0.470954959764251\n",
            "************************************************************** 127\n",
            "what is the tr accuracy:  0.9124666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.289778194784664\n",
            "what is the loss v  0.42013982732824207\n",
            "what is the loss test 0.47009558702054505\n",
            "************************************************************** 127\n",
            "what is the tr accuracy:  0.9124666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.289778194784664\n",
            "what is the loss v  0.42013982732824207\n",
            "what is the loss test 0.47009558702054505\n",
            "************************************************************** 128\n",
            "what is the tr accuracy:  0.9138666666666667\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.2873855212891418\n",
            "what is the loss v  0.4126627212000027\n",
            "what is the loss test 0.4684522139780694\n",
            "************************************************************** 128\n",
            "what is the tr accuracy:  0.9138666666666667\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.2873855212891418\n",
            "what is the loss v  0.4126627212000027\n",
            "what is the loss test 0.4684522139780694\n",
            "************************************************************** 129\n",
            "what is the tr accuracy:  0.914\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.28432712815952477\n",
            "what is the loss v  0.4052765940115829\n",
            "what is the loss test 0.46554903122623065\n",
            "************************************************************** 129\n",
            "what is the tr accuracy:  0.914\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.28432712815952477\n",
            "what is the loss v  0.4052765940115829\n",
            "what is the loss test 0.46554903122623065\n",
            "************************************************************** 130\n",
            "what is the tr accuracy:  0.9146666666666666\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.281180396937294\n",
            "what is the loss v  0.3989357654787342\n",
            "what is the loss test 0.46183849686800066\n",
            "************************************************************** 130\n",
            "what is the tr accuracy:  0.9146666666666666\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.281180396937294\n",
            "what is the loss v  0.3989357654787342\n",
            "what is the loss test 0.46183849686800066\n",
            "************************************************************** 131\n",
            "what is the tr accuracy:  0.9157333333333333\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.2786091640330387\n",
            "what is the loss v  0.39446091444139514\n",
            "what is the loss test 0.4581560885785926\n",
            "************************************************************** 131\n",
            "what is the tr accuracy:  0.9157333333333333\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.2786091640330387\n",
            "what is the loss v  0.39446091444139514\n",
            "what is the loss test 0.4581560885785926\n",
            "************************************************************** 132\n",
            "what is the tr accuracy:  0.9156\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8906020558002937\n",
            "what is the loss  0.2773789453571279\n",
            "what is the loss v  0.3922575415264281\n",
            "what is the loss test 0.4557556636969824\n",
            "************************************************************** 132\n",
            "what is the tr accuracy:  0.9156\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8906020558002937\n",
            "what is the loss  0.2773789453571279\n",
            "what is the loss v  0.3922575415264281\n",
            "what is the loss test 0.4557556636969824\n",
            "************************************************************** 133\n",
            "what is the tr accuracy:  0.9164\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.27764235264385134\n",
            "what is the loss v  0.3925070118727339\n",
            "what is the loss test 0.4550203978745682\n",
            "************************************************************** 133\n",
            "what is the tr accuracy:  0.9164\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.27764235264385134\n",
            "what is the loss v  0.3925070118727339\n",
            "what is the loss test 0.4550203978745682\n",
            "************************************************************** 134\n",
            "what is the tr accuracy:  0.916\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.2785555755007431\n",
            "what is the loss v  0.39377405817365707\n",
            "what is the loss test 0.45531341834276445\n",
            "************************************************************** 134\n",
            "what is the tr accuracy:  0.916\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.2785555755007431\n",
            "what is the loss v  0.39377405817365707\n",
            "what is the loss test 0.45531341834276445\n",
            "************************************************************** 135\n",
            "what is the tr accuracy:  0.9161333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.27860606701005186\n",
            "what is the loss v  0.3944949264154602\n",
            "what is the loss test 0.4553166013598989\n",
            "************************************************************** 135\n",
            "what is the tr accuracy:  0.9161333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.27860606701005186\n",
            "what is the loss v  0.3944949264154602\n",
            "what is the loss test 0.4553166013598989\n",
            "************************************************************** 136\n",
            "what is the tr accuracy:  0.9159333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.2767075881079002\n",
            "what is the loss v  0.3936782204510082\n",
            "what is the loss test 0.453719801213398\n",
            "************************************************************** 136\n",
            "what is the tr accuracy:  0.9159333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.2767075881079002\n",
            "what is the loss v  0.3936782204510082\n",
            "what is the loss test 0.453719801213398\n",
            "************************************************************** 137\n",
            "what is the tr accuracy:  0.9168\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.27328109081313\n",
            "what is the loss v  0.3917492564390364\n",
            "what is the loss test 0.45115669052495444\n",
            "************************************************************** 137\n",
            "what is the tr accuracy:  0.9168\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.27328109081313\n",
            "what is the loss v  0.3917492564390364\n",
            "what is the loss test 0.45115669052495444\n",
            "************************************************************** 138\n",
            "what is the tr accuracy:  0.9169333333333334\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.26981410008118106\n",
            "what is the loss v  0.38992939707737945\n",
            "what is the loss test 0.4493948263152286\n",
            "************************************************************** 138\n",
            "what is the tr accuracy:  0.9169333333333334\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.26981410008118106\n",
            "what is the loss v  0.38992939707737945\n",
            "what is the loss test 0.4493948263152286\n",
            "************************************************************** 139\n",
            "what is the tr accuracy:  0.9174\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8924375917767988\n",
            "what is the loss  0.2672997549730789\n",
            "what is the loss v  0.38932148424565016\n",
            "what is the loss test 0.4492451010993141\n",
            "************************************************************** 139\n",
            "what is the tr accuracy:  0.9174\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8924375917767988\n",
            "what is the loss  0.2672997549730789\n",
            "what is the loss v  0.38932148424565016\n",
            "what is the loss test 0.4492451010993141\n",
            "************************************************************** 140\n",
            "what is the tr accuracy:  0.9178666666666667\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.893906020558003\n",
            "what is the loss  0.26555177330401725\n",
            "what is the loss v  0.3901867560742002\n",
            "what is the loss test 0.4503479836109749\n",
            "************************************************************** 140\n",
            "what is the tr accuracy:  0.9178666666666667\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.893906020558003\n",
            "what is the loss  0.26555177330401725\n",
            "what is the loss v  0.3901867560742002\n",
            "what is the loss test 0.4503479836109749\n",
            "************************************************************** 141\n",
            "what is the tr accuracy:  0.9188\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.2639148415706098\n",
            "what is the loss v  0.39248654911851016\n",
            "what is the loss test 0.4516214833324159\n",
            "************************************************************** 141\n",
            "what is the tr accuracy:  0.9188\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.2639148415706098\n",
            "what is the loss v  0.39248654911851016\n",
            "what is the loss test 0.4516214833324159\n",
            "************************************************************** 142\n",
            "what is the tr accuracy:  0.9187333333333333\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.26219131734433165\n",
            "what is the loss v  0.39607856118406753\n",
            "what is the loss test 0.4526814169778796\n",
            "************************************************************** 142\n",
            "what is the tr accuracy:  0.9187333333333333\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.26219131734433165\n",
            "what is the loss v  0.39607856118406753\n",
            "what is the loss test 0.4526814169778796\n",
            "************************************************************** 143\n",
            "what is the tr accuracy:  0.9186666666666666\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8942731277533039\n",
            "what is the loss  0.260788670303284\n",
            "what is the loss v  0.40087131426057704\n",
            "what is the loss test 0.4538218201925593\n",
            "************************************************************** 143\n",
            "what is the tr accuracy:  0.9186666666666666\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8942731277533039\n",
            "what is the loss  0.260788670303284\n",
            "what is the loss v  0.40087131426057704\n",
            "what is the loss test 0.4538218201925593\n",
            "************************************************************** 144\n",
            "what is the tr accuracy:  0.9192666666666667\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.2600826636142096\n",
            "what is the loss v  0.40637637369814217\n",
            "what is the loss test 0.45535111247973586\n",
            "************************************************************** 144\n",
            "what is the tr accuracy:  0.9192666666666667\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.2600826636142096\n",
            "what is the loss v  0.40637637369814217\n",
            "what is the loss test 0.45535111247973586\n",
            "************************************************************** 145\n",
            "what is the tr accuracy:  0.9196\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8942731277533039\n",
            "what is the loss  0.25999008599119006\n",
            "what is the loss v  0.41138330138722834\n",
            "what is the loss test 0.4572487366158017\n",
            "************************************************************** 145\n",
            "what is the tr accuracy:  0.9196\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8942731277533039\n",
            "what is the loss  0.25999008599119006\n",
            "what is the loss v  0.41138330138722834\n",
            "what is the loss test 0.4572487366158017\n",
            "************************************************************** 146\n",
            "what is the tr accuracy:  0.9196666666666666\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.25966070381989387\n",
            "what is the loss v  0.41404298503134634\n",
            "what is the loss test 0.45862878465130424\n",
            "************************************************************** 146\n",
            "what is the tr accuracy:  0.9196666666666666\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.25966070381989387\n",
            "what is the loss v  0.41404298503134634\n",
            "what is the loss test 0.45862878465130424\n",
            "************************************************************** 147\n",
            "what is the tr accuracy:  0.9206666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8931718061674009\n",
            "what is the loss  0.2584641098518819\n",
            "what is the loss v  0.41376266410957113\n",
            "what is the loss test 0.45877732713881086\n",
            "************************************************************** 147\n",
            "what is the tr accuracy:  0.9206666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8931718061674009\n",
            "what is the loss  0.2584641098518819\n",
            "what is the loss v  0.41376266410957113\n",
            "what is the loss test 0.45877732713881086\n",
            "************************************************************** 148\n",
            "what is the tr accuracy:  0.9206\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.2564039252824386\n",
            "what is the loss v  0.41082592650198824\n",
            "what is the loss test 0.4577057465075165\n",
            "************************************************************** 148\n",
            "what is the tr accuracy:  0.9206\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.2564039252824386\n",
            "what is the loss v  0.41082592650198824\n",
            "what is the loss test 0.4577057465075165\n",
            "************************************************************** 149\n",
            "what is the tr accuracy:  0.9207333333333333\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8902349486049926\n",
            "what is the loss  0.25418329887201285\n",
            "what is the loss v  0.40610434223075287\n",
            "what is the loss test 0.45631874537170025\n",
            "************************************************************** 149\n",
            "what is the tr accuracy:  0.9207333333333333\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8902349486049926\n",
            "what is the loss  0.25418329887201285\n",
            "what is the loss v  0.40610434223075287\n",
            "what is the loss test 0.45631874537170025\n",
            "************************************************************** 150\n",
            "what is the tr accuracy:  0.9206\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8906020558002937\n",
            "what is the loss  0.25244010903367736\n",
            "what is the loss v  0.401192677010881\n",
            "what is the loss test 0.4552440699805837\n",
            "************************************************************** 150\n",
            "what is the tr accuracy:  0.9206\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8906020558002937\n",
            "what is the loss  0.25244010903367736\n",
            "what is the loss v  0.401192677010881\n",
            "what is the loss test 0.4552440699805837\n",
            "************************************************************** 151\n",
            "what is the tr accuracy:  0.9215333333333333\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.2510660553295189\n",
            "what is the loss v  0.39672688765965997\n",
            "what is the loss test 0.45458178129967225\n",
            "************************************************************** 151\n",
            "what is the tr accuracy:  0.9215333333333333\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.2510660553295189\n",
            "what is the loss v  0.39672688765965997\n",
            "what is the loss test 0.45458178129967225\n",
            "************************************************************** 152\n",
            "what is the tr accuracy:  0.9216666666666666\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.24968342491473133\n",
            "what is the loss v  0.39311061124848756\n",
            "what is the loss test 0.454087848103658\n",
            "************************************************************** 152\n",
            "what is the tr accuracy:  0.9216666666666666\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.24968342491473133\n",
            "what is the loss v  0.39311061124848756\n",
            "what is the loss test 0.454087848103658\n",
            "************************************************************** 153\n",
            "what is the tr accuracy:  0.922\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.24818012877242066\n",
            "what is the loss v  0.39081518272144705\n",
            "what is the loss test 0.4538241892112748\n",
            "************************************************************** 153\n",
            "what is the tr accuracy:  0.922\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.24818012877242066\n",
            "what is the loss v  0.39081518272144705\n",
            "what is the loss test 0.4538241892112748\n",
            "************************************************************** 154\n",
            "what is the tr accuracy:  0.9226666666666666\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.24684910193176457\n",
            "what is the loss v  0.39045607073473204\n",
            "what is the loss test 0.4539566222567893\n",
            "************************************************************** 154\n",
            "what is the tr accuracy:  0.9226666666666666\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.24684910193176457\n",
            "what is the loss v  0.39045607073473204\n",
            "what is the loss test 0.4539566222567893\n",
            "************************************************************** 155\n",
            "what is the tr accuracy:  0.9229333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.2460188517940279\n",
            "what is the loss v  0.3917820768605909\n",
            "what is the loss test 0.45467349087206216\n",
            "************************************************************** 155\n",
            "what is the tr accuracy:  0.9229333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.2460188517940279\n",
            "what is the loss v  0.3917820768605909\n",
            "what is the loss test 0.45467349087206216\n",
            "************************************************************** 156\n",
            "what is the tr accuracy:  0.9225333333333333\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.24569548618058712\n",
            "what is the loss v  0.39417886540090924\n",
            "what is the loss test 0.45597545735371814\n",
            "************************************************************** 156\n",
            "what is the tr accuracy:  0.9225333333333333\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.24569548618058712\n",
            "what is the loss v  0.39417886540090924\n",
            "what is the loss test 0.45597545735371814\n",
            "************************************************************** 157\n",
            "what is the tr accuracy:  0.9233333333333333\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.24519067350063808\n",
            "what is the loss v  0.3968053973137305\n",
            "what is the loss test 0.457024314156674\n",
            "************************************************************** 157\n",
            "what is the tr accuracy:  0.9233333333333333\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.24519067350063808\n",
            "what is the loss v  0.3968053973137305\n",
            "what is the loss test 0.457024314156674\n",
            "************************************************************** 158\n",
            "what is the tr accuracy:  0.9239333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.24377074795830286\n",
            "what is the loss v  0.3987158022354589\n",
            "what is the loss test 0.45700588275748383\n",
            "************************************************************** 158\n",
            "what is the tr accuracy:  0.9239333333333334\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.24377074795830286\n",
            "what is the loss v  0.3987158022354589\n",
            "what is the loss test 0.45700588275748383\n",
            "************************************************************** 159\n",
            "what is the tr accuracy:  0.9254\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8924375917767988\n",
            "what is the loss  0.24133000624530007\n",
            "what is the loss v  0.3993597408836413\n",
            "what is the loss test 0.45576412423620877\n",
            "************************************************************** 159\n",
            "what is the tr accuracy:  0.9254\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8924375917767988\n",
            "what is the loss  0.24133000624530007\n",
            "what is the loss v  0.3993597408836413\n",
            "what is the loss test 0.45576412423620877\n",
            "************************************************************** 160\n",
            "what is the tr accuracy:  0.925\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.23865306445850246\n",
            "what is the loss v  0.3991896670828085\n",
            "what is the loss test 0.45400147547134106\n",
            "************************************************************** 160\n",
            "what is the tr accuracy:  0.925\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.23865306445850246\n",
            "what is the loss v  0.3991896670828085\n",
            "what is the loss test 0.45400147547134106\n",
            "************************************************************** 161\n",
            "what is the tr accuracy:  0.9258\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.2366112410368906\n",
            "what is the loss v  0.3987292406933291\n",
            "what is the loss test 0.4527669149919432\n",
            "************************************************************** 161\n",
            "what is the tr accuracy:  0.9258\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8898678414096917\n",
            "what is the loss  0.2366112410368906\n",
            "what is the loss v  0.3987292406933291\n",
            "what is the loss test 0.4527669149919432\n",
            "************************************************************** 162\n",
            "what is the tr accuracy:  0.9258666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.23554227474989092\n",
            "what is the loss v  0.39859547441881077\n",
            "what is the loss test 0.45224307044412987\n",
            "************************************************************** 162\n",
            "what is the tr accuracy:  0.9258666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.23554227474989092\n",
            "what is the loss v  0.39859547441881077\n",
            "what is the loss test 0.45224307044412987\n",
            "************************************************************** 163\n",
            "what is the tr accuracy:  0.9253333333333333\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.23513764002188994\n",
            "what is the loss v  0.3989757700592717\n",
            "what is the loss test 0.4522296338637479\n",
            "************************************************************** 163\n",
            "what is the tr accuracy:  0.9253333333333333\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.23513764002188994\n",
            "what is the loss v  0.3989757700592717\n",
            "what is the loss test 0.4522296338637479\n",
            "************************************************************** 164\n",
            "what is the tr accuracy:  0.9249333333333334\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.23469374459685363\n",
            "what is the loss v  0.40000689842331244\n",
            "what is the loss test 0.45199440534993723\n",
            "************************************************************** 164\n",
            "what is the tr accuracy:  0.9249333333333334\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.23469374459685363\n",
            "what is the loss v  0.40000689842331244\n",
            "what is the loss test 0.45199440534993723\n",
            "************************************************************** 165\n",
            "what is the tr accuracy:  0.9258\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.23397214644430303\n",
            "what is the loss v  0.40165207707376627\n",
            "what is the loss test 0.4516016365619178\n",
            "************************************************************** 165\n",
            "what is the tr accuracy:  0.9258\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8880323054331865\n",
            "what is the loss  0.23397214644430303\n",
            "what is the loss v  0.40165207707376627\n",
            "what is the loss test 0.4516016365619178\n",
            "************************************************************** 166\n",
            "what is the tr accuracy:  0.9260666666666667\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.2333496646016635\n",
            "what is the loss v  0.40404506045218225\n",
            "what is the loss test 0.45158603720986057\n",
            "************************************************************** 166\n",
            "what is the tr accuracy:  0.9260666666666667\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8883994126284875\n",
            "what is the loss  0.2333496646016635\n",
            "what is the loss v  0.40404506045218225\n",
            "what is the loss test 0.45158603720986057\n",
            "************************************************************** 167\n",
            "what is the tr accuracy:  0.9268666666666666\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.2328405200701958\n",
            "what is the loss v  0.40663213218632743\n",
            "what is the loss test 0.4520482652619241\n",
            "************************************************************** 167\n",
            "what is the tr accuracy:  0.9268666666666666\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.2328405200701958\n",
            "what is the loss v  0.40663213218632743\n",
            "what is the loss test 0.4520482652619241\n",
            "************************************************************** 168\n",
            "what is the tr accuracy:  0.9268\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.23182181737664592\n",
            "what is the loss v  0.4079555072232063\n",
            "what is the loss test 0.4521424672691657\n",
            "************************************************************** 168\n",
            "what is the tr accuracy:  0.9268\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8887665198237885\n",
            "what is the loss  0.23182181737664592\n",
            "what is the loss v  0.4079555072232063\n",
            "what is the loss test 0.4521424672691657\n",
            "************************************************************** 169\n",
            "what is the tr accuracy:  0.9271333333333334\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8902349486049926\n",
            "what is the loss  0.2296030773851703\n",
            "what is the loss v  0.4069460984401128\n",
            "what is the loss test 0.45091012980314826\n",
            "************************************************************** 169\n",
            "what is the tr accuracy:  0.9271333333333334\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8902349486049926\n",
            "what is the loss  0.2296030773851703\n",
            "what is the loss v  0.4069460984401128\n",
            "what is the loss test 0.45091012980314826\n",
            "************************************************************** 170\n",
            "what is the tr accuracy:  0.9276\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.226363320388911\n",
            "what is the loss v  0.40403896378764653\n",
            "what is the loss test 0.44851780839351696\n",
            "************************************************************** 170\n",
            "what is the tr accuracy:  0.9276\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.226363320388911\n",
            "what is the loss v  0.40403896378764653\n",
            "what is the loss test 0.44851780839351696\n",
            "************************************************************** 171\n",
            "what is the tr accuracy:  0.9294666666666667\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.2231542552025169\n",
            "what is the loss v  0.4004125125471718\n",
            "what is the loss test 0.44623596593119286\n",
            "************************************************************** 171\n",
            "what is the tr accuracy:  0.9294666666666667\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.2231542552025169\n",
            "what is the loss v  0.4004125125471718\n",
            "what is the loss test 0.44623596593119286\n",
            "************************************************************** 172\n",
            "what is the tr accuracy:  0.93\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.22082694550528642\n",
            "what is the loss v  0.3977638705999763\n",
            "what is the loss test 0.44488625401524895\n",
            "************************************************************** 172\n",
            "what is the tr accuracy:  0.93\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.22082694550528642\n",
            "what is the loss v  0.3977638705999763\n",
            "what is the loss test 0.44488625401524895\n",
            "************************************************************** 173\n",
            "what is the tr accuracy:  0.9305333333333333\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.21965600231076204\n",
            "what is the loss v  0.39685555623957963\n",
            "what is the loss test 0.44513666891629794\n",
            "************************************************************** 173\n",
            "what is the tr accuracy:  0.9305333333333333\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.21965600231076204\n",
            "what is the loss v  0.39685555623957963\n",
            "what is the loss test 0.44513666891629794\n",
            "************************************************************** 174\n",
            "what is the tr accuracy:  0.9307333333333333\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.2191567456561072\n",
            "what is the loss v  0.39735363062767387\n",
            "what is the loss test 0.4467312860655559\n",
            "************************************************************** 174\n",
            "what is the tr accuracy:  0.9307333333333333\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8935389133627019\n",
            "what is the loss  0.2191567456561072\n",
            "what is the loss v  0.39735363062767387\n",
            "what is the loss test 0.4467312860655559\n",
            "************************************************************** 175\n",
            "what is the tr accuracy:  0.9305333333333333\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.21885988005001789\n",
            "what is the loss v  0.39876625598807847\n",
            "what is the loss test 0.44935860728096816\n",
            "************************************************************** 175\n",
            "what is the tr accuracy:  0.9305333333333333\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.21885988005001789\n",
            "what is the loss v  0.39876625598807847\n",
            "what is the loss test 0.44935860728096816\n",
            "************************************************************** 176\n",
            "what is the tr accuracy:  0.9306666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.21855010134975078\n",
            "what is the loss v  0.4004815035829157\n",
            "what is the loss test 0.4524866378400231\n",
            "************************************************************** 176\n",
            "what is the tr accuracy:  0.9306666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.21855010134975078\n",
            "what is the loss v  0.4004815035829157\n",
            "what is the loss test 0.4524866378400231\n",
            "************************************************************** 177\n",
            "what is the tr accuracy:  0.931\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.21821580543330063\n",
            "what is the loss v  0.40206799690614425\n",
            "what is the loss test 0.4557629655014624\n",
            "************************************************************** 177\n",
            "what is the tr accuracy:  0.931\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.21821580543330063\n",
            "what is the loss v  0.40206799690614425\n",
            "what is the loss test 0.4557629655014624\n",
            "************************************************************** 178\n",
            "what is the tr accuracy:  0.9307333333333333\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8990455212922174\n",
            "what is the loss  0.2178482361998806\n",
            "what is the loss v  0.40325604817751065\n",
            "what is the loss test 0.45875048303581895\n",
            "************************************************************** 178\n",
            "what is the tr accuracy:  0.9307333333333333\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8990455212922174\n",
            "what is the loss  0.2178482361998806\n",
            "what is the loss v  0.40325604817751065\n",
            "what is the loss test 0.45875048303581895\n",
            "************************************************************** 179\n",
            "what is the tr accuracy:  0.931\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8986784140969163\n",
            "what is the loss  0.21736500848237478\n",
            "what is the loss v  0.4038977289831092\n",
            "what is the loss test 0.46116358941395097\n",
            "************************************************************** 179\n",
            "what is the tr accuracy:  0.931\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8986784140969163\n",
            "what is the loss  0.21736500848237478\n",
            "what is the loss v  0.4038977289831092\n",
            "what is the loss test 0.46116358941395097\n",
            "************************************************************** 180\n",
            "what is the tr accuracy:  0.9309333333333333\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8990455212922174\n",
            "what is the loss  0.2165030726479418\n",
            "what is the loss v  0.40358335831413333\n",
            "what is the loss test 0.46265649435017775\n",
            "************************************************************** 180\n",
            "what is the tr accuracy:  0.9309333333333333\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8990455212922174\n",
            "what is the loss  0.2165030726479418\n",
            "what is the loss v  0.40358335831413333\n",
            "what is the loss test 0.46265649435017775\n",
            "************************************************************** 181\n",
            "what is the tr accuracy:  0.9314666666666667\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.21505578584513046\n",
            "what is the loss v  0.4023452654277211\n",
            "what is the loss test 0.46313785364253496\n",
            "************************************************************** 181\n",
            "what is the tr accuracy:  0.9314666666666667\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.21505578584513046\n",
            "what is the loss v  0.4023452654277211\n",
            "what is the loss test 0.46313785364253496\n",
            "************************************************************** 182\n",
            "what is the tr accuracy:  0.9316666666666666\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8986784140969163\n",
            "what is the loss  0.21349420716215178\n",
            "what is the loss v  0.40126784949612543\n",
            "what is the loss test 0.463215295600804\n",
            "************************************************************** 182\n",
            "what is the tr accuracy:  0.9316666666666666\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8986784140969163\n",
            "what is the loss  0.21349420716215178\n",
            "what is the loss v  0.40126784949612543\n",
            "what is the loss test 0.463215295600804\n",
            "************************************************************** 183\n",
            "what is the tr accuracy:  0.9324\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.2123956869592849\n",
            "what is the loss v  0.40136494245625426\n",
            "what is the loss test 0.46357763897448007\n",
            "************************************************************** 183\n",
            "what is the tr accuracy:  0.9324\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.2123956869592849\n",
            "what is the loss v  0.40136494245625426\n",
            "what is the loss test 0.46357763897448007\n",
            "************************************************************** 184\n",
            "what is the tr accuracy:  0.9328666666666666\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.21157151752912923\n",
            "what is the loss v  0.4029153733508466\n",
            "what is the loss test 0.46418191121098235\n",
            "************************************************************** 184\n",
            "what is the tr accuracy:  0.9328666666666666\n",
            "what is the v accuracy:  0.897\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.21157151752912923\n",
            "what is the loss v  0.4029153733508466\n",
            "what is the loss test 0.46418191121098235\n",
            "************************************************************** 185\n",
            "what is the tr accuracy:  0.933\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.2103483048179665\n",
            "what is the loss v  0.405551659773735\n",
            "what is the loss test 0.46480559103882935\n",
            "************************************************************** 185\n",
            "what is the tr accuracy:  0.933\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.2103483048179665\n",
            "what is the loss v  0.405551659773735\n",
            "what is the loss test 0.46480559103882935\n",
            "************************************************************** 186\n",
            "what is the tr accuracy:  0.9336666666666666\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.20858662090099545\n",
            "what is the loss v  0.4086617702205668\n",
            "what is the loss test 0.4652996300271156\n",
            "************************************************************** 186\n",
            "what is the tr accuracy:  0.9336666666666666\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.20858662090099545\n",
            "what is the loss v  0.4086617702205668\n",
            "what is the loss test 0.4652996300271156\n",
            "************************************************************** 187\n",
            "what is the tr accuracy:  0.9340666666666667\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.20696264377922943\n",
            "what is the loss v  0.4121319497449561\n",
            "what is the loss test 0.4659617997002355\n",
            "************************************************************** 187\n",
            "what is the tr accuracy:  0.9340666666666667\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.20696264377922943\n",
            "what is the loss v  0.4121319497449561\n",
            "what is the loss test 0.4659617997002355\n",
            "************************************************************** 188\n",
            "what is the tr accuracy:  0.9348\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.895741556534508\n",
            "what is the loss  0.20596484268720758\n",
            "what is the loss v  0.41566812879235043\n",
            "what is the loss test 0.46700263214350785\n",
            "************************************************************** 188\n",
            "what is the tr accuracy:  0.9348\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.895741556534508\n",
            "what is the loss  0.20596484268720758\n",
            "what is the loss v  0.41566812879235043\n",
            "what is the loss test 0.46700263214350785\n",
            "************************************************************** 189\n",
            "what is the tr accuracy:  0.9345333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.20541112456228106\n",
            "what is the loss v  0.4185976019030779\n",
            "what is the loss test 0.4680234073523105\n",
            "************************************************************** 189\n",
            "what is the tr accuracy:  0.9345333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.20541112456228106\n",
            "what is the loss v  0.4185976019030779\n",
            "what is the loss test 0.4680234073523105\n",
            "************************************************************** 190\n",
            "what is the tr accuracy:  0.9354\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.20475128025539316\n",
            "what is the loss v  0.4197743819494987\n",
            "what is the loss test 0.46822261914637636\n",
            "************************************************************** 190\n",
            "what is the tr accuracy:  0.9354\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.20475128025539316\n",
            "what is the loss v  0.4197743819494987\n",
            "what is the loss test 0.46822261914637636\n",
            "************************************************************** 191\n",
            "what is the tr accuracy:  0.936\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.895741556534508\n",
            "what is the loss  0.20353335023946878\n",
            "what is the loss v  0.418691001484681\n",
            "what is the loss test 0.46713448019427506\n",
            "************************************************************** 191\n",
            "what is the tr accuracy:  0.936\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.895741556534508\n",
            "what is the loss  0.20353335023946878\n",
            "what is the loss v  0.418691001484681\n",
            "what is the loss test 0.46713448019427506\n",
            "************************************************************** 192\n",
            "what is the tr accuracy:  0.9366666666666666\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.20191039158824478\n",
            "what is the loss v  0.41561001402937536\n",
            "what is the loss test 0.46520328530544514\n",
            "************************************************************** 192\n",
            "what is the tr accuracy:  0.9366666666666666\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.20191039158824478\n",
            "what is the loss v  0.41561001402937536\n",
            "what is the loss test 0.46520328530544514\n",
            "************************************************************** 193\n",
            "what is the tr accuracy:  0.9362666666666667\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.20050011822133984\n",
            "what is the loss v  0.4111347369049903\n",
            "what is the loss test 0.46324103178539733\n",
            "************************************************************** 193\n",
            "what is the tr accuracy:  0.9362666666666667\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.20050011822133984\n",
            "what is the loss v  0.4111347369049903\n",
            "what is the loss test 0.46324103178539733\n",
            "************************************************************** 194\n",
            "what is the tr accuracy:  0.9363333333333334\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8979441997063142\n",
            "what is the loss  0.19973972817706254\n",
            "what is the loss v  0.4066693317026229\n",
            "what is the loss test 0.4620496043039516\n",
            "************************************************************** 194\n",
            "what is the tr accuracy:  0.9363333333333334\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8979441997063142\n",
            "what is the loss  0.19973972817706254\n",
            "what is the loss v  0.4066693317026229\n",
            "what is the loss test 0.4620496043039516\n",
            "************************************************************** 195\n",
            "what is the tr accuracy:  0.9362\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.19949887732734428\n",
            "what is the loss v  0.40294630504821555\n",
            "what is the loss test 0.46179645953848514\n",
            "************************************************************** 195\n",
            "what is the tr accuracy:  0.9362\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.19949887732734428\n",
            "what is the loss v  0.40294630504821555\n",
            "what is the loss test 0.46179645953848514\n",
            "************************************************************** 196\n",
            "what is the tr accuracy:  0.9362\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.19917106161975226\n",
            "what is the loss v  0.3998500563597763\n",
            "what is the loss test 0.4620252430964253\n",
            "************************************************************** 196\n",
            "what is the tr accuracy:  0.9362\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.19917106161975226\n",
            "what is the loss v  0.3998500563597763\n",
            "what is the loss test 0.4620252430964253\n",
            "************************************************************** 197\n",
            "what is the tr accuracy:  0.9366\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8975770925110133\n",
            "what is the loss  0.19842546214382276\n",
            "what is the loss v  0.39773301236938274\n",
            "what is the loss test 0.46236677931387204\n",
            "************************************************************** 197\n",
            "what is the tr accuracy:  0.9366\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8975770925110133\n",
            "what is the loss  0.19842546214382276\n",
            "what is the loss v  0.39773301236938274\n",
            "what is the loss test 0.46236677931387204\n",
            "************************************************************** 198\n",
            "what is the tr accuracy:  0.9367333333333333\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.19742891010217042\n",
            "what is the loss v  0.3967827354497119\n",
            "what is the loss test 0.4630106892953324\n",
            "************************************************************** 198\n",
            "what is the tr accuracy:  0.9367333333333333\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.19742891010217042\n",
            "what is the loss v  0.3967827354497119\n",
            "what is the loss test 0.4630106892953324\n",
            "************************************************************** 199\n",
            "what is the tr accuracy:  0.9368\n",
            "what is the v accuracy:  0.906\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.1965538693821128\n",
            "what is the loss v  0.3969299178486272\n",
            "what is the loss test 0.46411708786092415\n",
            "************************************************************** 199\n",
            "what is the tr accuracy:  0.9368\n",
            "what is the v accuracy:  0.906\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.1965538693821128\n",
            "what is the loss v  0.3969299178486272\n",
            "what is the loss test 0.46411708786092415\n",
            "min loss is 0.44488625401524895 at epoch 172\n",
            "min loss is 0.44488625401524895 at epoch 172\n",
            "************************************************************** 0\n",
            "what is the tr accuracy:  0.10273333333333333\n",
            "what is the v accuracy:  0.115\n",
            "what is the t accuracy:  0.10425844346549193\n",
            "what is the loss  2.5887468154453828\n",
            "what is the loss v  2.56657645814129\n",
            "what is the loss test 2.5873804143771633\n",
            "************************************************************** 0\n",
            "what is the tr accuracy:  0.10273333333333333\n",
            "what is the v accuracy:  0.115\n",
            "what is the t accuracy:  0.10425844346549193\n",
            "what is the loss  2.5887468154453828\n",
            "what is the loss v  2.56657645814129\n",
            "what is the loss test 2.5873804143771633\n",
            "************************************************************** 1\n",
            "what is the tr accuracy:  0.23586666666666667\n",
            "what is the v accuracy:  0.235\n",
            "what is the t accuracy:  0.2474302496328928\n",
            "what is the loss  2.2415893401107487\n",
            "what is the loss v  2.2843652458208625\n",
            "what is the loss test 2.2252991073157795\n",
            "************************************************************** 1\n",
            "what is the tr accuracy:  0.23586666666666667\n",
            "what is the v accuracy:  0.235\n",
            "what is the t accuracy:  0.2474302496328928\n",
            "what is the loss  2.2415893401107487\n",
            "what is the loss v  2.2843652458208625\n",
            "what is the loss test 2.2252991073157795\n",
            "************************************************************** 2\n",
            "what is the tr accuracy:  0.5519333333333334\n",
            "what is the v accuracy:  0.552\n",
            "what is the t accuracy:  0.540381791483113\n",
            "what is the loss  1.8096372657100377\n",
            "what is the loss v  1.8392304810931752\n",
            "what is the loss test 1.8218275069811098\n",
            "************************************************************** 2\n",
            "what is the tr accuracy:  0.5519333333333334\n",
            "what is the v accuracy:  0.552\n",
            "what is the t accuracy:  0.540381791483113\n",
            "what is the loss  1.8096372657100377\n",
            "what is the loss v  1.8392304810931752\n",
            "what is the loss test 1.8218275069811098\n",
            "************************************************************** 3\n",
            "what is the tr accuracy:  0.6188666666666667\n",
            "what is the v accuracy:  0.617\n",
            "what is the t accuracy:  0.6163729809104258\n",
            "what is the loss  1.2530089459294294\n",
            "what is the loss v  1.2628031413607013\n",
            "what is the loss test 1.231868360488869\n",
            "************************************************************** 3\n",
            "what is the tr accuracy:  0.6188666666666667\n",
            "what is the v accuracy:  0.617\n",
            "what is the t accuracy:  0.6163729809104258\n",
            "what is the loss  1.2530089459294294\n",
            "what is the loss v  1.2628031413607013\n",
            "what is the loss test 1.231868360488869\n",
            "************************************************************** 4\n",
            "what is the tr accuracy:  0.7209333333333333\n",
            "what is the v accuracy:  0.712\n",
            "what is the t accuracy:  0.7220998531571219\n",
            "what is the loss  0.9689610344516821\n",
            "what is the loss v  1.0132402567145695\n",
            "what is the loss test 0.9624128112563303\n",
            "************************************************************** 4\n",
            "what is the tr accuracy:  0.7209333333333333\n",
            "what is the v accuracy:  0.712\n",
            "what is the t accuracy:  0.7220998531571219\n",
            "what is the loss  0.9689610344516821\n",
            "what is the loss v  1.0132402567145695\n",
            "what is the loss test 0.9624128112563303\n",
            "************************************************************** 5\n",
            "what is the tr accuracy:  0.6320666666666667\n",
            "what is the v accuracy:  0.621\n",
            "what is the t accuracy:  0.6339941262848752\n",
            "what is the loss  1.2513420828793558\n",
            "what is the loss v  1.2839597717744193\n",
            "what is the loss test 1.2769506255885381\n",
            "************************************************************** 5\n",
            "what is the tr accuracy:  0.6320666666666667\n",
            "what is the v accuracy:  0.621\n",
            "what is the t accuracy:  0.6339941262848752\n",
            "what is the loss  1.2513420828793558\n",
            "what is the loss v  1.2839597717744193\n",
            "what is the loss test 1.2769506255885381\n",
            "************************************************************** 6\n",
            "what is the tr accuracy:  0.7718666666666667\n",
            "what is the v accuracy:  0.768\n",
            "what is the t accuracy:  0.7687224669603524\n",
            "what is the loss  0.8632657855619134\n",
            "what is the loss v  0.8829884710498936\n",
            "what is the loss test 0.8601722000828574\n",
            "************************************************************** 6\n",
            "what is the tr accuracy:  0.7718666666666667\n",
            "what is the v accuracy:  0.768\n",
            "what is the t accuracy:  0.7687224669603524\n",
            "what is the loss  0.8632657855619134\n",
            "what is the loss v  0.8829884710498936\n",
            "what is the loss test 0.8601722000828574\n",
            "************************************************************** 7\n",
            "what is the tr accuracy:  0.7187333333333333\n",
            "what is the v accuracy:  0.716\n",
            "what is the t accuracy:  0.7334801762114538\n",
            "what is the loss  0.8413871154199566\n",
            "what is the loss v  0.8281029525319981\n",
            "what is the loss test 0.7937630903550312\n",
            "************************************************************** 7\n",
            "what is the tr accuracy:  0.7187333333333333\n",
            "what is the v accuracy:  0.716\n",
            "what is the t accuracy:  0.7334801762114538\n",
            "what is the loss  0.8413871154199566\n",
            "what is the loss v  0.8281029525319981\n",
            "what is the loss test 0.7937630903550312\n",
            "************************************************************** 8\n",
            "what is the tr accuracy:  0.7501333333333333\n",
            "what is the v accuracy:  0.744\n",
            "what is the t accuracy:  0.7599118942731278\n",
            "what is the loss  0.942044512981113\n",
            "what is the loss v  0.9372387466717041\n",
            "what is the loss test 0.8749089896296176\n",
            "************************************************************** 8\n",
            "what is the tr accuracy:  0.7501333333333333\n",
            "what is the v accuracy:  0.744\n",
            "what is the t accuracy:  0.7599118942731278\n",
            "what is the loss  0.942044512981113\n",
            "what is the loss v  0.9372387466717041\n",
            "what is the loss test 0.8749089896296176\n",
            "************************************************************** 9\n",
            "what is the tr accuracy:  0.7848\n",
            "what is the v accuracy:  0.794\n",
            "what is the t accuracy:  0.7944199706314243\n",
            "what is the loss  0.7800915156148279\n",
            "what is the loss v  0.7601850237577386\n",
            "what is the loss test 0.7275530659281024\n",
            "************************************************************** 9\n",
            "what is the tr accuracy:  0.7848\n",
            "what is the v accuracy:  0.794\n",
            "what is the t accuracy:  0.7944199706314243\n",
            "what is the loss  0.7800915156148279\n",
            "what is the loss v  0.7601850237577386\n",
            "what is the loss test 0.7275530659281024\n",
            "************************************************************** 10\n",
            "what is the tr accuracy:  0.8526666666666667\n",
            "what is the v accuracy:  0.85\n",
            "what is the t accuracy:  0.855359765051395\n",
            "what is the loss  0.5981534321314195\n",
            "what is the loss v  0.5834152603207412\n",
            "what is the loss test 0.58389073874548\n",
            "************************************************************** 10\n",
            "what is the tr accuracy:  0.8526666666666667\n",
            "what is the v accuracy:  0.85\n",
            "what is the t accuracy:  0.855359765051395\n",
            "what is the loss  0.5981534321314195\n",
            "what is the loss v  0.5834152603207412\n",
            "what is the loss test 0.58389073874548\n",
            "************************************************************** 11\n",
            "what is the tr accuracy:  0.8287333333333333\n",
            "what is the v accuracy:  0.829\n",
            "what is the t accuracy:  0.8344346549192364\n",
            "what is the loss  0.707337174305306\n",
            "what is the loss v  0.6997788553278003\n",
            "what is the loss test 0.7152492486135756\n",
            "************************************************************** 11\n",
            "what is the tr accuracy:  0.8287333333333333\n",
            "what is the v accuracy:  0.829\n",
            "what is the t accuracy:  0.8344346549192364\n",
            "what is the loss  0.707337174305306\n",
            "what is the loss v  0.6997788553278003\n",
            "what is the loss test 0.7152492486135756\n",
            "************************************************************** 12\n",
            "what is the tr accuracy:  0.8042666666666667\n",
            "what is the v accuracy:  0.805\n",
            "what is the t accuracy:  0.8091042584434655\n",
            "what is the loss  0.8259420664189956\n",
            "what is the loss v  0.810142707329865\n",
            "what is the loss test 0.8422024806918148\n",
            "************************************************************** 12\n",
            "what is the tr accuracy:  0.8042666666666667\n",
            "what is the v accuracy:  0.805\n",
            "what is the t accuracy:  0.8091042584434655\n",
            "what is the loss  0.8259420664189956\n",
            "what is the loss v  0.810142707329865\n",
            "what is the loss test 0.8422024806918148\n",
            "************************************************************** 13\n",
            "what is the tr accuracy:  0.8282\n",
            "what is the v accuracy:  0.827\n",
            "what is the t accuracy:  0.8296622613803231\n",
            "what is the loss  0.7215386937857968\n",
            "what is the loss v  0.715552708488389\n",
            "what is the loss test 0.7487468725934326\n",
            "************************************************************** 13\n",
            "what is the tr accuracy:  0.8282\n",
            "what is the v accuracy:  0.827\n",
            "what is the t accuracy:  0.8296622613803231\n",
            "what is the loss  0.7215386937857968\n",
            "what is the loss v  0.715552708488389\n",
            "what is the loss test 0.7487468725934326\n",
            "************************************************************** 14\n",
            "what is the tr accuracy:  0.8599333333333333\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8623348017621145\n",
            "what is the loss  0.5882037036004472\n",
            "what is the loss v  0.5671582413241492\n",
            "what is the loss test 0.5959404099486583\n",
            "************************************************************** 14\n",
            "what is the tr accuracy:  0.8599333333333333\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8623348017621145\n",
            "what is the loss  0.5882037036004472\n",
            "what is the loss v  0.5671582413241492\n",
            "what is the loss test 0.5959404099486583\n",
            "************************************************************** 15\n",
            "what is the tr accuracy:  0.8382\n",
            "what is the v accuracy:  0.842\n",
            "what is the t accuracy:  0.8516886930983847\n",
            "what is the loss  0.598837957181885\n",
            "what is the loss v  0.566735476783776\n",
            "what is the loss test 0.573865371730803\n",
            "************************************************************** 15\n",
            "what is the tr accuracy:  0.8382\n",
            "what is the v accuracy:  0.842\n",
            "what is the t accuracy:  0.8516886930983847\n",
            "what is the loss  0.598837957181885\n",
            "what is the loss v  0.566735476783776\n",
            "what is the loss test 0.573865371730803\n",
            "************************************************************** 16\n",
            "what is the tr accuracy:  0.8512666666666666\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8582966226138032\n",
            "what is the loss  0.6434489859477577\n",
            "what is the loss v  0.6094533124832368\n",
            "what is the loss test 0.6024983311283946\n",
            "************************************************************** 16\n",
            "what is the tr accuracy:  0.8512666666666666\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8582966226138032\n",
            "what is the loss  0.6434489859477577\n",
            "what is the loss v  0.6094533124832368\n",
            "what is the loss test 0.6024983311283946\n",
            "************************************************************** 17\n",
            "what is the tr accuracy:  0.8427333333333333\n",
            "what is the v accuracy:  0.855\n",
            "what is the t accuracy:  0.8546255506607929\n",
            "what is the loss  0.676344852394755\n",
            "what is the loss v  0.6455641989196303\n",
            "what is the loss test 0.6343103833012922\n",
            "************************************************************** 17\n",
            "what is the tr accuracy:  0.8427333333333333\n",
            "what is the v accuracy:  0.855\n",
            "what is the t accuracy:  0.8546255506607929\n",
            "what is the loss  0.676344852394755\n",
            "what is the loss v  0.6455641989196303\n",
            "what is the loss test 0.6343103833012922\n",
            "************************************************************** 18\n",
            "what is the tr accuracy:  0.8522\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8634361233480177\n",
            "what is the loss  0.6319038873997631\n",
            "what is the loss v  0.6100336432111295\n",
            "what is the loss test 0.5988311261333502\n",
            "************************************************************** 18\n",
            "what is the tr accuracy:  0.8522\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8634361233480177\n",
            "what is the loss  0.6319038873997631\n",
            "what is the loss v  0.6100336432111295\n",
            "what is the loss test 0.5988311261333502\n",
            "************************************************************** 19\n",
            "what is the tr accuracy:  0.8585333333333334\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8704111600587372\n",
            "what is the loss  0.5593766904959386\n",
            "what is the loss v  0.5493193082066493\n",
            "what is the loss test 0.545557954636746\n",
            "************************************************************** 19\n",
            "what is the tr accuracy:  0.8585333333333334\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8704111600587372\n",
            "what is the loss  0.5593766904959386\n",
            "what is the loss v  0.5493193082066493\n",
            "what is the loss test 0.545557954636746\n",
            "************************************************************** 20\n",
            "what is the tr accuracy:  0.8628\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8678414096916299\n",
            "what is the loss  0.5172392530053245\n",
            "what is the loss v  0.5209702374362003\n",
            "what is the loss test 0.5195564213831496\n",
            "************************************************************** 20\n",
            "what is the tr accuracy:  0.8628\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.8678414096916299\n",
            "what is the loss  0.5172392530053245\n",
            "what is the loss v  0.5209702374362003\n",
            "what is the loss test 0.5195564213831496\n",
            "************************************************************** 21\n",
            "what is the tr accuracy:  0.8746\n",
            "what is the v accuracy:  0.868\n",
            "what is the t accuracy:  0.8751835535976505\n",
            "what is the loss  0.48240145263582956\n",
            "what is the loss v  0.48752474594104933\n",
            "what is the loss test 0.4974931806059321\n",
            "************************************************************** 21\n",
            "what is the tr accuracy:  0.8746\n",
            "what is the v accuracy:  0.868\n",
            "what is the t accuracy:  0.8751835535976505\n",
            "what is the loss  0.48240145263582956\n",
            "what is the loss v  0.48752474594104933\n",
            "what is the loss test 0.4974931806059321\n",
            "************************************************************** 22\n",
            "what is the tr accuracy:  0.8610666666666666\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8608663729809104\n",
            "what is the loss  0.5087728590460969\n",
            "what is the loss v  0.5069561400730905\n",
            "what is the loss test 0.5315829473221688\n",
            "************************************************************** 22\n",
            "what is the tr accuracy:  0.8610666666666666\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8608663729809104\n",
            "what is the loss  0.5087728590460969\n",
            "what is the loss v  0.5069561400730905\n",
            "what is the loss test 0.5315829473221688\n",
            "************************************************************** 23\n",
            "what is the tr accuracy:  0.8496666666666667\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8465491923641704\n",
            "what is the loss  0.5437655700786411\n",
            "what is the loss v  0.5371608534817753\n",
            "what is the loss test 0.5748956705782622\n",
            "************************************************************** 23\n",
            "what is the tr accuracy:  0.8496666666666667\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8465491923641704\n",
            "what is the loss  0.5437655700786411\n",
            "what is the loss v  0.5371608534817753\n",
            "what is the loss test 0.5748956705782622\n",
            "************************************************************** 24\n",
            "what is the tr accuracy:  0.8524666666666667\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.8480176211453745\n",
            "what is the loss  0.548555940408766\n",
            "what is the loss v  0.5392430901922297\n",
            "what is the loss test 0.5858543723671797\n",
            "************************************************************** 24\n",
            "what is the tr accuracy:  0.8524666666666667\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.8480176211453745\n",
            "what is the loss  0.548555940408766\n",
            "what is the loss v  0.5392430901922297\n",
            "what is the loss test 0.5858543723671797\n",
            "************************************************************** 25\n",
            "what is the tr accuracy:  0.8662\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8582966226138032\n",
            "what is the loss  0.504493025716114\n",
            "what is the loss v  0.4892149942781448\n",
            "what is the loss test 0.5414705185779668\n",
            "************************************************************** 25\n",
            "what is the tr accuracy:  0.8662\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8582966226138032\n",
            "what is the loss  0.504493025716114\n",
            "what is the loss v  0.4892149942781448\n",
            "what is the loss test 0.5414705185779668\n",
            "************************************************************** 26\n",
            "what is the tr accuracy:  0.8797333333333334\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8729809104258444\n",
            "what is the loss  0.4582102720138429\n",
            "what is the loss v  0.44081006716263016\n",
            "what is the loss test 0.4883857896922086\n",
            "************************************************************** 26\n",
            "what is the tr accuracy:  0.8797333333333334\n",
            "what is the v accuracy:  0.879\n",
            "what is the t accuracy:  0.8729809104258444\n",
            "what is the loss  0.4582102720138429\n",
            "what is the loss v  0.44081006716263016\n",
            "what is the loss test 0.4883857896922086\n",
            "************************************************************** 27\n",
            "what is the tr accuracy:  0.8836666666666667\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.4381845184327433\n",
            "what is the loss v  0.4242006856390528\n",
            "what is the loss test 0.4628127098326722\n",
            "************************************************************** 27\n",
            "what is the tr accuracy:  0.8836666666666667\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.4381845184327433\n",
            "what is the loss v  0.4242006856390528\n",
            "what is the loss test 0.4628127098326722\n",
            "************************************************************** 28\n",
            "what is the tr accuracy:  0.8824\n",
            "what is the v accuracy:  0.88\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.437104470088214\n",
            "what is the loss v  0.42848739882835274\n",
            "what is the loss test 0.4573005562165642\n",
            "************************************************************** 28\n",
            "what is the tr accuracy:  0.8824\n",
            "what is the v accuracy:  0.88\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.437104470088214\n",
            "what is the loss v  0.42848739882835274\n",
            "what is the loss test 0.4573005562165642\n",
            "************************************************************** 29\n",
            "what is the tr accuracy:  0.8772666666666666\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.4484611347184142\n",
            "what is the loss v  0.44597465697989835\n",
            "what is the loss test 0.4637499196190565\n",
            "************************************************************** 29\n",
            "what is the tr accuracy:  0.8772666666666666\n",
            "what is the v accuracy:  0.876\n",
            "what is the t accuracy:  0.8821585903083701\n",
            "what is the loss  0.4484611347184142\n",
            "what is the loss v  0.44597465697989835\n",
            "what is the loss test 0.4637499196190565\n",
            "************************************************************** 30\n",
            "what is the tr accuracy:  0.8766666666666667\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.4619985803020529\n",
            "what is the loss v  0.46068712478817786\n",
            "what is the loss test 0.4740796455206529\n",
            "************************************************************** 30\n",
            "what is the tr accuracy:  0.8766666666666667\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.4619985803020529\n",
            "what is the loss v  0.46068712478817786\n",
            "what is the loss test 0.4740796455206529\n",
            "************************************************************** 31\n",
            "what is the tr accuracy:  0.8776\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8792217327459618\n",
            "what is the loss  0.45990321280366747\n",
            "what is the loss v  0.4544823507566748\n",
            "what is the loss test 0.4722317584628522\n",
            "************************************************************** 31\n",
            "what is the tr accuracy:  0.8776\n",
            "what is the v accuracy:  0.881\n",
            "what is the t accuracy:  0.8792217327459618\n",
            "what is the loss  0.45990321280366747\n",
            "what is the loss v  0.4544823507566748\n",
            "what is the loss test 0.4722317584628522\n",
            "************************************************************** 32\n",
            "what is the tr accuracy:  0.8814\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.881424375917768\n",
            "what is the loss  0.4415666553339612\n",
            "what is the loss v  0.4307008383237233\n",
            "what is the loss test 0.45740477969965715\n",
            "************************************************************** 32\n",
            "what is the tr accuracy:  0.8814\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.881424375917768\n",
            "what is the loss  0.4415666553339612\n",
            "what is the loss v  0.4307008383237233\n",
            "what is the loss test 0.45740477969965715\n",
            "************************************************************** 33\n",
            "what is the tr accuracy:  0.8878666666666667\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.42685501709200296\n",
            "what is the loss v  0.4094008462212886\n",
            "what is the loss test 0.44765716183166204\n",
            "************************************************************** 33\n",
            "what is the tr accuracy:  0.8878666666666667\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.42685501709200296\n",
            "what is the loss v  0.4094008462212886\n",
            "what is the loss test 0.44765716183166204\n",
            "************************************************************** 34\n",
            "what is the tr accuracy:  0.8928666666666667\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.4084617661639221\n",
            "what is the loss v  0.3881812686771689\n",
            "what is the loss test 0.43594047443755707\n",
            "************************************************************** 34\n",
            "what is the tr accuracy:  0.8928666666666667\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8876651982378855\n",
            "what is the loss  0.4084617661639221\n",
            "what is the loss v  0.3881812686771689\n",
            "what is the loss test 0.43594047443755707\n",
            "************************************************************** 35\n",
            "what is the tr accuracy:  0.8960666666666667\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.39195389181706447\n",
            "what is the loss v  0.37782136078693396\n",
            "what is the loss test 0.42773582010095174\n",
            "************************************************************** 35\n",
            "what is the tr accuracy:  0.8960666666666667\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.39195389181706447\n",
            "what is the loss v  0.37782136078693396\n",
            "what is the loss test 0.42773582010095174\n",
            "************************************************************** 36\n",
            "what is the tr accuracy:  0.8963333333333333\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.38731356393487026\n",
            "what is the loss v  0.3868223208495426\n",
            "what is the loss test 0.43242656703553983\n",
            "************************************************************** 36\n",
            "what is the tr accuracy:  0.8963333333333333\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8891336270190896\n",
            "what is the loss  0.38731356393487026\n",
            "what is the loss v  0.3868223208495426\n",
            "what is the loss test 0.43242656703553983\n",
            "************************************************************** 37\n",
            "what is the tr accuracy:  0.8977333333333334\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8854625550660793\n",
            "what is the loss  0.39053618621950204\n",
            "what is the loss v  0.40672835559304554\n",
            "what is the loss test 0.44470953445110817\n",
            "************************************************************** 37\n",
            "what is the tr accuracy:  0.8977333333333334\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8854625550660793\n",
            "what is the loss  0.39053618621950204\n",
            "what is the loss v  0.40672835559304554\n",
            "what is the loss test 0.44470953445110817\n",
            "************************************************************** 38\n",
            "what is the tr accuracy:  0.8954\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8828928046989721\n",
            "what is the loss  0.3974640028374056\n",
            "what is the loss v  0.4282767473124656\n",
            "what is the loss test 0.45911775162381924\n",
            "************************************************************** 38\n",
            "what is the tr accuracy:  0.8954\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8828928046989721\n",
            "what is the loss  0.3974640028374056\n",
            "what is the loss v  0.4282767473124656\n",
            "what is the loss test 0.45911775162381924\n",
            "************************************************************** 39\n",
            "what is the tr accuracy:  0.8930666666666667\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.4030107335371987\n",
            "what is the loss v  0.4430364446984864\n",
            "what is the loss test 0.47030236294997924\n",
            "************************************************************** 39\n",
            "what is the tr accuracy:  0.8930666666666667\n",
            "what is the v accuracy:  0.889\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.4030107335371987\n",
            "what is the loss v  0.4430364446984864\n",
            "what is the loss test 0.47030236294997924\n",
            "************************************************************** 40\n",
            "what is the tr accuracy:  0.8958666666666667\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.39117415478918777\n",
            "what is the loss v  0.4312682373165279\n",
            "what is the loss test 0.4604096519316638\n",
            "************************************************************** 40\n",
            "what is the tr accuracy:  0.8958666666666667\n",
            "what is the v accuracy:  0.893\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.39117415478918777\n",
            "what is the loss v  0.4312682373165279\n",
            "what is the loss test 0.4604096519316638\n",
            "************************************************************** 41\n",
            "what is the tr accuracy:  0.8990666666666667\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.377118774793418\n",
            "what is the loss v  0.4119884302401994\n",
            "what is the loss test 0.4438530390788576\n",
            "************************************************************** 41\n",
            "what is the tr accuracy:  0.8990666666666667\n",
            "what is the v accuracy:  0.894\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.377118774793418\n",
            "what is the loss v  0.4119884302401994\n",
            "what is the loss test 0.4438530390788576\n",
            "************************************************************** 42\n",
            "what is the tr accuracy:  0.9008\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8961086637298091\n",
            "what is the loss  0.3757104168351487\n",
            "what is the loss v  0.4086736028480867\n",
            "what is the loss test 0.43836121188568494\n",
            "************************************************************** 42\n",
            "what is the tr accuracy:  0.9008\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8961086637298091\n",
            "what is the loss  0.3757104168351487\n",
            "what is the loss v  0.4086736028480867\n",
            "what is the loss test 0.43836121188568494\n",
            "************************************************************** 43\n",
            "what is the tr accuracy:  0.9016\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.372793786041442\n",
            "what is the loss v  0.4117729992549826\n",
            "what is the loss test 0.43441656307434806\n",
            "************************************************************** 43\n",
            "what is the tr accuracy:  0.9016\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.372793786041442\n",
            "what is the loss v  0.4117729992549826\n",
            "what is the loss test 0.43441656307434806\n",
            "************************************************************** 44\n",
            "what is the tr accuracy:  0.9032666666666667\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8961086637298091\n",
            "what is the loss  0.3602042221918167\n",
            "what is the loss v  0.407441533534806\n",
            "what is the loss test 0.42489359409636424\n",
            "************************************************************** 44\n",
            "what is the tr accuracy:  0.9032666666666667\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8961086637298091\n",
            "what is the loss  0.3602042221918167\n",
            "what is the loss v  0.407441533534806\n",
            "what is the loss test 0.42489359409636424\n",
            "************************************************************** 45\n",
            "what is the tr accuracy:  0.9055333333333333\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8979441997063142\n",
            "what is the loss  0.34751247117355294\n",
            "what is the loss v  0.40397834482542944\n",
            "what is the loss test 0.41690191150982503\n",
            "************************************************************** 45\n",
            "what is the tr accuracy:  0.9055333333333333\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8979441997063142\n",
            "what is the loss  0.34751247117355294\n",
            "what is the loss v  0.40397834482542944\n",
            "what is the loss test 0.41690191150982503\n",
            "************************************************************** 46\n",
            "what is the tr accuracy:  0.9068\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.34460022041353694\n",
            "what is the loss v  0.40804840922449115\n",
            "what is the loss test 0.41736552480404276\n",
            "************************************************************** 46\n",
            "what is the tr accuracy:  0.9068\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.34460022041353694\n",
            "what is the loss v  0.40804840922449115\n",
            "what is the loss test 0.41736552480404276\n",
            "************************************************************** 47\n",
            "what is the tr accuracy:  0.9048666666666667\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.34827063059271657\n",
            "what is the loss v  0.4167238483180098\n",
            "what is the loss test 0.42362496872993016\n",
            "************************************************************** 47\n",
            "what is the tr accuracy:  0.9048666666666667\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.34827063059271657\n",
            "what is the loss v  0.4167238483180098\n",
            "what is the loss test 0.42362496872993016\n",
            "************************************************************** 48\n",
            "what is the tr accuracy:  0.9036666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.3500447089950573\n",
            "what is the loss v  0.4199056571678535\n",
            "what is the loss test 0.428193806875277\n",
            "************************************************************** 48\n",
            "what is the tr accuracy:  0.9036666666666666\n",
            "what is the v accuracy:  0.895\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.3500447089950573\n",
            "what is the loss v  0.4199056571678535\n",
            "what is the loss test 0.428193806875277\n",
            "************************************************************** 49\n",
            "what is the tr accuracy:  0.9031333333333333\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.34910185242466846\n",
            "what is the loss v  0.4156504838280863\n",
            "what is the loss test 0.43054554963861985\n",
            "************************************************************** 49\n",
            "what is the tr accuracy:  0.9031333333333333\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.8895007342143906\n",
            "what is the loss  0.34910185242466846\n",
            "what is the loss v  0.4156504838280863\n",
            "what is the loss test 0.43054554963861985\n",
            "************************************************************** 50\n",
            "what is the tr accuracy:  0.9052\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.3406313624536087\n",
            "what is the loss v  0.39861656924981387\n",
            "what is the loss test 0.4260277089386144\n",
            "************************************************************** 50\n",
            "what is the tr accuracy:  0.9052\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.3406313624536087\n",
            "what is the loss v  0.39861656924981387\n",
            "what is the loss test 0.4260277089386144\n",
            "************************************************************** 51\n",
            "what is the tr accuracy:  0.9086\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.32987305614906215\n",
            "what is the loss v  0.37980873400607634\n",
            "what is the loss test 0.4193286519820271\n",
            "************************************************************** 51\n",
            "what is the tr accuracy:  0.9086\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.32987305614906215\n",
            "what is the loss v  0.37980873400607634\n",
            "what is the loss test 0.4193286519820271\n",
            "************************************************************** 52\n",
            "what is the tr accuracy:  0.91\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.32778263922576334\n",
            "what is the loss v  0.37671226799481383\n",
            "what is the loss test 0.42162196889211656\n",
            "************************************************************** 52\n",
            "what is the tr accuracy:  0.91\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.32778263922576334\n",
            "what is the loss v  0.37671226799481383\n",
            "what is the loss test 0.42162196889211656\n",
            "************************************************************** 53\n",
            "what is the tr accuracy:  0.9114\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.3230071765950443\n",
            "what is the loss v  0.374469014387028\n",
            "what is the loss test 0.422023599970291\n",
            "************************************************************** 53\n",
            "what is the tr accuracy:  0.9114\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8928046989720999\n",
            "what is the loss  0.3230071765950443\n",
            "what is the loss v  0.374469014387028\n",
            "what is the loss test 0.422023599970291\n",
            "************************************************************** 54\n",
            "what is the tr accuracy:  0.9116666666666666\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.3131811472347489\n",
            "what is the loss v  0.3683339763215783\n",
            "what is the loss test 0.417144139830406\n",
            "************************************************************** 54\n",
            "what is the tr accuracy:  0.9116666666666666\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8913362701908958\n",
            "what is the loss  0.3131811472347489\n",
            "what is the loss v  0.3683339763215783\n",
            "what is the loss test 0.417144139830406\n",
            "************************************************************** 55\n",
            "what is the tr accuracy:  0.9125333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.3086405482092009\n",
            "what is the loss v  0.37024857383662024\n",
            "what is the loss test 0.4155582472164802\n",
            "************************************************************** 55\n",
            "what is the tr accuracy:  0.9125333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.3086405482092009\n",
            "what is the loss v  0.37024857383662024\n",
            "what is the loss test 0.4155582472164802\n",
            "************************************************************** 56\n",
            "what is the tr accuracy:  0.9117333333333333\n",
            "what is the v accuracy:  0.905\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.3080804700696066\n",
            "what is the loss v  0.37857232692386833\n",
            "what is the loss test 0.4165608584472743\n",
            "************************************************************** 56\n",
            "what is the tr accuracy:  0.9117333333333333\n",
            "what is the v accuracy:  0.905\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.3080804700696066\n",
            "what is the loss v  0.37857232692386833\n",
            "what is the loss test 0.4165608584472743\n",
            "************************************************************** 57\n",
            "what is the tr accuracy:  0.9116666666666666\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.3085260971120626\n",
            "what is the loss v  0.38773017440707475\n",
            "what is the loss test 0.41948775250944015\n",
            "************************************************************** 57\n",
            "what is the tr accuracy:  0.9116666666666666\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.3085260971120626\n",
            "what is the loss v  0.38773017440707475\n",
            "what is the loss test 0.41948775250944015\n",
            "************************************************************** 58\n",
            "what is the tr accuracy:  0.9129333333333334\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.3087419395043018\n",
            "what is the loss v  0.3932451884227404\n",
            "what is the loss test 0.4234924748191646\n",
            "************************************************************** 58\n",
            "what is the tr accuracy:  0.9129333333333334\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8920704845814978\n",
            "what is the loss  0.3087419395043018\n",
            "what is the loss v  0.3932451884227404\n",
            "what is the loss test 0.4234924748191646\n",
            "************************************************************** 59\n",
            "what is the tr accuracy:  0.9148\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.30683281046747213\n",
            "what is the loss v  0.39287615032925827\n",
            "what is the loss test 0.42633482802087486\n",
            "************************************************************** 59\n",
            "what is the tr accuracy:  0.9148\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.30683281046747213\n",
            "what is the loss v  0.39287615032925827\n",
            "what is the loss test 0.42633482802087486\n",
            "************************************************************** 60\n",
            "what is the tr accuracy:  0.9159333333333334\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.3019720421055266\n",
            "what is the loss v  0.3869324837136097\n",
            "what is the loss test 0.4262590221645331\n",
            "************************************************************** 60\n",
            "what is the tr accuracy:  0.9159333333333334\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.894640234948605\n",
            "what is the loss  0.3019720421055266\n",
            "what is the loss v  0.3869324837136097\n",
            "what is the loss test 0.4262590221645331\n",
            "************************************************************** 61\n",
            "what is the tr accuracy:  0.9181333333333334\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.29685972134543687\n",
            "what is the loss v  0.3805903114206449\n",
            "what is the loss test 0.42582261102224844\n",
            "************************************************************** 61\n",
            "what is the tr accuracy:  0.9181333333333334\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.29685972134543687\n",
            "what is the loss v  0.3805903114206449\n",
            "what is the loss test 0.42582261102224844\n",
            "************************************************************** 62\n",
            "what is the tr accuracy:  0.9189333333333334\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.2933908725499008\n",
            "what is the loss v  0.37762747528432233\n",
            "what is the loss test 0.4268321082281874\n",
            "************************************************************** 62\n",
            "what is the tr accuracy:  0.9189333333333334\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.2933908725499008\n",
            "what is the loss v  0.37762747528432233\n",
            "what is the loss test 0.4268321082281874\n",
            "************************************************************** 63\n",
            "what is the tr accuracy:  0.9190666666666667\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.29032672799775366\n",
            "what is the loss v  0.3773682787611906\n",
            "what is the loss test 0.42639930458684155\n",
            "************************************************************** 63\n",
            "what is the tr accuracy:  0.9190666666666667\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.29032672799775366\n",
            "what is the loss v  0.3773682787611906\n",
            "what is the loss test 0.42639930458684155\n",
            "************************************************************** 64\n",
            "what is the tr accuracy:  0.9198666666666667\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.2861193665737685\n",
            "what is the loss v  0.37713555482548355\n",
            "what is the loss test 0.42293010292936595\n",
            "************************************************************** 64\n",
            "what is the tr accuracy:  0.9198666666666667\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8964757709251101\n",
            "what is the loss  0.2861193665737685\n",
            "what is the loss v  0.37713555482548355\n",
            "what is the loss test 0.42293010292936595\n",
            "************************************************************** 65\n",
            "what is the tr accuracy:  0.9217333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.28114124864593415\n",
            "what is the loss v  0.3766036266250362\n",
            "what is the loss test 0.41827688793144524\n",
            "************************************************************** 65\n",
            "what is the tr accuracy:  0.9217333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8968428781204112\n",
            "what is the loss  0.28114124864593415\n",
            "what is the loss v  0.3766036266250362\n",
            "what is the loss test 0.41827688793144524\n",
            "************************************************************** 66\n",
            "what is the tr accuracy:  0.9217333333333333\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.2770396332761404\n",
            "what is the loss v  0.37677808148043324\n",
            "what is the loss test 0.41544664564694495\n",
            "************************************************************** 66\n",
            "what is the tr accuracy:  0.9217333333333333\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8972099853157122\n",
            "what is the loss  0.2770396332761404\n",
            "what is the loss v  0.37677808148043324\n",
            "what is the loss test 0.41544664564694495\n",
            "************************************************************** 67\n",
            "what is the tr accuracy:  0.9231333333333334\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.2729895710700241\n",
            "what is the loss v  0.3748956046502867\n",
            "what is the loss test 0.4147244710028816\n",
            "************************************************************** 67\n",
            "what is the tr accuracy:  0.9231333333333334\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8953744493392071\n",
            "what is the loss  0.2729895710700241\n",
            "what is the loss v  0.3748956046502867\n",
            "what is the loss test 0.4147244710028816\n",
            "************************************************************** 68\n",
            "what is the tr accuracy:  0.9238666666666666\n",
            "what is the v accuracy:  0.907\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.2689617871018732\n",
            "what is the loss v  0.3713114946894492\n",
            "what is the loss test 0.4158748989944257\n",
            "************************************************************** 68\n",
            "what is the tr accuracy:  0.9238666666666666\n",
            "what is the v accuracy:  0.907\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.2689617871018732\n",
            "what is the loss v  0.3713114946894492\n",
            "what is the loss test 0.4158748989944257\n",
            "************************************************************** 69\n",
            "what is the tr accuracy:  0.9232666666666667\n",
            "what is the v accuracy:  0.908\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.26562832601776415\n",
            "what is the loss v  0.36771531953992326\n",
            "what is the loss test 0.41843057192404964\n",
            "************************************************************** 69\n",
            "what is the tr accuracy:  0.9232666666666667\n",
            "what is the v accuracy:  0.908\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.26562832601776415\n",
            "what is the loss v  0.36771531953992326\n",
            "what is the loss test 0.41843057192404964\n",
            "************************************************************** 70\n",
            "what is the tr accuracy:  0.9239333333333334\n",
            "what is the v accuracy:  0.91\n",
            "what is the t accuracy:  0.9005139500734214\n",
            "what is the loss  0.26291994472462327\n",
            "what is the loss v  0.36469846183711374\n",
            "what is the loss test 0.4210910733090419\n",
            "************************************************************** 70\n",
            "what is the tr accuracy:  0.9239333333333334\n",
            "what is the v accuracy:  0.91\n",
            "what is the t accuracy:  0.9005139500734214\n",
            "what is the loss  0.26291994472462327\n",
            "what is the loss v  0.36469846183711374\n",
            "what is the loss test 0.4210910733090419\n",
            "************************************************************** 71\n",
            "what is the tr accuracy:  0.9239333333333334\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.26010464233364616\n",
            "what is the loss v  0.36255526657219955\n",
            "what is the loss test 0.42284415699838784\n",
            "************************************************************** 71\n",
            "what is the tr accuracy:  0.9239333333333334\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.26010464233364616\n",
            "what is the loss v  0.36255526657219955\n",
            "what is the loss test 0.42284415699838784\n",
            "************************************************************** 72\n",
            "what is the tr accuracy:  0.9244666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9016152716593245\n",
            "what is the loss  0.2565871655729418\n",
            "what is the loss v  0.3605182973610772\n",
            "what is the loss test 0.42357848872649323\n",
            "************************************************************** 72\n",
            "what is the tr accuracy:  0.9244666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9016152716593245\n",
            "what is the loss  0.2565871655729418\n",
            "what is the loss v  0.3605182973610772\n",
            "what is the loss test 0.42357848872649323\n",
            "************************************************************** 73\n",
            "what is the tr accuracy:  0.9249333333333334\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.2530486908735319\n",
            "what is the loss v  0.3598747457422458\n",
            "what is the loss test 0.42337744657864645\n",
            "************************************************************** 73\n",
            "what is the tr accuracy:  0.9249333333333334\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.2530486908735319\n",
            "what is the loss v  0.3598747457422458\n",
            "what is the loss test 0.42337744657864645\n",
            "************************************************************** 74\n",
            "what is the tr accuracy:  0.9262666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.25036956642840347\n",
            "what is the loss v  0.36080649316166136\n",
            "what is the loss test 0.42313469648931407\n",
            "************************************************************** 74\n",
            "what is the tr accuracy:  0.9262666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.25036956642840347\n",
            "what is the loss v  0.36080649316166136\n",
            "what is the loss test 0.42313469648931407\n",
            "************************************************************** 75\n",
            "what is the tr accuracy:  0.9278666666666666\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.24824615380820986\n",
            "what is the loss v  0.36249484141107424\n",
            "what is the loss test 0.42392794629572883\n",
            "************************************************************** 75\n",
            "what is the tr accuracy:  0.9278666666666666\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.24824615380820986\n",
            "what is the loss v  0.36249484141107424\n",
            "what is the loss test 0.42392794629572883\n",
            "************************************************************** 76\n",
            "what is the tr accuracy:  0.928\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.24572888667941267\n",
            "what is the loss v  0.3636719118680827\n",
            "what is the loss test 0.42548905183948144\n",
            "************************************************************** 76\n",
            "what is the tr accuracy:  0.928\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.8994126284875184\n",
            "what is the loss  0.24572888667941267\n",
            "what is the loss v  0.3636719118680827\n",
            "what is the loss test 0.42548905183948144\n",
            "************************************************************** 77\n",
            "what is the tr accuracy:  0.9284\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.8975770925110133\n",
            "what is the loss  0.24194806806445776\n",
            "what is the loss v  0.3634595495974219\n",
            "what is the loss test 0.4271761375671665\n",
            "************************************************************** 77\n",
            "what is the tr accuracy:  0.9284\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.8975770925110133\n",
            "what is the loss  0.24194806806445776\n",
            "what is the loss v  0.3634595495974219\n",
            "what is the loss test 0.4271761375671665\n",
            "************************************************************** 78\n",
            "what is the tr accuracy:  0.9292\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.23755080360139635\n",
            "what is the loss v  0.3631309072667095\n",
            "what is the loss test 0.4300375052535552\n",
            "************************************************************** 78\n",
            "what is the tr accuracy:  0.9292\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.23755080360139635\n",
            "what is the loss v  0.3631309072667095\n",
            "what is the loss test 0.4300375052535552\n",
            "************************************************************** 79\n",
            "what is the tr accuracy:  0.93\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9005139500734214\n",
            "what is the loss  0.23333868820310388\n",
            "what is the loss v  0.36306397638124177\n",
            "what is the loss test 0.43369642640970624\n",
            "************************************************************** 79\n",
            "what is the tr accuracy:  0.93\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9005139500734214\n",
            "what is the loss  0.23333868820310388\n",
            "what is the loss v  0.36306397638124177\n",
            "what is the loss test 0.43369642640970624\n",
            "************************************************************** 80\n",
            "what is the tr accuracy:  0.9307333333333333\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.22973887078522087\n",
            "what is the loss v  0.3638305049986815\n",
            "what is the loss test 0.437457163853749\n",
            "************************************************************** 80\n",
            "what is the tr accuracy:  0.9307333333333333\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.22973887078522087\n",
            "what is the loss v  0.3638305049986815\n",
            "what is the loss test 0.437457163853749\n",
            "************************************************************** 81\n",
            "what is the tr accuracy:  0.9315333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.22629425272176096\n",
            "what is the loss v  0.36595205969696853\n",
            "what is the loss test 0.4400504891805715\n",
            "************************************************************** 81\n",
            "what is the tr accuracy:  0.9315333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.22629425272176096\n",
            "what is the loss v  0.36595205969696853\n",
            "what is the loss test 0.4400504891805715\n",
            "************************************************************** 82\n",
            "what is the tr accuracy:  0.9322666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9023494860499266\n",
            "what is the loss  0.22297692807198757\n",
            "what is the loss v  0.36965358641756224\n",
            "what is the loss test 0.4411640924509713\n",
            "************************************************************** 82\n",
            "what is the tr accuracy:  0.9322666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9023494860499266\n",
            "what is the loss  0.22297692807198757\n",
            "what is the loss v  0.36965358641756224\n",
            "what is the loss test 0.4411640924509713\n",
            "************************************************************** 83\n",
            "what is the tr accuracy:  0.9334666666666667\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.21955853329886757\n",
            "what is the loss v  0.3734942731748754\n",
            "what is the loss test 0.44058384892241853\n",
            "************************************************************** 83\n",
            "what is the tr accuracy:  0.9334666666666667\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.21955853329886757\n",
            "what is the loss v  0.3734942731748754\n",
            "what is the loss test 0.44058384892241853\n",
            "************************************************************** 84\n",
            "what is the tr accuracy:  0.934\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9045521292217328\n",
            "what is the loss  0.2156003679277497\n",
            "what is the loss v  0.37619455163485316\n",
            "what is the loss test 0.4380877955870702\n",
            "************************************************************** 84\n",
            "what is the tr accuracy:  0.934\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9045521292217328\n",
            "what is the loss  0.2156003679277497\n",
            "what is the loss v  0.37619455163485316\n",
            "what is the loss test 0.4380877955870702\n",
            "************************************************************** 85\n",
            "what is the tr accuracy:  0.9349333333333333\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.21110403235007527\n",
            "what is the loss v  0.37734440118258417\n",
            "what is the loss test 0.43458502078515376\n",
            "************************************************************** 85\n",
            "what is the tr accuracy:  0.9349333333333333\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.21110403235007527\n",
            "what is the loss v  0.37734440118258417\n",
            "what is the loss test 0.43458502078515376\n",
            "************************************************************** 86\n",
            "what is the tr accuracy:  0.9354666666666667\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9016152716593245\n",
            "what is the loss  0.20645943812318177\n",
            "what is the loss v  0.37610139311212076\n",
            "what is the loss test 0.4316205512501256\n",
            "************************************************************** 86\n",
            "what is the tr accuracy:  0.9354666666666667\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9016152716593245\n",
            "what is the loss  0.20645943812318177\n",
            "what is the loss v  0.37610139311212076\n",
            "what is the loss test 0.4316205512501256\n",
            "************************************************************** 87\n",
            "what is the tr accuracy:  0.9367333333333333\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.20231844531166882\n",
            "what is the loss v  0.3739274386548639\n",
            "what is the loss test 0.4301340017771145\n",
            "************************************************************** 87\n",
            "what is the tr accuracy:  0.9367333333333333\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9008810572687225\n",
            "what is the loss  0.20231844531166882\n",
            "what is the loss v  0.3739274386548639\n",
            "what is the loss test 0.4301340017771145\n",
            "************************************************************** 88\n",
            "what is the tr accuracy:  0.9384\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9001468428781204\n",
            "what is the loss  0.19924824578037267\n",
            "what is the loss v  0.3724679884617606\n",
            "what is the loss test 0.4306257348255112\n",
            "************************************************************** 88\n",
            "what is the tr accuracy:  0.9384\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9001468428781204\n",
            "what is the loss  0.19924824578037267\n",
            "what is the loss v  0.3724679884617606\n",
            "what is the loss test 0.4306257348255112\n",
            "************************************************************** 89\n",
            "what is the tr accuracy:  0.9393333333333334\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9023494860499266\n",
            "what is the loss  0.19726724549872537\n",
            "what is the loss v  0.37311332423887517\n",
            "what is the loss test 0.4325316403012834\n",
            "************************************************************** 89\n",
            "what is the tr accuracy:  0.9393333333333334\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9023494860499266\n",
            "what is the loss  0.19726724549872537\n",
            "what is the loss v  0.37311332423887517\n",
            "what is the loss test 0.4325316403012834\n",
            "************************************************************** 90\n",
            "what is the tr accuracy:  0.9404\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.19567168454246794\n",
            "what is the loss v  0.375399663124287\n",
            "what is the loss test 0.4341586373506086\n",
            "************************************************************** 90\n",
            "what is the tr accuracy:  0.9404\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9019823788546255\n",
            "what is the loss  0.19567168454246794\n",
            "what is the loss v  0.375399663124287\n",
            "what is the loss test 0.4341586373506086\n",
            "************************************************************** 91\n",
            "what is the tr accuracy:  0.9409333333333333\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9027165932452276\n",
            "what is the loss  0.1930667408064388\n",
            "what is the loss v  0.3779894950876189\n",
            "what is the loss test 0.43410937079435463\n",
            "************************************************************** 91\n",
            "what is the tr accuracy:  0.9409333333333333\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9027165932452276\n",
            "what is the loss  0.1930667408064388\n",
            "what is the loss v  0.3779894950876189\n",
            "what is the loss test 0.43410937079435463\n",
            "************************************************************** 92\n",
            "what is the tr accuracy:  0.9420666666666667\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9027165932452276\n",
            "what is the loss  0.18911942508203175\n",
            "what is the loss v  0.3804418460580343\n",
            "what is the loss test 0.4322249838999829\n",
            "************************************************************** 92\n",
            "what is the tr accuracy:  0.9420666666666667\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9027165932452276\n",
            "what is the loss  0.18911942508203175\n",
            "what is the loss v  0.3804418460580343\n",
            "what is the loss test 0.4322249838999829\n",
            "************************************************************** 93\n",
            "what is the tr accuracy:  0.9437333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.18529601587379005\n",
            "what is the loss v  0.3835847240069954\n",
            "what is the loss test 0.430262292233628\n",
            "************************************************************** 93\n",
            "what is the tr accuracy:  0.9437333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.18529601587379005\n",
            "what is the loss v  0.3835847240069954\n",
            "what is the loss test 0.430262292233628\n",
            "************************************************************** 94\n",
            "what is the tr accuracy:  0.9446666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9045521292217328\n",
            "what is the loss  0.18287326122156325\n",
            "what is the loss v  0.38778073787259404\n",
            "what is the loss test 0.4294771641589741\n",
            "************************************************************** 94\n",
            "what is the tr accuracy:  0.9446666666666667\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9045521292217328\n",
            "what is the loss  0.18287326122156325\n",
            "what is the loss v  0.38778073787259404\n",
            "what is the loss test 0.4294771641589741\n",
            "************************************************************** 95\n",
            "what is the tr accuracy:  0.9452\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.9052863436123348\n",
            "what is the loss  0.18107965520503227\n",
            "what is the loss v  0.3909566953199374\n",
            "what is the loss test 0.42971427814723423\n",
            "************************************************************** 95\n",
            "what is the tr accuracy:  0.9452\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.9052863436123348\n",
            "what is the loss  0.18107965520503227\n",
            "what is the loss v  0.3909566953199374\n",
            "what is the loss test 0.42971427814723423\n",
            "************************************************************** 96\n",
            "what is the tr accuracy:  0.9452666666666667\n",
            "what is the v accuracy:  0.911\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.17818382361514687\n",
            "what is the loss v  0.3904615974452445\n",
            "what is the loss test 0.4300070302639908\n",
            "************************************************************** 96\n",
            "what is the tr accuracy:  0.9452666666666667\n",
            "what is the v accuracy:  0.911\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.17818382361514687\n",
            "what is the loss v  0.3904615974452445\n",
            "what is the loss test 0.4300070302639908\n",
            "************************************************************** 97\n",
            "what is the tr accuracy:  0.9466666666666667\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.17418419266877858\n",
            "what is the loss v  0.38617455974075177\n",
            "what is the loss test 0.4301278340123405\n",
            "************************************************************** 97\n",
            "what is the tr accuracy:  0.9466666666666667\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.17418419266877858\n",
            "what is the loss v  0.38617455974075177\n",
            "what is the loss test 0.4301278340123405\n",
            "************************************************************** 98\n",
            "what is the tr accuracy:  0.9480666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.1705170329465834\n",
            "what is the loss v  0.3812858169276164\n",
            "what is the loss test 0.4311773845338682\n",
            "************************************************************** 98\n",
            "what is the tr accuracy:  0.9480666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.1705170329465834\n",
            "what is the loss v  0.3812858169276164\n",
            "what is the loss test 0.4311773845338682\n",
            "************************************************************** 99\n",
            "what is the tr accuracy:  0.9488666666666666\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9063876651982379\n",
            "what is the loss  0.16813529975055935\n",
            "what is the loss v  0.3783101395900182\n",
            "what is the loss test 0.4333174285662181\n",
            "************************************************************** 99\n",
            "what is the tr accuracy:  0.9488666666666666\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9063876651982379\n",
            "what is the loss  0.16813529975055935\n",
            "what is the loss v  0.3783101395900182\n",
            "what is the loss test 0.4333174285662181\n",
            "************************************************************** 100\n",
            "what is the tr accuracy:  0.9492666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9074889867841409\n",
            "what is the loss  0.16613818979970268\n",
            "what is the loss v  0.3776003060578717\n",
            "what is the loss test 0.4356877618582128\n",
            "************************************************************** 100\n",
            "what is the tr accuracy:  0.9492666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9074889867841409\n",
            "what is the loss  0.16613818979970268\n",
            "what is the loss v  0.3776003060578717\n",
            "what is the loss test 0.4356877618582128\n",
            "************************************************************** 101\n",
            "what is the tr accuracy:  0.9492666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.16343739031660492\n",
            "what is the loss v  0.3777296305569862\n",
            "what is the loss test 0.4370483942882083\n",
            "************************************************************** 101\n",
            "what is the tr accuracy:  0.9492666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.16343739031660492\n",
            "what is the loss v  0.3777296305569862\n",
            "what is the loss test 0.4370483942882083\n",
            "************************************************************** 102\n",
            "what is the tr accuracy:  0.95\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.16007854179754274\n",
            "what is the loss v  0.3783574582355182\n",
            "what is the loss test 0.4372752431206062\n",
            "************************************************************** 102\n",
            "what is the tr accuracy:  0.95\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.16007854179754274\n",
            "what is the loss v  0.3783574582355182\n",
            "what is the loss test 0.4372752431206062\n",
            "************************************************************** 103\n",
            "what is the tr accuracy:  0.951\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.15669255758627265\n",
            "what is the loss v  0.379854610205763\n",
            "what is the loss test 0.4371290534027295\n",
            "************************************************************** 103\n",
            "what is the tr accuracy:  0.951\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.15669255758627265\n",
            "what is the loss v  0.379854610205763\n",
            "what is the loss test 0.4371290534027295\n",
            "************************************************************** 104\n",
            "what is the tr accuracy:  0.9522\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.15344315373409737\n",
            "what is the loss v  0.38186126907657253\n",
            "what is the loss test 0.4377157897255186\n",
            "************************************************************** 104\n",
            "what is the tr accuracy:  0.9522\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.15344315373409737\n",
            "what is the loss v  0.38186126907657253\n",
            "what is the loss test 0.4377157897255186\n",
            "************************************************************** 105\n",
            "what is the tr accuracy:  0.9531333333333334\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.15018931222035622\n",
            "what is the loss v  0.38367254716116317\n",
            "what is the loss test 0.43967794761733064\n",
            "************************************************************** 105\n",
            "what is the tr accuracy:  0.9531333333333334\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.15018931222035622\n",
            "what is the loss v  0.38367254716116317\n",
            "what is the loss test 0.43967794761733064\n",
            "************************************************************** 106\n",
            "what is the tr accuracy:  0.9549333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.14687330739648\n",
            "what is the loss v  0.38488447688488836\n",
            "what is the loss test 0.4430843797952063\n",
            "************************************************************** 106\n",
            "what is the tr accuracy:  0.9549333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.14687330739648\n",
            "what is the loss v  0.38488447688488836\n",
            "what is the loss test 0.4430843797952063\n",
            "************************************************************** 107\n",
            "what is the tr accuracy:  0.9548666666666666\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9060205580029369\n",
            "what is the loss  0.14385826476347494\n",
            "what is the loss v  0.3860733599970031\n",
            "what is the loss test 0.4477454639081553\n",
            "************************************************************** 107\n",
            "what is the tr accuracy:  0.9548666666666666\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9060205580029369\n",
            "what is the loss  0.14385826476347494\n",
            "what is the loss v  0.3860733599970031\n",
            "what is the loss test 0.4477454639081553\n",
            "************************************************************** 108\n",
            "what is the tr accuracy:  0.9546666666666667\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.14151900956726313\n",
            "what is the loss v  0.3883440744096296\n",
            "what is the loss test 0.4533367861461448\n",
            "************************************************************** 108\n",
            "what is the tr accuracy:  0.9546666666666667\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.14151900956726313\n",
            "what is the loss v  0.3883440744096296\n",
            "what is the loss test 0.4533367861461448\n",
            "************************************************************** 109\n",
            "what is the tr accuracy:  0.9552\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.13971556286879733\n",
            "what is the loss v  0.39212458517122667\n",
            "what is the loss test 0.45878874350022375\n",
            "************************************************************** 109\n",
            "what is the tr accuracy:  0.9552\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.13971556286879733\n",
            "what is the loss v  0.39212458517122667\n",
            "what is the loss test 0.45878874350022375\n",
            "************************************************************** 110\n",
            "what is the tr accuracy:  0.9559333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.13784860386973768\n",
            "what is the loss v  0.3965949531303213\n",
            "what is the loss test 0.4629811037474827\n",
            "************************************************************** 110\n",
            "what is the tr accuracy:  0.9559333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.13784860386973768\n",
            "what is the loss v  0.3965949531303213\n",
            "what is the loss test 0.4629811037474827\n",
            "************************************************************** 111\n",
            "what is the tr accuracy:  0.9571333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.13555551535609295\n",
            "what is the loss v  0.4009012527902479\n",
            "what is the loss test 0.4651539679769661\n",
            "************************************************************** 111\n",
            "what is the tr accuracy:  0.9571333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.13555551535609295\n",
            "what is the loss v  0.4009012527902479\n",
            "what is the loss test 0.4651539679769661\n",
            "************************************************************** 112\n",
            "what is the tr accuracy:  0.9580666666666666\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.13326617545500155\n",
            "what is the loss v  0.4050804517735021\n",
            "what is the loss test 0.46625138270093713\n",
            "************************************************************** 112\n",
            "what is the tr accuracy:  0.9580666666666666\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.13326617545500155\n",
            "what is the loss v  0.4050804517735021\n",
            "what is the loss test 0.46625138270093713\n",
            "************************************************************** 113\n",
            "what is the tr accuracy:  0.9584666666666667\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.13175263371234494\n",
            "what is the loss v  0.4093260639472208\n",
            "what is the loss test 0.4674460045998276\n",
            "************************************************************** 113\n",
            "what is the tr accuracy:  0.9584666666666667\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.13175263371234494\n",
            "what is the loss v  0.4093260639472208\n",
            "what is the loss test 0.4674460045998276\n",
            "************************************************************** 114\n",
            "what is the tr accuracy:  0.9588666666666666\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.13089561191029436\n",
            "what is the loss v  0.41311873245431857\n",
            "what is the loss test 0.46924676429333484\n",
            "************************************************************** 114\n",
            "what is the tr accuracy:  0.9588666666666666\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.13089561191029436\n",
            "what is the loss v  0.41311873245431857\n",
            "what is the loss test 0.46924676429333484\n",
            "************************************************************** 115\n",
            "what is the tr accuracy:  0.9596666666666667\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.1296421567883777\n",
            "what is the loss v  0.4155190762109702\n",
            "what is the loss test 0.47126436159628454\n",
            "************************************************************** 115\n",
            "what is the tr accuracy:  0.9596666666666667\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.1296421567883777\n",
            "what is the loss v  0.4155190762109702\n",
            "what is the loss test 0.47126436159628454\n",
            "************************************************************** 116\n",
            "what is the tr accuracy:  0.9602\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.12727068181772963\n",
            "what is the loss v  0.4160924053636813\n",
            "what is the loss test 0.47324057973479255\n",
            "************************************************************** 116\n",
            "what is the tr accuracy:  0.9602\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.12727068181772963\n",
            "what is the loss v  0.4160924053636813\n",
            "what is the loss test 0.47324057973479255\n",
            "************************************************************** 117\n",
            "what is the tr accuracy:  0.9611333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.12415421239229611\n",
            "what is the loss v  0.4159171315368543\n",
            "what is the loss test 0.4754345208885521\n",
            "************************************************************** 117\n",
            "what is the tr accuracy:  0.9611333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.12415421239229611\n",
            "what is the loss v  0.4159171315368543\n",
            "what is the loss test 0.4754345208885521\n",
            "************************************************************** 118\n",
            "what is the tr accuracy:  0.9621333333333333\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.12125952811631635\n",
            "what is the loss v  0.41635182152998723\n",
            "what is the loss test 0.47828259003726575\n",
            "************************************************************** 118\n",
            "what is the tr accuracy:  0.9621333333333333\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.12125952811631635\n",
            "what is the loss v  0.41635182152998723\n",
            "what is the loss test 0.47828259003726575\n",
            "************************************************************** 119\n",
            "what is the tr accuracy:  0.9629333333333333\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.11899059956717441\n",
            "what is the loss v  0.4182169293335327\n",
            "what is the loss test 0.48146201233716096\n",
            "************************************************************** 119\n",
            "what is the tr accuracy:  0.9629333333333333\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.11899059956717441\n",
            "what is the loss v  0.4182169293335327\n",
            "what is the loss test 0.48146201233716096\n",
            "************************************************************** 120\n",
            "what is the tr accuracy:  0.9634\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.1169742878768456\n",
            "what is the loss v  0.42104535472573024\n",
            "what is the loss test 0.4840551176086994\n",
            "************************************************************** 120\n",
            "what is the tr accuracy:  0.9634\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.1169742878768456\n",
            "what is the loss v  0.42104535472573024\n",
            "what is the loss test 0.4840551176086994\n",
            "************************************************************** 121\n",
            "what is the tr accuracy:  0.9636\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.11449635100534407\n",
            "what is the loss v  0.42387098016120867\n",
            "what is the loss test 0.48516955778581944\n",
            "************************************************************** 121\n",
            "what is the tr accuracy:  0.9636\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.11449635100534407\n",
            "what is the loss v  0.42387098016120867\n",
            "what is the loss test 0.48516955778581944\n",
            "************************************************************** 122\n",
            "what is the tr accuracy:  0.9648\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.111549598372338\n",
            "what is the loss v  0.42645953011646365\n",
            "what is the loss test 0.4846930486824408\n",
            "************************************************************** 122\n",
            "what is the tr accuracy:  0.9648\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.111549598372338\n",
            "what is the loss v  0.42645953011646365\n",
            "what is the loss test 0.4846930486824408\n",
            "************************************************************** 123\n",
            "what is the tr accuracy:  0.9654\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.10880086717941928\n",
            "what is the loss v  0.42933560356341116\n",
            "what is the loss test 0.483680146972959\n",
            "************************************************************** 123\n",
            "what is the tr accuracy:  0.9654\n",
            "what is the v accuracy:  0.916\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.10880086717941928\n",
            "what is the loss v  0.42933560356341116\n",
            "what is the loss test 0.483680146972959\n",
            "************************************************************** 124\n",
            "what is the tr accuracy:  0.9657333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.908223201174743\n",
            "what is the loss  0.10690705903177938\n",
            "what is the loss v  0.43278858282277366\n",
            "what is the loss test 0.4832085832781911\n",
            "************************************************************** 124\n",
            "what is the tr accuracy:  0.9657333333333333\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.908223201174743\n",
            "what is the loss  0.10690705903177938\n",
            "what is the loss v  0.43278858282277366\n",
            "what is the loss test 0.4832085832781911\n",
            "************************************************************** 125\n",
            "what is the tr accuracy:  0.9662666666666667\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.10558220036768823\n",
            "what is the loss v  0.4361592714237944\n",
            "what is the loss test 0.4833216083772553\n",
            "************************************************************** 125\n",
            "what is the tr accuracy:  0.9662666666666667\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.907856093979442\n",
            "what is the loss  0.10558220036768823\n",
            "what is the loss v  0.4361592714237944\n",
            "what is the loss test 0.4833216083772553\n",
            "************************************************************** 126\n",
            "what is the tr accuracy:  0.9665333333333334\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.908223201174743\n",
            "what is the loss  0.10415486728983542\n",
            "what is the loss v  0.43817498217360523\n",
            "what is the loss test 0.48367933119092593\n",
            "************************************************************** 126\n",
            "what is the tr accuracy:  0.9665333333333334\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.908223201174743\n",
            "what is the loss  0.10415486728983542\n",
            "what is the loss v  0.43817498217360523\n",
            "what is the loss test 0.48367933119092593\n",
            "************************************************************** 127\n",
            "what is the tr accuracy:  0.967\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.1023418720787773\n",
            "what is the loss v  0.4384112002506592\n",
            "what is the loss test 0.48389955964780385\n",
            "************************************************************** 127\n",
            "what is the tr accuracy:  0.967\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.1023418720787773\n",
            "what is the loss v  0.4384112002506592\n",
            "what is the loss test 0.48389955964780385\n",
            "************************************************************** 128\n",
            "what is the tr accuracy:  0.9686666666666667\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.10023438860339223\n",
            "what is the loss v  0.43746170852849736\n",
            "what is the loss test 0.4841741571597554\n",
            "************************************************************** 128\n",
            "what is the tr accuracy:  0.9686666666666667\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.10023438860339223\n",
            "what is the loss v  0.43746170852849736\n",
            "what is the loss test 0.4841741571597554\n",
            "************************************************************** 129\n",
            "what is the tr accuracy:  0.9694\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9074889867841409\n",
            "what is the loss  0.0982898362187937\n",
            "what is the loss v  0.4364927976801318\n",
            "what is the loss test 0.48504722082561474\n",
            "************************************************************** 129\n",
            "what is the tr accuracy:  0.9694\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9074889867841409\n",
            "what is the loss  0.0982898362187937\n",
            "what is the loss v  0.4364927976801318\n",
            "what is the loss test 0.48504722082561474\n",
            "************************************************************** 130\n",
            "what is the tr accuracy:  0.9696\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0967016340442692\n",
            "what is the loss v  0.4362015315688427\n",
            "what is the loss test 0.48631997058163\n",
            "************************************************************** 130\n",
            "what is the tr accuracy:  0.9696\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0967016340442692\n",
            "what is the loss v  0.4362015315688427\n",
            "what is the loss test 0.48631997058163\n",
            "************************************************************** 131\n",
            "what is the tr accuracy:  0.9698\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.09521174413706869\n",
            "what is the loss v  0.4366478728080647\n",
            "what is the loss test 0.4878051207504302\n",
            "************************************************************** 131\n",
            "what is the tr accuracy:  0.9698\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.09521174413706869\n",
            "what is the loss v  0.4366478728080647\n",
            "what is the loss test 0.4878051207504302\n",
            "************************************************************** 132\n",
            "what is the tr accuracy:  0.9701333333333333\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.09353551339439209\n",
            "what is the loss v  0.437792463794486\n",
            "what is the loss test 0.4892753448604281\n",
            "************************************************************** 132\n",
            "what is the tr accuracy:  0.9701333333333333\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.09353551339439209\n",
            "what is the loss v  0.437792463794486\n",
            "what is the loss test 0.4892753448604281\n",
            "************************************************************** 133\n",
            "what is the tr accuracy:  0.9702666666666667\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.09156694799006458\n",
            "what is the loss v  0.43933417689406445\n",
            "what is the loss test 0.4904872383267857\n",
            "************************************************************** 133\n",
            "what is the tr accuracy:  0.9702666666666667\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.09156694799006458\n",
            "what is the loss v  0.43933417689406445\n",
            "what is the loss test 0.4904872383267857\n",
            "************************************************************** 134\n",
            "what is the tr accuracy:  0.9712666666666666\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.08935381256811106\n",
            "what is the loss v  0.4412871118893173\n",
            "what is the loss test 0.49162780857595884\n",
            "************************************************************** 134\n",
            "what is the tr accuracy:  0.9712666666666666\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.08935381256811106\n",
            "what is the loss v  0.4412871118893173\n",
            "what is the loss test 0.49162780857595884\n",
            "************************************************************** 135\n",
            "what is the tr accuracy:  0.9718666666666667\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0871026879807109\n",
            "what is the loss v  0.44390907094918247\n",
            "what is the loss test 0.4929926722241206\n",
            "************************************************************** 135\n",
            "what is the tr accuracy:  0.9718666666666667\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0871026879807109\n",
            "what is the loss v  0.44390907094918247\n",
            "what is the loss test 0.4929926722241206\n",
            "************************************************************** 136\n",
            "what is the tr accuracy:  0.973\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.08503034312022152\n",
            "what is the loss v  0.44700973654892595\n",
            "what is the loss test 0.49492164539330347\n",
            "************************************************************** 136\n",
            "what is the tr accuracy:  0.973\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.08503034312022152\n",
            "what is the loss v  0.44700973654892595\n",
            "what is the loss test 0.49492164539330347\n",
            "************************************************************** 137\n",
            "what is the tr accuracy:  0.9740666666666666\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.08327743305641477\n",
            "what is the loss v  0.45056630999033687\n",
            "what is the loss test 0.49797469420933727\n",
            "************************************************************** 137\n",
            "what is the tr accuracy:  0.9740666666666666\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.08327743305641477\n",
            "what is the loss v  0.45056630999033687\n",
            "what is the loss test 0.49797469420933727\n",
            "************************************************************** 138\n",
            "what is the tr accuracy:  0.9744666666666667\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0818230542825032\n",
            "what is the loss v  0.45425678907095607\n",
            "what is the loss test 0.5021313874797763\n",
            "************************************************************** 138\n",
            "what is the tr accuracy:  0.9744666666666667\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0818230542825032\n",
            "what is the loss v  0.45425678907095607\n",
            "what is the loss test 0.5021313874797763\n",
            "************************************************************** 139\n",
            "what is the tr accuracy:  0.9752\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.0804005783425855\n",
            "what is the loss v  0.4572626231655926\n",
            "what is the loss test 0.5070042974327146\n",
            "************************************************************** 139\n",
            "what is the tr accuracy:  0.9752\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.0804005783425855\n",
            "what is the loss v  0.4572626231655926\n",
            "what is the loss test 0.5070042974327146\n",
            "************************************************************** 140\n",
            "what is the tr accuracy:  0.9753333333333334\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.07886167825473239\n",
            "what is the loss v  0.45938947744721437\n",
            "what is the loss test 0.5120504892062195\n",
            "************************************************************** 140\n",
            "what is the tr accuracy:  0.9753333333333334\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.07886167825473239\n",
            "what is the loss v  0.45938947744721437\n",
            "what is the loss test 0.5120504892062195\n",
            "************************************************************** 141\n",
            "what is the tr accuracy:  0.9755333333333334\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.07725833919196799\n",
            "what is the loss v  0.46097721592046514\n",
            "what is the loss test 0.5167665400970203\n",
            "************************************************************** 141\n",
            "what is the tr accuracy:  0.9755333333333334\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.07725833919196799\n",
            "what is the loss v  0.46097721592046514\n",
            "what is the loss test 0.5167665400970203\n",
            "************************************************************** 142\n",
            "what is the tr accuracy:  0.9764\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.07571902113643046\n",
            "what is the loss v  0.46255671614680466\n",
            "what is the loss test 0.5208821940422304\n",
            "************************************************************** 142\n",
            "what is the tr accuracy:  0.9764\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.07571902113643046\n",
            "what is the loss v  0.46255671614680466\n",
            "what is the loss test 0.5208821940422304\n",
            "************************************************************** 143\n",
            "what is the tr accuracy:  0.9762\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.07432879998157538\n",
            "what is the loss v  0.46446244713892143\n",
            "what is the loss test 0.5243924133316776\n",
            "************************************************************** 143\n",
            "what is the tr accuracy:  0.9762\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.07432879998157538\n",
            "what is the loss v  0.46446244713892143\n",
            "what is the loss test 0.5243924133316776\n",
            "************************************************************** 144\n",
            "what is the tr accuracy:  0.9769333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.07304046780026524\n",
            "what is the loss v  0.46647936191619077\n",
            "what is the loss test 0.5269602036243115\n",
            "************************************************************** 144\n",
            "what is the tr accuracy:  0.9769333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.07304046780026524\n",
            "what is the loss v  0.46647936191619077\n",
            "what is the loss test 0.5269602036243115\n",
            "************************************************************** 145\n",
            "what is the tr accuracy:  0.9775333333333334\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.07178250567186735\n",
            "what is the loss v  0.468835429418513\n",
            "what is the loss test 0.5285357933780365\n",
            "************************************************************** 145\n",
            "what is the tr accuracy:  0.9775333333333334\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.07178250567186735\n",
            "what is the loss v  0.468835429418513\n",
            "what is the loss test 0.5285357933780365\n",
            "************************************************************** 146\n",
            "what is the tr accuracy:  0.978\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.07048508351458177\n",
            "what is the loss v  0.4715906062792983\n",
            "what is the loss test 0.5291352103345063\n",
            "************************************************************** 146\n",
            "what is the tr accuracy:  0.978\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.07048508351458177\n",
            "what is the loss v  0.4715906062792983\n",
            "what is the loss test 0.5291352103345063\n",
            "************************************************************** 147\n",
            "what is the tr accuracy:  0.979\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.06915065069965166\n",
            "what is the loss v  0.474388617157095\n",
            "what is the loss test 0.5293153729339952\n",
            "************************************************************** 147\n",
            "what is the tr accuracy:  0.979\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.06915065069965166\n",
            "what is the loss v  0.474388617157095\n",
            "what is the loss test 0.5293153729339952\n",
            "************************************************************** 148\n",
            "what is the tr accuracy:  0.9790666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.06788919637124015\n",
            "what is the loss v  0.47711186893325014\n",
            "what is the loss test 0.5296559146857615\n",
            "************************************************************** 148\n",
            "what is the tr accuracy:  0.9790666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.06788919637124015\n",
            "what is the loss v  0.47711186893325014\n",
            "what is the loss test 0.5296559146857615\n",
            "************************************************************** 149\n",
            "what is the tr accuracy:  0.979\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.06686024372969875\n",
            "what is the loss v  0.47968787051172923\n",
            "what is the loss test 0.5308435820684511\n",
            "************************************************************** 149\n",
            "what is the tr accuracy:  0.979\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.06686024372969875\n",
            "what is the loss v  0.47968787051172923\n",
            "what is the loss test 0.5308435820684511\n",
            "************************************************************** 150\n",
            "what is the tr accuracy:  0.9790666666666666\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.06602436334036402\n",
            "what is the loss v  0.48193278102586923\n",
            "what is the loss test 0.533014100896307\n",
            "************************************************************** 150\n",
            "what is the tr accuracy:  0.9790666666666666\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.06602436334036402\n",
            "what is the loss v  0.48193278102586923\n",
            "what is the loss test 0.533014100896307\n",
            "************************************************************** 151\n",
            "what is the tr accuracy:  0.9793333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.06512868052974866\n",
            "what is the loss v  0.4832871200136949\n",
            "what is the loss test 0.5360912501469931\n",
            "************************************************************** 151\n",
            "what is the tr accuracy:  0.9793333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.06512868052974866\n",
            "what is the loss v  0.4832871200136949\n",
            "what is the loss test 0.5360912501469931\n",
            "************************************************************** 152\n",
            "what is the tr accuracy:  0.9796666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.06393434775003201\n",
            "what is the loss v  0.4833940665718569\n",
            "what is the loss test 0.5399339380304071\n",
            "************************************************************** 152\n",
            "what is the tr accuracy:  0.9796666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.06393434775003201\n",
            "what is the loss v  0.4833940665718569\n",
            "what is the loss test 0.5399339380304071\n",
            "************************************************************** 153\n",
            "what is the tr accuracy:  0.9802\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.062458084379591966\n",
            "what is the loss v  0.4825242152252077\n",
            "what is the loss test 0.544270467376012\n",
            "************************************************************** 153\n",
            "what is the tr accuracy:  0.9802\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.062458084379591966\n",
            "what is the loss v  0.4825242152252077\n",
            "what is the loss test 0.544270467376012\n",
            "************************************************************** 154\n",
            "what is the tr accuracy:  0.9804666666666667\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.061007083979350826\n",
            "what is the loss v  0.4815650054617153\n",
            "what is the loss test 0.5489752983993516\n",
            "************************************************************** 154\n",
            "what is the tr accuracy:  0.9804666666666667\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.061007083979350826\n",
            "what is the loss v  0.4815650054617153\n",
            "what is the loss test 0.5489752983993516\n",
            "************************************************************** 155\n",
            "what is the tr accuracy:  0.9810666666666666\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.05980970438682886\n",
            "what is the loss v  0.4812198270296459\n",
            "what is the loss test 0.553872526883525\n",
            "************************************************************** 155\n",
            "what is the tr accuracy:  0.9810666666666666\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.05980970438682886\n",
            "what is the loss v  0.4812198270296459\n",
            "what is the loss test 0.553872526883525\n",
            "************************************************************** 156\n",
            "what is the tr accuracy:  0.9814666666666667\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.05879070212237531\n",
            "what is the loss v  0.4818224395769022\n",
            "what is the loss test 0.5585655526658106\n",
            "************************************************************** 156\n",
            "what is the tr accuracy:  0.9814666666666667\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.05879070212237531\n",
            "what is the loss v  0.4818224395769022\n",
            "what is the loss test 0.5585655526658106\n",
            "************************************************************** 157\n",
            "what is the tr accuracy:  0.983\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.05767996783898645\n",
            "what is the loss v  0.4832686464403051\n",
            "what is the loss test 0.5626129654720321\n",
            "************************************************************** 157\n",
            "what is the tr accuracy:  0.983\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.05767996783898645\n",
            "what is the loss v  0.4832686464403051\n",
            "what is the loss test 0.5626129654720321\n",
            "************************************************************** 158\n",
            "what is the tr accuracy:  0.9830666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.056293857018413974\n",
            "what is the loss v  0.4854395215460782\n",
            "what is the loss test 0.5658164817745455\n",
            "************************************************************** 158\n",
            "what is the tr accuracy:  0.9830666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.056293857018413974\n",
            "what is the loss v  0.4854395215460782\n",
            "what is the loss test 0.5658164817745455\n",
            "************************************************************** 159\n",
            "what is the tr accuracy:  0.9835333333333334\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.05468425902643212\n",
            "what is the loss v  0.48832635067165103\n",
            "what is the loss test 0.5683723553276366\n",
            "************************************************************** 159\n",
            "what is the tr accuracy:  0.9835333333333334\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.05468425902643212\n",
            "what is the loss v  0.48832635067165103\n",
            "what is the loss test 0.5683723553276366\n",
            "************************************************************** 160\n",
            "what is the tr accuracy:  0.9839333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.05315287343417068\n",
            "what is the loss v  0.4921203495850241\n",
            "what is the loss test 0.5707945193168111\n",
            "************************************************************** 160\n",
            "what is the tr accuracy:  0.9839333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.05315287343417068\n",
            "what is the loss v  0.4921203495850241\n",
            "what is the loss test 0.5707945193168111\n",
            "************************************************************** 161\n",
            "what is the tr accuracy:  0.9847333333333333\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.05197386667670678\n",
            "what is the loss v  0.4971932671124287\n",
            "what is the loss test 0.5736245232056739\n",
            "************************************************************** 161\n",
            "what is the tr accuracy:  0.9847333333333333\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.05197386667670678\n",
            "what is the loss v  0.4971932671124287\n",
            "what is the loss test 0.5736245232056739\n",
            "************************************************************** 162\n",
            "what is the tr accuracy:  0.9852666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.05118005043039062\n",
            "what is the loss v  0.503164866032867\n",
            "what is the loss test 0.5771925428163734\n",
            "************************************************************** 162\n",
            "what is the tr accuracy:  0.9852666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.05118005043039062\n",
            "what is the loss v  0.503164866032867\n",
            "what is the loss test 0.5771925428163734\n",
            "************************************************************** 163\n",
            "what is the tr accuracy:  0.9851333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.05054837456140093\n",
            "what is the loss v  0.5090845523968521\n",
            "what is the loss test 0.5811998218973918\n",
            "************************************************************** 163\n",
            "what is the tr accuracy:  0.9851333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.05054837456140093\n",
            "what is the loss v  0.5090845523968521\n",
            "what is the loss test 0.5811998218973918\n",
            "************************************************************** 164\n",
            "what is the tr accuracy:  0.9854666666666667\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.04970227501858562\n",
            "what is the loss v  0.5142217777184628\n",
            "what is the loss test 0.5851961619117204\n",
            "************************************************************** 164\n",
            "what is the tr accuracy:  0.9854666666666667\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.04970227501858562\n",
            "what is the loss v  0.5142217777184628\n",
            "what is the loss test 0.5851961619117204\n",
            "************************************************************** 165\n",
            "what is the tr accuracy:  0.9860666666666666\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.04853896473643979\n",
            "what is the loss v  0.5181299634245932\n",
            "what is the loss test 0.5892139336924898\n",
            "************************************************************** 165\n",
            "what is the tr accuracy:  0.9860666666666666\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.04853896473643979\n",
            "what is the loss v  0.5181299634245932\n",
            "what is the loss test 0.5892139336924898\n",
            "************************************************************** 166\n",
            "what is the tr accuracy:  0.9866666666666667\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.04727004063598405\n",
            "what is the loss v  0.5210529874983026\n",
            "what is the loss test 0.593441313299058\n",
            "************************************************************** 166\n",
            "what is the tr accuracy:  0.9866666666666667\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.04727004063598405\n",
            "what is the loss v  0.5210529874983026\n",
            "what is the loss test 0.593441313299058\n",
            "************************************************************** 167\n",
            "what is the tr accuracy:  0.9864666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0462030672394177\n",
            "what is the loss v  0.5239102859876354\n",
            "what is the loss test 0.5979858932320782\n",
            "************************************************************** 167\n",
            "what is the tr accuracy:  0.9864666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.0462030672394177\n",
            "what is the loss v  0.5239102859876354\n",
            "what is the loss test 0.5979858932320782\n",
            "************************************************************** 168\n",
            "what is the tr accuracy:  0.9862666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.04542206312416338\n",
            "what is the loss v  0.5271516675368524\n",
            "what is the loss test 0.6029500447003305\n",
            "************************************************************** 168\n",
            "what is the tr accuracy:  0.9862666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.04542206312416338\n",
            "what is the loss v  0.5271516675368524\n",
            "what is the loss test 0.6029500447003305\n",
            "************************************************************** 169\n",
            "what is the tr accuracy:  0.9864\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.044781045948570714\n",
            "what is the loss v  0.5311812778822745\n",
            "what is the loss test 0.6081849078339999\n",
            "************************************************************** 169\n",
            "what is the tr accuracy:  0.9864\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.044781045948570714\n",
            "what is the loss v  0.5311812778822745\n",
            "what is the loss test 0.6081849078339999\n",
            "************************************************************** 170\n",
            "what is the tr accuracy:  0.9868\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.04407037282591115\n",
            "what is the loss v  0.5362164709446251\n",
            "what is the loss test 0.6134283322783112\n",
            "************************************************************** 170\n",
            "what is the tr accuracy:  0.9868\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.04407037282591115\n",
            "what is the loss v  0.5362164709446251\n",
            "what is the loss test 0.6134283322783112\n",
            "************************************************************** 171\n",
            "what is the tr accuracy:  0.987\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.043203111525063925\n",
            "what is the loss v  0.5423239683392882\n",
            "what is the loss test 0.6184497425873458\n",
            "************************************************************** 171\n",
            "what is the tr accuracy:  0.987\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.043203111525063925\n",
            "what is the loss v  0.5423239683392882\n",
            "what is the loss test 0.6184497425873458\n",
            "************************************************************** 172\n",
            "what is the tr accuracy:  0.9876\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.0422962986666929\n",
            "what is the loss v  0.5493797212012954\n",
            "what is the loss test 0.6231288101591067\n",
            "************************************************************** 172\n",
            "what is the tr accuracy:  0.9876\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.0422962986666929\n",
            "what is the loss v  0.5493797212012954\n",
            "what is the loss test 0.6231288101591067\n",
            "************************************************************** 173\n",
            "what is the tr accuracy:  0.988\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.04148270768753896\n",
            "what is the loss v  0.5569861048149386\n",
            "what is the loss test 0.6274479968266522\n",
            "************************************************************** 173\n",
            "what is the tr accuracy:  0.988\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.04148270768753896\n",
            "what is the loss v  0.5569861048149386\n",
            "what is the loss test 0.6274479968266522\n",
            "************************************************************** 174\n",
            "what is the tr accuracy:  0.9879333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.040852522201512025\n",
            "what is the loss v  0.564477216239745\n",
            "what is the loss test 0.6315010832333068\n",
            "************************************************************** 174\n",
            "what is the tr accuracy:  0.9879333333333333\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.908957415565345\n",
            "what is the loss  0.040852522201512025\n",
            "what is the loss v  0.564477216239745\n",
            "what is the loss test 0.6315010832333068\n",
            "************************************************************** 175\n",
            "what is the tr accuracy:  0.9876\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.0402679931144323\n",
            "what is the loss v  0.5710221681605867\n",
            "what is the loss test 0.6351285243945011\n",
            "************************************************************** 175\n",
            "what is the tr accuracy:  0.9876\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.0402679931144323\n",
            "what is the loss v  0.5710221681605867\n",
            "what is the loss test 0.6351285243945011\n",
            "************************************************************** 176\n",
            "what is the tr accuracy:  0.9876\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.03954106114864797\n",
            "what is the loss v  0.57613332803909\n",
            "what is the loss test 0.6383891442406192\n",
            "************************************************************** 176\n",
            "what is the tr accuracy:  0.9876\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.03954106114864797\n",
            "what is the loss v  0.57613332803909\n",
            "what is the loss test 0.6383891442406192\n",
            "************************************************************** 177\n",
            "what is the tr accuracy:  0.9878666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.03863668920829896\n",
            "what is the loss v  0.5793798100077234\n",
            "what is the loss test 0.6415963912999314\n",
            "************************************************************** 177\n",
            "what is the tr accuracy:  0.9878666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.03863668920829896\n",
            "what is the loss v  0.5793798100077234\n",
            "what is the loss test 0.6415963912999314\n",
            "************************************************************** 178\n",
            "what is the tr accuracy:  0.9882666666666666\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.03773287097118221\n",
            "what is the loss v  0.5810474972125421\n",
            "what is the loss test 0.6450227035341286\n",
            "************************************************************** 178\n",
            "what is the tr accuracy:  0.9882666666666666\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.03773287097118221\n",
            "what is the loss v  0.5810474972125421\n",
            "what is the loss test 0.6450227035341286\n",
            "************************************************************** 179\n",
            "what is the tr accuracy:  0.9888666666666667\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.037000603897673665\n",
            "what is the loss v  0.5817017576285114\n",
            "what is the loss test 0.64899074993141\n",
            "************************************************************** 179\n",
            "what is the tr accuracy:  0.9888666666666667\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.037000603897673665\n",
            "what is the loss v  0.5817017576285114\n",
            "what is the loss test 0.64899074993141\n",
            "************************************************************** 180\n",
            "what is the tr accuracy:  0.9892\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.03646882631220671\n",
            "what is the loss v  0.5819710664686222\n",
            "what is the loss test 0.6535063145420441\n",
            "************************************************************** 180\n",
            "what is the tr accuracy:  0.9892\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.03646882631220671\n",
            "what is the loss v  0.5819710664686222\n",
            "what is the loss test 0.6535063145420441\n",
            "************************************************************** 181\n",
            "what is the tr accuracy:  0.9895333333333334\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.03603227422569761\n",
            "what is the loss v  0.5821102587172778\n",
            "what is the loss test 0.6584315556988573\n",
            "************************************************************** 181\n",
            "what is the tr accuracy:  0.9895333333333334\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.03603227422569761\n",
            "what is the loss v  0.5821102587172778\n",
            "what is the loss test 0.6584315556988573\n",
            "************************************************************** 182\n",
            "what is the tr accuracy:  0.9896\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.035537330995251554\n",
            "what is the loss v  0.5825308499833621\n",
            "what is the loss test 0.6634831716075845\n",
            "************************************************************** 182\n",
            "what is the tr accuracy:  0.9896\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.035537330995251554\n",
            "what is the loss v  0.5825308499833621\n",
            "what is the loss test 0.6634831716075845\n",
            "************************************************************** 183\n",
            "what is the tr accuracy:  0.9897333333333334\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.03489161282369303\n",
            "what is the loss v  0.5834789271778099\n",
            "what is the loss test 0.6684900634435729\n",
            "************************************************************** 183\n",
            "what is the tr accuracy:  0.9897333333333334\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.03489161282369303\n",
            "what is the loss v  0.5834789271778099\n",
            "what is the loss test 0.6684900634435729\n",
            "************************************************************** 184\n",
            "what is the tr accuracy:  0.9900666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.034124537317442395\n",
            "what is the loss v  0.5850136397166318\n",
            "what is the loss test 0.6732978415838664\n",
            "************************************************************** 184\n",
            "what is the tr accuracy:  0.9900666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.034124537317442395\n",
            "what is the loss v  0.5850136397166318\n",
            "what is the loss test 0.6732978415838664\n",
            "************************************************************** 185\n",
            "what is the tr accuracy:  0.9904666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.033351452682076836\n",
            "what is the loss v  0.5872192226846897\n",
            "what is the loss test 0.6778134629484529\n",
            "************************************************************** 185\n",
            "what is the tr accuracy:  0.9904666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.033351452682076836\n",
            "what is the loss v  0.5872192226846897\n",
            "what is the loss test 0.6778134629484529\n",
            "************************************************************** 186\n",
            "what is the tr accuracy:  0.9906\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.03266744097730587\n",
            "what is the loss v  0.5899841830140995\n",
            "what is the loss test 0.68204319393351\n",
            "************************************************************** 186\n",
            "what is the tr accuracy:  0.9906\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9111600587371512\n",
            "what is the loss  0.03266744097730587\n",
            "what is the loss v  0.5899841830140995\n",
            "what is the loss test 0.68204319393351\n",
            "************************************************************** 187\n",
            "what is the tr accuracy:  0.9907333333333334\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.03209427182795711\n",
            "what is the loss v  0.593032686599715\n",
            "what is the loss test 0.6858645684119765\n",
            "************************************************************** 187\n",
            "what is the tr accuracy:  0.9907333333333334\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.03209427182795711\n",
            "what is the loss v  0.593032686599715\n",
            "what is the loss test 0.6858645684119765\n",
            "************************************************************** 188\n",
            "what is the tr accuracy:  0.9908666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.031563680180531184\n",
            "what is the loss v  0.5959152063908362\n",
            "what is the loss test 0.6890619150406477\n",
            "************************************************************** 188\n",
            "what is the tr accuracy:  0.9908666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.031563680180531184\n",
            "what is the loss v  0.5959152063908362\n",
            "what is the loss test 0.6890619150406477\n",
            "************************************************************** 189\n",
            "what is the tr accuracy:  0.9908666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.030971875667356114\n",
            "what is the loss v  0.5981863968648867\n",
            "what is the loss test 0.6914693296480074\n",
            "************************************************************** 189\n",
            "what is the tr accuracy:  0.9908666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9096916299559471\n",
            "what is the loss  0.030971875667356114\n",
            "what is the loss v  0.5981863968648867\n",
            "what is the loss test 0.6914693296480074\n",
            "************************************************************** 190\n",
            "what is the tr accuracy:  0.9910666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.030279413967700623\n",
            "what is the loss v  0.5998480476517938\n",
            "what is the loss test 0.6931807598042934\n",
            "************************************************************** 190\n",
            "what is the tr accuracy:  0.9910666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.030279413967700623\n",
            "what is the loss v  0.5998480476517938\n",
            "what is the loss test 0.6931807598042934\n",
            "************************************************************** 191\n",
            "what is the tr accuracy:  0.9914\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.029541050987665473\n",
            "what is the loss v  0.6010672349172917\n",
            "what is the loss test 0.6944689477490624\n",
            "************************************************************** 191\n",
            "what is the tr accuracy:  0.9914\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9104258443465492\n",
            "what is the loss  0.029541050987665473\n",
            "what is the loss v  0.6010672349172917\n",
            "what is the loss test 0.6944689477490624\n",
            "************************************************************** 192\n",
            "what is the tr accuracy:  0.9918\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.028832958614709928\n",
            "what is the loss v  0.6019777798483139\n",
            "what is the loss test 0.6957853700500566\n",
            "************************************************************** 192\n",
            "what is the tr accuracy:  0.9918\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.028832958614709928\n",
            "what is the loss v  0.6019777798483139\n",
            "what is the loss test 0.6957853700500566\n",
            "************************************************************** 193\n",
            "what is the tr accuracy:  0.9922\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.028240826365684677\n",
            "what is the loss v  0.6028026548840524\n",
            "what is the loss test 0.697452165685379\n",
            "************************************************************** 193\n",
            "what is the tr accuracy:  0.9922\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.028240826365684677\n",
            "what is the loss v  0.6028026548840524\n",
            "what is the loss test 0.697452165685379\n",
            "************************************************************** 194\n",
            "what is the tr accuracy:  0.9923333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.027769556402524743\n",
            "what is the loss v  0.6038705588780985\n",
            "what is the loss test 0.6996407774687987\n",
            "************************************************************** 194\n",
            "what is the tr accuracy:  0.9923333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.027769556402524743\n",
            "what is the loss v  0.6038705588780985\n",
            "what is the loss test 0.6996407774687987\n",
            "************************************************************** 195\n",
            "what is the tr accuracy:  0.9925333333333334\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.02737230293168177\n",
            "what is the loss v  0.6051882518075926\n",
            "what is the loss test 0.7022790485486545\n",
            "************************************************************** 195\n",
            "what is the tr accuracy:  0.9925333333333334\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.02737230293168177\n",
            "what is the loss v  0.6051882518075926\n",
            "what is the loss test 0.7022790485486545\n",
            "************************************************************** 196\n",
            "what is the tr accuracy:  0.9926\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.026946041765094318\n",
            "what is the loss v  0.6067970647202358\n",
            "what is the loss test 0.7052277288102866\n",
            "************************************************************** 196\n",
            "what is the tr accuracy:  0.9926\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.026946041765094318\n",
            "what is the loss v  0.6067970647202358\n",
            "what is the loss test 0.7052277288102866\n",
            "************************************************************** 197\n",
            "what is the tr accuracy:  0.9928\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.026397410987868516\n",
            "what is the loss v  0.6087889128864599\n",
            "what is the loss test 0.7082309647931598\n",
            "************************************************************** 197\n",
            "what is the tr accuracy:  0.9928\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.026397410987868516\n",
            "what is the loss v  0.6087889128864599\n",
            "what is the loss test 0.7082309647931598\n",
            "************************************************************** 198\n",
            "what is the tr accuracy:  0.993\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.025727377790679396\n",
            "what is the loss v  0.6111488825652703\n",
            "what is the loss test 0.7110903164875593\n",
            "************************************************************** 198\n",
            "what is the tr accuracy:  0.993\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.025727377790679396\n",
            "what is the loss v  0.6111488825652703\n",
            "what is the loss test 0.7110903164875593\n",
            "************************************************************** 199\n",
            "what is the tr accuracy:  0.9929333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.025021255722223728\n",
            "what is the loss v  0.613802575034475\n",
            "what is the loss test 0.7139357616913177\n",
            "************************************************************** 199\n",
            "what is the tr accuracy:  0.9929333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.025021255722223728\n",
            "what is the loss v  0.613802575034475\n",
            "what is the loss test 0.7139357616913177\n",
            "min loss is 0.4147244710028816 at epoch 67\n",
            "min loss is 0.4147244710028816 at epoch 67\n",
            "************************************************************** 0\n",
            "what is the tr accuracy:  0.10646666666666667\n",
            "what is the v accuracy:  0.127\n",
            "what is the t accuracy:  0.11270190895741557\n",
            "what is the loss  2.3513150062056454\n",
            "what is the loss v  2.3374734203026066\n",
            "what is the loss test 2.3535583415900256\n",
            "************************************************************** 0\n",
            "what is the tr accuracy:  0.10646666666666667\n",
            "what is the v accuracy:  0.127\n",
            "what is the t accuracy:  0.11270190895741557\n",
            "what is the loss  2.3513150062056454\n",
            "what is the loss v  2.3374734203026066\n",
            "what is the loss test 2.3535583415900256\n",
            "************************************************************** 1\n",
            "what is the tr accuracy:  0.17753333333333332\n",
            "what is the v accuracy:  0.185\n",
            "what is the t accuracy:  0.1670337738619677\n",
            "what is the loss  2.2220763885841675\n",
            "what is the loss v  2.251711725023023\n",
            "what is the loss test 2.2421829054433076\n",
            "************************************************************** 1\n",
            "what is the tr accuracy:  0.17753333333333332\n",
            "what is the v accuracy:  0.185\n",
            "what is the t accuracy:  0.1670337738619677\n",
            "what is the loss  2.2220763885841675\n",
            "what is the loss v  2.251711725023023\n",
            "what is the loss test 2.2421829054433076\n",
            "************************************************************** 2\n",
            "what is the tr accuracy:  0.5362666666666667\n",
            "what is the v accuracy:  0.534\n",
            "what is the t accuracy:  0.5499265785609398\n",
            "what is the loss  2.1956328898062702\n",
            "what is the loss v  2.2249424394211523\n",
            "what is the loss test 2.12264970864733\n",
            "************************************************************** 2\n",
            "what is the tr accuracy:  0.5362666666666667\n",
            "what is the v accuracy:  0.534\n",
            "what is the t accuracy:  0.5499265785609398\n",
            "what is the loss  2.1956328898062702\n",
            "what is the loss v  2.2249424394211523\n",
            "what is the loss test 2.12264970864733\n",
            "************************************************************** 3\n",
            "what is the tr accuracy:  0.4132\n",
            "what is the v accuracy:  0.396\n",
            "what is the t accuracy:  0.42841409691629956\n",
            "what is the loss  2.5354289000533763\n",
            "what is the loss v  2.5825008311097437\n",
            "what is the loss test 2.4157239337707503\n",
            "************************************************************** 3\n",
            "what is the tr accuracy:  0.4132\n",
            "what is the v accuracy:  0.396\n",
            "what is the t accuracy:  0.42841409691629956\n",
            "what is the loss  2.5354289000533763\n",
            "what is the loss v  2.5825008311097437\n",
            "what is the loss test 2.4157239337707503\n",
            "************************************************************** 4\n",
            "what is the tr accuracy:  0.4984\n",
            "what is the v accuracy:  0.495\n",
            "what is the t accuracy:  0.5036710719530103\n",
            "what is the loss  2.1669185470983265\n",
            "what is the loss v  2.235938535339689\n",
            "what is the loss test 2.144813580635545\n",
            "************************************************************** 4\n",
            "what is the tr accuracy:  0.4984\n",
            "what is the v accuracy:  0.495\n",
            "what is the t accuracy:  0.5036710719530103\n",
            "what is the loss  2.1669185470983265\n",
            "what is the loss v  2.235938535339689\n",
            "what is the loss test 2.144813580635545\n",
            "************************************************************** 5\n",
            "what is the tr accuracy:  0.6228\n",
            "what is the v accuracy:  0.62\n",
            "what is the t accuracy:  0.6255506607929515\n",
            "what is the loss  1.3454046959402925\n",
            "what is the loss v  1.372166889375252\n",
            "what is the loss test 1.3730815766483133\n",
            "************************************************************** 5\n",
            "what is the tr accuracy:  0.6228\n",
            "what is the v accuracy:  0.62\n",
            "what is the t accuracy:  0.6255506607929515\n",
            "what is the loss  1.3454046959402925\n",
            "what is the loss v  1.372166889375252\n",
            "what is the loss test 1.3730815766483133\n",
            "************************************************************** 6\n",
            "what is the tr accuracy:  0.5916\n",
            "what is the v accuracy:  0.579\n",
            "what is the t accuracy:  0.5961820851688693\n",
            "what is the loss  1.5259948968337573\n",
            "what is the loss v  1.5439881441918\n",
            "what is the loss test 1.5352841982694252\n",
            "************************************************************** 6\n",
            "what is the tr accuracy:  0.5916\n",
            "what is the v accuracy:  0.579\n",
            "what is the t accuracy:  0.5961820851688693\n",
            "what is the loss  1.5259948968337573\n",
            "what is the loss v  1.5439881441918\n",
            "what is the loss test 1.5352841982694252\n",
            "************************************************************** 7\n",
            "what is the tr accuracy:  0.6740666666666667\n",
            "what is the v accuracy:  0.664\n",
            "what is the t accuracy:  0.6813509544787077\n",
            "what is the loss  1.0265294109878969\n",
            "what is the loss v  1.028817780873194\n",
            "what is the loss test 1.0015916416254842\n",
            "************************************************************** 7\n",
            "what is the tr accuracy:  0.6740666666666667\n",
            "what is the v accuracy:  0.664\n",
            "what is the t accuracy:  0.6813509544787077\n",
            "what is the loss  1.0265294109878969\n",
            "what is the loss v  1.028817780873194\n",
            "what is the loss test 1.0015916416254842\n",
            "************************************************************** 8\n",
            "what is the tr accuracy:  0.6978\n",
            "what is the v accuracy:  0.704\n",
            "what is the t accuracy:  0.7121879588839941\n",
            "what is the loss  0.9071324527428741\n",
            "what is the loss v  0.9114079795339224\n",
            "what is the loss test 0.8737089319145386\n",
            "************************************************************** 8\n",
            "what is the tr accuracy:  0.6978\n",
            "what is the v accuracy:  0.704\n",
            "what is the t accuracy:  0.7121879588839941\n",
            "what is the loss  0.9071324527428741\n",
            "what is the loss v  0.9114079795339224\n",
            "what is the loss test 0.8737089319145386\n",
            "************************************************************** 9\n",
            "what is the tr accuracy:  0.7853333333333333\n",
            "what is the v accuracy:  0.783\n",
            "what is the t accuracy:  0.7933186490455213\n",
            "what is the loss  0.7806149354990671\n",
            "what is the loss v  0.7921667671924428\n",
            "what is the loss test 0.7591685679964573\n",
            "************************************************************** 9\n",
            "what is the tr accuracy:  0.7853333333333333\n",
            "what is the v accuracy:  0.783\n",
            "what is the t accuracy:  0.7933186490455213\n",
            "what is the loss  0.7806149354990671\n",
            "what is the loss v  0.7921667671924428\n",
            "what is the loss test 0.7591685679964573\n",
            "************************************************************** 10\n",
            "what is the tr accuracy:  0.7722\n",
            "what is the v accuracy:  0.772\n",
            "what is the t accuracy:  0.7709251101321586\n",
            "what is the loss  0.7580923017689364\n",
            "what is the loss v  0.7669893846579321\n",
            "what is the loss test 0.7516598287496316\n",
            "************************************************************** 10\n",
            "what is the tr accuracy:  0.7722\n",
            "what is the v accuracy:  0.772\n",
            "what is the t accuracy:  0.7709251101321586\n",
            "what is the loss  0.7580923017689364\n",
            "what is the loss v  0.7669893846579321\n",
            "what is the loss test 0.7516598287496316\n",
            "************************************************************** 11\n",
            "what is the tr accuracy:  0.8388666666666666\n",
            "what is the v accuracy:  0.837\n",
            "what is the t accuracy:  0.841776798825257\n",
            "what is the loss  0.5909898099428216\n",
            "what is the loss v  0.5856680363460892\n",
            "what is the loss test 0.5961765589207196\n",
            "************************************************************** 11\n",
            "what is the tr accuracy:  0.8388666666666666\n",
            "what is the v accuracy:  0.837\n",
            "what is the t accuracy:  0.841776798825257\n",
            "what is the loss  0.5909898099428216\n",
            "what is the loss v  0.5856680363460892\n",
            "what is the loss test 0.5961765589207196\n",
            "************************************************************** 12\n",
            "what is the tr accuracy:  0.8489333333333333\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8520558002936858\n",
            "what is the loss  0.5200500011774858\n",
            "what is the loss v  0.5047889244268153\n",
            "what is the loss test 0.5277557368956919\n",
            "************************************************************** 12\n",
            "what is the tr accuracy:  0.8489333333333333\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8520558002936858\n",
            "what is the loss  0.5200500011774858\n",
            "what is the loss v  0.5047889244268153\n",
            "what is the loss test 0.5277557368956919\n",
            "************************************************************** 13\n",
            "what is the tr accuracy:  0.8304666666666667\n",
            "what is the v accuracy:  0.833\n",
            "what is the t accuracy:  0.8399412628487518\n",
            "what is the loss  0.5657747134620219\n",
            "what is the loss v  0.5469396702419388\n",
            "what is the loss test 0.5607900979483387\n",
            "************************************************************** 13\n",
            "what is the tr accuracy:  0.8304666666666667\n",
            "what is the v accuracy:  0.833\n",
            "what is the t accuracy:  0.8399412628487518\n",
            "what is the loss  0.5657747134620219\n",
            "what is the loss v  0.5469396702419388\n",
            "what is the loss test 0.5607900979483387\n",
            "************************************************************** 14\n",
            "what is the tr accuracy:  0.8409333333333333\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8494860499265786\n",
            "what is the loss  0.5583508998441414\n",
            "what is the loss v  0.5381440368520107\n",
            "what is the loss test 0.5416022447361807\n",
            "************************************************************** 14\n",
            "what is the tr accuracy:  0.8409333333333333\n",
            "what is the v accuracy:  0.849\n",
            "what is the t accuracy:  0.8494860499265786\n",
            "what is the loss  0.5583508998441414\n",
            "what is the loss v  0.5381440368520107\n",
            "what is the loss test 0.5416022447361807\n",
            "************************************************************** 15\n",
            "what is the tr accuracy:  0.8369333333333333\n",
            "what is the v accuracy:  0.838\n",
            "what is the t accuracy:  0.8538913362701909\n",
            "what is the loss  0.5319887506434698\n",
            "what is the loss v  0.5226890622870444\n",
            "what is the loss test 0.5154238746843469\n",
            "************************************************************** 15\n",
            "what is the tr accuracy:  0.8369333333333333\n",
            "what is the v accuracy:  0.838\n",
            "what is the t accuracy:  0.8538913362701909\n",
            "what is the loss  0.5319887506434698\n",
            "what is the loss v  0.5226890622870444\n",
            "what is the loss test 0.5154238746843469\n",
            "************************************************************** 16\n",
            "what is the tr accuracy:  0.8657333333333334\n",
            "what is the v accuracy:  0.866\n",
            "what is the t accuracy:  0.8707782672540382\n",
            "what is the loss  0.5292472287530096\n",
            "what is the loss v  0.5224056153063582\n",
            "what is the loss test 0.5245793817862412\n",
            "************************************************************** 16\n",
            "what is the tr accuracy:  0.8657333333333334\n",
            "what is the v accuracy:  0.866\n",
            "what is the t accuracy:  0.8707782672540382\n",
            "what is the loss  0.5292472287530096\n",
            "what is the loss v  0.5224056153063582\n",
            "what is the loss test 0.5245793817862412\n",
            "************************************************************** 17\n",
            "what is the tr accuracy:  0.8530666666666666\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.854992657856094\n",
            "what is the loss  0.6067837400642918\n",
            "what is the loss v  0.6092524520420376\n",
            "what is the loss test 0.6120269685950614\n",
            "************************************************************** 17\n",
            "what is the tr accuracy:  0.8530666666666666\n",
            "what is the v accuracy:  0.847\n",
            "what is the t accuracy:  0.854992657856094\n",
            "what is the loss  0.6067837400642918\n",
            "what is the loss v  0.6092524520420376\n",
            "what is the loss test 0.6120269685950614\n",
            "************************************************************** 18\n",
            "what is the tr accuracy:  0.8676\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.867474302496329\n",
            "what is the loss  0.5625978000621712\n",
            "what is the loss v  0.561466732400026\n",
            "what is the loss test 0.5719868210025136\n",
            "************************************************************** 18\n",
            "what is the tr accuracy:  0.8676\n",
            "what is the v accuracy:  0.863\n",
            "what is the t accuracy:  0.867474302496329\n",
            "what is the loss  0.5625978000621712\n",
            "what is the loss v  0.561466732400026\n",
            "what is the loss test 0.5719868210025136\n",
            "************************************************************** 19\n",
            "what is the tr accuracy:  0.8757333333333334\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.511890697730977\n",
            "what is the loss v  0.5058813597294297\n",
            "what is the loss test 0.5237068285460141\n",
            "************************************************************** 19\n",
            "what is the tr accuracy:  0.8757333333333334\n",
            "what is the v accuracy:  0.875\n",
            "what is the t accuracy:  0.880690161527166\n",
            "what is the loss  0.511890697730977\n",
            "what is the loss v  0.5058813597294297\n",
            "what is the loss test 0.5237068285460141\n",
            "************************************************************** 20\n",
            "what is the tr accuracy:  0.8628\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8733480176211453\n",
            "what is the loss  0.5756942519644141\n",
            "what is the loss v  0.5713444965653967\n",
            "what is the loss test 0.5812416580007145\n",
            "************************************************************** 20\n",
            "what is the tr accuracy:  0.8628\n",
            "what is the v accuracy:  0.861\n",
            "what is the t accuracy:  0.8733480176211453\n",
            "what is the loss  0.5756942519644141\n",
            "what is the loss v  0.5713444965653967\n",
            "what is the loss test 0.5812416580007145\n",
            "************************************************************** 21\n",
            "what is the tr accuracy:  0.8669333333333333\n",
            "what is the v accuracy:  0.872\n",
            "what is the t accuracy:  0.8762848751835536\n",
            "what is the loss  0.556333468245041\n",
            "what is the loss v  0.5468040869895663\n",
            "what is the loss test 0.5628128846930844\n",
            "************************************************************** 21\n",
            "what is the tr accuracy:  0.8669333333333333\n",
            "what is the v accuracy:  0.872\n",
            "what is the t accuracy:  0.8762848751835536\n",
            "what is the loss  0.556333468245041\n",
            "what is the loss v  0.5468040869895663\n",
            "what is the loss test 0.5628128846930844\n",
            "************************************************************** 22\n",
            "what is the tr accuracy:  0.8772\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8828928046989721\n",
            "what is the loss  0.5245294448492873\n",
            "what is the loss v  0.5126538952144548\n",
            "what is the loss test 0.5307704185390072\n",
            "************************************************************** 22\n",
            "what is the tr accuracy:  0.8772\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8828928046989721\n",
            "what is the loss  0.5245294448492873\n",
            "what is the loss v  0.5126538952144548\n",
            "what is the loss test 0.5307704185390072\n",
            "************************************************************** 23\n",
            "what is the tr accuracy:  0.8854666666666666\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.49205529097880246\n",
            "what is the loss v  0.47069264023054347\n",
            "what is the loss test 0.4992565220982849\n",
            "************************************************************** 23\n",
            "what is the tr accuracy:  0.8854666666666666\n",
            "what is the v accuracy:  0.888\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.49205529097880246\n",
            "what is the loss v  0.47069264023054347\n",
            "what is the loss test 0.4992565220982849\n",
            "************************************************************** 24\n",
            "what is the tr accuracy:  0.8620666666666666\n",
            "what is the v accuracy:  0.858\n",
            "what is the t accuracy:  0.8612334801762115\n",
            "what is the loss  0.5561025821903007\n",
            "what is the loss v  0.5471066844841737\n",
            "what is the loss test 0.5626713907355118\n",
            "************************************************************** 24\n",
            "what is the tr accuracy:  0.8620666666666666\n",
            "what is the v accuracy:  0.858\n",
            "what is the t accuracy:  0.8612334801762115\n",
            "what is the loss  0.5561025821903007\n",
            "what is the loss v  0.5471066844841737\n",
            "what is the loss test 0.5626713907355118\n",
            "************************************************************** 25\n",
            "what is the tr accuracy:  0.8853333333333333\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8854625550660793\n",
            "what is the loss  0.47959290474404054\n",
            "what is the loss v  0.47765049962120565\n",
            "what is the loss test 0.5007759763517458\n",
            "************************************************************** 25\n",
            "what is the tr accuracy:  0.8853333333333333\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8854625550660793\n",
            "what is the loss  0.47959290474404054\n",
            "what is the loss v  0.47765049962120565\n",
            "what is the loss test 0.5007759763517458\n",
            "************************************************************** 26\n",
            "what is the tr accuracy:  0.8796\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.5036337084391886\n",
            "what is the loss v  0.5221319562964828\n",
            "what is the loss test 0.5311184311913444\n",
            "************************************************************** 26\n",
            "what is the tr accuracy:  0.8796\n",
            "what is the v accuracy:  0.878\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.5036337084391886\n",
            "what is the loss v  0.5221319562964828\n",
            "what is the loss test 0.5311184311913444\n",
            "************************************************************** 27\n",
            "what is the tr accuracy:  0.8824666666666666\n",
            "what is the v accuracy:  0.874\n",
            "what is the t accuracy:  0.8832599118942731\n",
            "what is the loss  0.4762713911992126\n",
            "what is the loss v  0.4821816336864848\n",
            "what is the loss test 0.5109457015852558\n",
            "************************************************************** 27\n",
            "what is the tr accuracy:  0.8824666666666666\n",
            "what is the v accuracy:  0.874\n",
            "what is the t accuracy:  0.8832599118942731\n",
            "what is the loss  0.4762713911992126\n",
            "what is the loss v  0.4821816336864848\n",
            "what is the loss test 0.5109457015852558\n",
            "************************************************************** 28\n",
            "what is the tr accuracy:  0.8764\n",
            "what is the v accuracy:  0.865\n",
            "what is the t accuracy:  0.8777533039647577\n",
            "what is the loss  0.4880832680946689\n",
            "what is the loss v  0.4956101844638425\n",
            "what is the loss test 0.5245254778855235\n",
            "************************************************************** 28\n",
            "what is the tr accuracy:  0.8764\n",
            "what is the v accuracy:  0.865\n",
            "what is the t accuracy:  0.8777533039647577\n",
            "what is the loss  0.4880832680946689\n",
            "what is the loss v  0.4956101844638425\n",
            "what is the loss test 0.5245254778855235\n",
            "************************************************************** 29\n",
            "what is the tr accuracy:  0.8895333333333333\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.43277821941773104\n",
            "what is the loss v  0.446902383760994\n",
            "what is the loss test 0.4768737574117898\n",
            "************************************************************** 29\n",
            "what is the tr accuracy:  0.8895333333333333\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8836270190895742\n",
            "what is the loss  0.43277821941773104\n",
            "what is the loss v  0.446902383760994\n",
            "what is the loss test 0.4768737574117898\n",
            "************************************************************** 30\n",
            "what is the tr accuracy:  0.8832\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8755506607929515\n",
            "what is the loss  0.45459253355367657\n",
            "what is the loss v  0.4602206952771131\n",
            "what is the loss test 0.5044927719371988\n",
            "************************************************************** 30\n",
            "what is the tr accuracy:  0.8832\n",
            "what is the v accuracy:  0.882\n",
            "what is the t accuracy:  0.8755506607929515\n",
            "what is the loss  0.45459253355367657\n",
            "what is the loss v  0.4602206952771131\n",
            "what is the loss test 0.5044927719371988\n",
            "************************************************************** 31\n",
            "what is the tr accuracy:  0.8848\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8766519823788547\n",
            "what is the loss  0.43876991881736105\n",
            "what is the loss v  0.4371515299791002\n",
            "what is the loss test 0.48170895619560195\n",
            "************************************************************** 31\n",
            "what is the tr accuracy:  0.8848\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8766519823788547\n",
            "what is the loss  0.43876991881736105\n",
            "what is the loss v  0.4371515299791002\n",
            "what is the loss test 0.48170895619560195\n",
            "************************************************************** 32\n",
            "what is the tr accuracy:  0.8662666666666666\n",
            "what is the v accuracy:  0.865\n",
            "what is the t accuracy:  0.8616005873715125\n",
            "what is the loss  0.4473141364944721\n",
            "what is the loss v  0.44686021164629564\n",
            "what is the loss test 0.4884660144375816\n",
            "************************************************************** 32\n",
            "what is the tr accuracy:  0.8662666666666666\n",
            "what is the v accuracy:  0.865\n",
            "what is the t accuracy:  0.8616005873715125\n",
            "what is the loss  0.4473141364944721\n",
            "what is the loss v  0.44686021164629564\n",
            "what is the loss test 0.4884660144375816\n",
            "************************************************************** 33\n",
            "what is the tr accuracy:  0.8906666666666667\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8781204111600588\n",
            "what is the loss  0.3947529560555689\n",
            "what is the loss v  0.39714358061524063\n",
            "what is the loss test 0.44857414254153527\n",
            "************************************************************** 33\n",
            "what is the tr accuracy:  0.8906666666666667\n",
            "what is the v accuracy:  0.892\n",
            "what is the t accuracy:  0.8781204111600588\n",
            "what is the loss  0.3947529560555689\n",
            "what is the loss v  0.39714358061524063\n",
            "what is the loss test 0.44857414254153527\n",
            "************************************************************** 34\n",
            "what is the tr accuracy:  0.8886666666666667\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8803230543318649\n",
            "what is the loss  0.39925204751644655\n",
            "what is the loss v  0.41633908714424006\n",
            "what is the loss test 0.4551928888665995\n",
            "************************************************************** 34\n",
            "what is the tr accuracy:  0.8886666666666667\n",
            "what is the v accuracy:  0.885\n",
            "what is the t accuracy:  0.8803230543318649\n",
            "what is the loss  0.39925204751644655\n",
            "what is the loss v  0.41633908714424006\n",
            "what is the loss test 0.4551928888665995\n",
            "************************************************************** 35\n",
            "what is the tr accuracy:  0.8890666666666667\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.4001575872258182\n",
            "what is the loss v  0.43105402403081805\n",
            "what is the loss test 0.4532802964716356\n",
            "************************************************************** 35\n",
            "what is the tr accuracy:  0.8890666666666667\n",
            "what is the v accuracy:  0.883\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.4001575872258182\n",
            "what is the loss v  0.43105402403081805\n",
            "what is the loss test 0.4532802964716356\n",
            "************************************************************** 36\n",
            "what is the tr accuracy:  0.8954\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.37000872571790905\n",
            "what is the loss v  0.39641594390346724\n",
            "what is the loss test 0.42639480373321337\n",
            "************************************************************** 36\n",
            "what is the tr accuracy:  0.8954\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.8872980910425844\n",
            "what is the loss  0.37000872571790905\n",
            "what is the loss v  0.39641594390346724\n",
            "what is the loss test 0.42639480373321337\n",
            "************************************************************** 37\n",
            "what is the tr accuracy:  0.8945333333333333\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.8839941262848752\n",
            "what is the loss  0.3622214060161042\n",
            "what is the loss v  0.3764120815290454\n",
            "what is the loss test 0.42592432220445137\n",
            "************************************************************** 37\n",
            "what is the tr accuracy:  0.8945333333333333\n",
            "what is the v accuracy:  0.884\n",
            "what is the t accuracy:  0.8839941262848752\n",
            "what is the loss  0.3622214060161042\n",
            "what is the loss v  0.3764120815290454\n",
            "what is the loss test 0.42592432220445137\n",
            "************************************************************** 38\n",
            "what is the tr accuracy:  0.8936\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.3621855717980487\n",
            "what is the loss v  0.36310644315402624\n",
            "what is the loss test 0.4301578918842505\n",
            "************************************************************** 38\n",
            "what is the tr accuracy:  0.8936\n",
            "what is the v accuracy:  0.891\n",
            "what is the t accuracy:  0.881791483113069\n",
            "what is the loss  0.3621855717980487\n",
            "what is the loss v  0.36310644315402624\n",
            "what is the loss test 0.4301578918842505\n",
            "************************************************************** 39\n",
            "what is the tr accuracy:  0.8996\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.34614595492508526\n",
            "what is the loss v  0.34297548201846223\n",
            "what is the loss test 0.41525024698742635\n",
            "************************************************************** 39\n",
            "what is the tr accuracy:  0.8996\n",
            "what is the v accuracy:  0.898\n",
            "what is the t accuracy:  0.8847283406754772\n",
            "what is the loss  0.34614595492508526\n",
            "what is the loss v  0.34297548201846223\n",
            "what is the loss test 0.41525024698742635\n",
            "************************************************************** 40\n",
            "what is the tr accuracy:  0.9060666666666667\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.34265952492923535\n",
            "what is the loss v  0.3444026303481353\n",
            "what is the loss test 0.41126045951416335\n",
            "************************************************************** 40\n",
            "what is the tr accuracy:  0.9060666666666667\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.8909691629955947\n",
            "what is the loss  0.34265952492923535\n",
            "what is the loss v  0.3444026303481353\n",
            "what is the loss test 0.41126045951416335\n",
            "************************************************************** 41\n",
            "what is the tr accuracy:  0.9084\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.893906020558003\n",
            "what is the loss  0.3335895255889951\n",
            "what is the loss v  0.3431711440147181\n",
            "what is the loss test 0.40393527331102985\n",
            "************************************************************** 41\n",
            "what is the tr accuracy:  0.9084\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.893906020558003\n",
            "what is the loss  0.3335895255889951\n",
            "what is the loss v  0.3431711440147181\n",
            "what is the loss test 0.40393527331102985\n",
            "************************************************************** 42\n",
            "what is the tr accuracy:  0.9086\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.32256174929533155\n",
            "what is the loss v  0.33952518511133417\n",
            "what is the loss test 0.39736561466721937\n",
            "************************************************************** 42\n",
            "what is the tr accuracy:  0.9086\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8917033773861968\n",
            "what is the loss  0.32256174929533155\n",
            "what is the loss v  0.33952518511133417\n",
            "what is the loss test 0.39736561466721937\n",
            "************************************************************** 43\n",
            "what is the tr accuracy:  0.9063333333333333\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.3277326512755273\n",
            "what is the loss v  0.35671022304215555\n",
            "what is the loss test 0.4059808884741247\n",
            "************************************************************** 43\n",
            "what is the tr accuracy:  0.9063333333333333\n",
            "what is the v accuracy:  0.899\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.3277326512755273\n",
            "what is the loss v  0.35671022304215555\n",
            "what is the loss test 0.4059808884741247\n",
            "************************************************************** 44\n",
            "what is the tr accuracy:  0.9058666666666667\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.3248720766393423\n",
            "what is the loss v  0.36848911318398064\n",
            "what is the loss test 0.4060253740301341\n",
            "************************************************************** 44\n",
            "what is the tr accuracy:  0.9058666666666667\n",
            "what is the v accuracy:  0.896\n",
            "what is the t accuracy:  0.895007342143906\n",
            "what is the loss  0.3248720766393423\n",
            "what is the loss v  0.36848911318398064\n",
            "what is the loss test 0.4060253740301341\n",
            "************************************************************** 45\n",
            "what is the tr accuracy:  0.9075333333333333\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8986784140969163\n",
            "what is the loss  0.31770183306183497\n",
            "what is the loss v  0.3700971988305573\n",
            "what is the loss test 0.3994425400537413\n",
            "************************************************************** 45\n",
            "what is the tr accuracy:  0.9075333333333333\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.8986784140969163\n",
            "what is the loss  0.31770183306183497\n",
            "what is the loss v  0.3700971988305573\n",
            "what is the loss test 0.3994425400537413\n",
            "************************************************************** 46\n",
            "what is the tr accuracy:  0.9086666666666666\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.3143064253961547\n",
            "what is the loss v  0.36631708536775115\n",
            "what is the loss test 0.39412046161317194\n",
            "************************************************************** 46\n",
            "what is the tr accuracy:  0.9086666666666666\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8997797356828194\n",
            "what is the loss  0.3143064253961547\n",
            "what is the loss v  0.36631708536775115\n",
            "what is the loss test 0.39412046161317194\n",
            "************************************************************** 47\n",
            "what is the tr accuracy:  0.9120666666666667\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.30299821781457453\n",
            "what is the loss v  0.34736474401283846\n",
            "what is the loss test 0.38161217810188586\n",
            "************************************************************** 47\n",
            "what is the tr accuracy:  0.9120666666666667\n",
            "what is the v accuracy:  0.901\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.30299821781457453\n",
            "what is the loss v  0.34736474401283846\n",
            "what is the loss test 0.38161217810188586\n",
            "************************************************************** 48\n",
            "what is the tr accuracy:  0.9129333333333334\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8990455212922174\n",
            "what is the loss  0.2962243348215108\n",
            "what is the loss v  0.33197328206769866\n",
            "what is the loss test 0.37475281749237777\n",
            "************************************************************** 48\n",
            "what is the tr accuracy:  0.9129333333333334\n",
            "what is the v accuracy:  0.903\n",
            "what is the t accuracy:  0.8990455212922174\n",
            "what is the loss  0.2962243348215108\n",
            "what is the loss v  0.33197328206769866\n",
            "what is the loss test 0.37475281749237777\n",
            "************************************************************** 49\n",
            "what is the tr accuracy:  0.9121333333333334\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.9005139500734214\n",
            "what is the loss  0.29930908739255196\n",
            "what is the loss v  0.32832584435109397\n",
            "what is the loss test 0.3793217096074649\n",
            "************************************************************** 49\n",
            "what is the tr accuracy:  0.9121333333333334\n",
            "what is the v accuracy:  0.9\n",
            "what is the t accuracy:  0.9005139500734214\n",
            "what is the loss  0.29930908739255196\n",
            "what is the loss v  0.32832584435109397\n",
            "what is the loss test 0.3793217096074649\n",
            "************************************************************** 50\n",
            "what is the tr accuracy:  0.9105333333333333\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.2999238638903338\n",
            "what is the loss v  0.32636738251309383\n",
            "what is the loss test 0.3824723998807802\n",
            "************************************************************** 50\n",
            "what is the tr accuracy:  0.9105333333333333\n",
            "what is the v accuracy:  0.904\n",
            "what is the t accuracy:  0.8983113069016153\n",
            "what is the loss  0.2999238638903338\n",
            "what is the loss v  0.32636738251309383\n",
            "what is the loss test 0.3824723998807802\n",
            "************************************************************** 51\n",
            "what is the tr accuracy:  0.9145333333333333\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.8979441997063142\n",
            "what is the loss  0.2877543386100406\n",
            "what is the loss v  0.31518021486023895\n",
            "what is the loss test 0.37338702478202457\n",
            "************************************************************** 51\n",
            "what is the tr accuracy:  0.9145333333333333\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.8979441997063142\n",
            "what is the loss  0.2877543386100406\n",
            "what is the loss v  0.31518021486023895\n",
            "what is the loss test 0.37338702478202457\n",
            "************************************************************** 52\n",
            "what is the tr accuracy:  0.9202666666666667\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.2718699713195227\n",
            "what is the loss v  0.30401230056655415\n",
            "what is the loss test 0.35990879309291707\n",
            "************************************************************** 52\n",
            "what is the tr accuracy:  0.9202666666666667\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.2718699713195227\n",
            "what is the loss v  0.30401230056655415\n",
            "what is the loss test 0.35990879309291707\n",
            "************************************************************** 53\n",
            "what is the tr accuracy:  0.9226666666666666\n",
            "what is the v accuracy:  0.91\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.26656056611813495\n",
            "what is the loss v  0.3073725513948351\n",
            "what is the loss test 0.35577400503208173\n",
            "************************************************************** 53\n",
            "what is the tr accuracy:  0.9226666666666666\n",
            "what is the v accuracy:  0.91\n",
            "what is the t accuracy:  0.9071218795888399\n",
            "what is the loss  0.26656056611813495\n",
            "what is the loss v  0.3073725513948351\n",
            "what is the loss test 0.35577400503208173\n",
            "************************************************************** 54\n",
            "what is the tr accuracy:  0.9229333333333334\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.9063876651982379\n",
            "what is the loss  0.271276944880634\n",
            "what is the loss v  0.3229664771707473\n",
            "what is the loss test 0.3618747730188544\n",
            "************************************************************** 54\n",
            "what is the tr accuracy:  0.9229333333333334\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.9063876651982379\n",
            "what is the loss  0.271276944880634\n",
            "what is the loss v  0.3229664771707473\n",
            "what is the loss test 0.3618747730188544\n",
            "************************************************************** 55\n",
            "what is the tr accuracy:  0.9226666666666666\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.2652463174049224\n",
            "what is the loss v  0.3266398816009558\n",
            "what is the loss test 0.36005847199425745\n",
            "************************************************************** 55\n",
            "what is the tr accuracy:  0.9226666666666666\n",
            "what is the v accuracy:  0.913\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.2652463174049224\n",
            "what is the loss v  0.3266398816009558\n",
            "what is the loss test 0.36005847199425745\n",
            "************************************************************** 56\n",
            "what is the tr accuracy:  0.9215333333333333\n",
            "what is the v accuracy:  0.906\n",
            "what is the t accuracy:  0.9038179148311307\n",
            "what is the loss  0.2643971702315161\n",
            "what is the loss v  0.3357477209029308\n",
            "what is the loss test 0.36206690273356296\n",
            "************************************************************** 56\n",
            "what is the tr accuracy:  0.9215333333333333\n",
            "what is the v accuracy:  0.906\n",
            "what is the t accuracy:  0.9038179148311307\n",
            "what is the loss  0.2643971702315161\n",
            "what is the loss v  0.3357477209029308\n",
            "what is the loss test 0.36206690273356296\n",
            "************************************************************** 57\n",
            "what is the tr accuracy:  0.9195333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.9027165932452276\n",
            "what is the loss  0.264475792046049\n",
            "what is the loss v  0.3449937083908761\n",
            "what is the loss test 0.3631772248884027\n",
            "************************************************************** 57\n",
            "what is the tr accuracy:  0.9195333333333333\n",
            "what is the v accuracy:  0.902\n",
            "what is the t accuracy:  0.9027165932452276\n",
            "what is the loss  0.264475792046049\n",
            "what is the loss v  0.3449937083908761\n",
            "what is the loss test 0.3631772248884027\n",
            "************************************************************** 58\n",
            "what is the tr accuracy:  0.9228\n",
            "what is the v accuracy:  0.905\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.25701691274935334\n",
            "what is the loss v  0.3430901275449159\n",
            "what is the loss test 0.357553123746919\n",
            "************************************************************** 58\n",
            "what is the tr accuracy:  0.9228\n",
            "what is the v accuracy:  0.905\n",
            "what is the t accuracy:  0.9034508076358296\n",
            "what is the loss  0.25701691274935334\n",
            "what is the loss v  0.3430901275449159\n",
            "what is the loss test 0.357553123746919\n",
            "************************************************************** 59\n",
            "what is the tr accuracy:  0.9266\n",
            "what is the v accuracy:  0.907\n",
            "what is the t accuracy:  0.9041850220264317\n",
            "what is the loss  0.24877004731762378\n",
            "what is the loss v  0.33712919677309194\n",
            "what is the loss test 0.352814418806089\n",
            "************************************************************** 59\n",
            "what is the tr accuracy:  0.9266\n",
            "what is the v accuracy:  0.907\n",
            "what is the t accuracy:  0.9041850220264317\n",
            "what is the loss  0.24877004731762378\n",
            "what is the loss v  0.33712919677309194\n",
            "what is the loss test 0.352814418806089\n",
            "************************************************************** 60\n",
            "what is the tr accuracy:  0.9282666666666667\n",
            "what is the v accuracy:  0.908\n",
            "what is the t accuracy:  0.906754772393539\n",
            "what is the loss  0.24677434363856354\n",
            "what is the loss v  0.334625091219782\n",
            "what is the loss test 0.3555277014700381\n",
            "************************************************************** 60\n",
            "what is the tr accuracy:  0.9282666666666667\n",
            "what is the v accuracy:  0.908\n",
            "what is the t accuracy:  0.906754772393539\n",
            "what is the loss  0.24677434363856354\n",
            "what is the loss v  0.334625091219782\n",
            "what is the loss test 0.3555277014700381\n",
            "************************************************************** 61\n",
            "what is the tr accuracy:  0.9289333333333334\n",
            "what is the v accuracy:  0.906\n",
            "what is the t accuracy:  0.9074889867841409\n",
            "what is the loss  0.24291369405861674\n",
            "what is the loss v  0.3256411655847015\n",
            "what is the loss test 0.35794721241297456\n",
            "************************************************************** 61\n",
            "what is the tr accuracy:  0.9289333333333334\n",
            "what is the v accuracy:  0.906\n",
            "what is the t accuracy:  0.9074889867841409\n",
            "what is the loss  0.24291369405861674\n",
            "what is the loss v  0.3256411655847015\n",
            "what is the loss test 0.35794721241297456\n",
            "************************************************************** 62\n",
            "what is the tr accuracy:  0.9304666666666667\n",
            "what is the v accuracy:  0.91\n",
            "what is the t accuracy:  0.9045521292217328\n",
            "what is the loss  0.2363936772429665\n",
            "what is the loss v  0.3140758993514992\n",
            "what is the loss test 0.3579109297068941\n",
            "************************************************************** 62\n",
            "what is the tr accuracy:  0.9304666666666667\n",
            "what is the v accuracy:  0.91\n",
            "what is the t accuracy:  0.9045521292217328\n",
            "what is the loss  0.2363936772429665\n",
            "what is the loss v  0.3140758993514992\n",
            "what is the loss test 0.3579109297068941\n",
            "************************************************************** 63\n",
            "what is the tr accuracy:  0.9315333333333333\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.2317399948544028\n",
            "what is the loss v  0.3076553423611192\n",
            "what is the loss test 0.3591017375465817\n",
            "************************************************************** 63\n",
            "what is the tr accuracy:  0.9315333333333333\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9085903083700441\n",
            "what is the loss  0.2317399948544028\n",
            "what is the loss v  0.3076553423611192\n",
            "what is the loss test 0.3591017375465817\n",
            "************************************************************** 64\n",
            "what is the tr accuracy:  0.932\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.2281078754786206\n",
            "what is the loss v  0.3064357098840449\n",
            "what is the loss test 0.3604847994260466\n",
            "************************************************************** 64\n",
            "what is the tr accuracy:  0.932\n",
            "what is the v accuracy:  0.918\n",
            "what is the t accuracy:  0.9056534508076358\n",
            "what is the loss  0.2281078754786206\n",
            "what is the loss v  0.3064357098840449\n",
            "what is the loss test 0.3604847994260466\n",
            "************************************************************** 65\n",
            "what is the tr accuracy:  0.9346\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.908223201174743\n",
            "what is the loss  0.22126319433026154\n",
            "what is the loss v  0.30530337116880396\n",
            "what is the loss test 0.35863667330292176\n",
            "************************************************************** 65\n",
            "what is the tr accuracy:  0.9346\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.908223201174743\n",
            "what is the loss  0.22126319433026154\n",
            "what is the loss v  0.30530337116880396\n",
            "what is the loss test 0.35863667330292176\n",
            "************************************************************** 66\n",
            "what is the tr accuracy:  0.9382\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.21318488497554167\n",
            "what is the loss v  0.3048945346984635\n",
            "what is the loss test 0.3553401849159659\n",
            "************************************************************** 66\n",
            "what is the tr accuracy:  0.9382\n",
            "what is the v accuracy:  0.92\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.21318488497554167\n",
            "what is the loss v  0.3048945346984635\n",
            "what is the loss test 0.3553401849159659\n",
            "************************************************************** 67\n",
            "what is the tr accuracy:  0.9378666666666666\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.21012912881287454\n",
            "what is the loss v  0.31147050004160737\n",
            "what is the loss test 0.35679032161518837\n",
            "************************************************************** 67\n",
            "what is the tr accuracy:  0.9378666666666666\n",
            "what is the v accuracy:  0.919\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.21012912881287454\n",
            "what is the loss v  0.31147050004160737\n",
            "what is the loss test 0.35679032161518837\n",
            "************************************************************** 68\n",
            "what is the tr accuracy:  0.936\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.21356308514523042\n",
            "what is the loss v  0.3256605945167829\n",
            "what is the loss test 0.3642975622056822\n",
            "************************************************************** 68\n",
            "what is the tr accuracy:  0.936\n",
            "what is the v accuracy:  0.917\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.21356308514523042\n",
            "what is the loss v  0.3256605945167829\n",
            "what is the loss test 0.3642975622056822\n",
            "************************************************************** 69\n",
            "what is the tr accuracy:  0.9352\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.21606636405939847\n",
            "what is the loss v  0.3378412692324041\n",
            "what is the loss test 0.3696917156957132\n",
            "************************************************************** 69\n",
            "what is the tr accuracy:  0.9352\n",
            "what is the v accuracy:  0.912\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.21606636405939847\n",
            "what is the loss v  0.3378412692324041\n",
            "what is the loss test 0.3696917156957132\n",
            "************************************************************** 70\n",
            "what is the tr accuracy:  0.9352\n",
            "what is the v accuracy:  0.911\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.21253452084973704\n",
            "what is the loss v  0.34110667625673674\n",
            "what is the loss test 0.368645087298186\n",
            "************************************************************** 70\n",
            "what is the tr accuracy:  0.9352\n",
            "what is the v accuracy:  0.911\n",
            "what is the t accuracy:  0.9100587371512482\n",
            "what is the loss  0.21253452084973704\n",
            "what is the loss v  0.34110667625673674\n",
            "what is the loss test 0.368645087298186\n",
            "************************************************************** 71\n",
            "what is the tr accuracy:  0.9368666666666666\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.20654878290846537\n",
            "what is the loss v  0.3383373446893778\n",
            "what is the loss test 0.36560712520881095\n",
            "************************************************************** 71\n",
            "what is the tr accuracy:  0.9368666666666666\n",
            "what is the v accuracy:  0.914\n",
            "what is the t accuracy:  0.9093245227606461\n",
            "what is the loss  0.20654878290846537\n",
            "what is the loss v  0.3383373446893778\n",
            "what is the loss test 0.36560712520881095\n",
            "************************************************************** 72\n",
            "what is the tr accuracy:  0.9388666666666666\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.20014563687061732\n",
            "what is the loss v  0.3315169665515616\n",
            "what is the loss test 0.36342470704317814\n",
            "************************************************************** 72\n",
            "what is the tr accuracy:  0.9388666666666666\n",
            "what is the v accuracy:  0.915\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.20014563687061732\n",
            "what is the loss v  0.3315169665515616\n",
            "what is the loss test 0.36342470704317814\n",
            "************************************************************** 73\n",
            "what is the tr accuracy:  0.9412666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.19488486741822228\n",
            "what is the loss v  0.3217669985709659\n",
            "what is the loss test 0.363295600176561\n",
            "************************************************************** 73\n",
            "what is the tr accuracy:  0.9412666666666667\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.19488486741822228\n",
            "what is the loss v  0.3217669985709659\n",
            "what is the loss test 0.363295600176561\n",
            "************************************************************** 74\n",
            "what is the tr accuracy:  0.9420666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.1918691583100388\n",
            "what is the loss v  0.3135359568569508\n",
            "what is the loss test 0.365504153787786\n",
            "************************************************************** 74\n",
            "what is the tr accuracy:  0.9420666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.1918691583100388\n",
            "what is the loss v  0.3135359568569508\n",
            "what is the loss test 0.365504153787786\n",
            "************************************************************** 75\n",
            "what is the tr accuracy:  0.9432\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.19090536378509854\n",
            "what is the loss v  0.30949919059013564\n",
            "what is the loss test 0.3689918545122039\n",
            "************************************************************** 75\n",
            "what is the tr accuracy:  0.9432\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.19090536378509854\n",
            "what is the loss v  0.30949919059013564\n",
            "what is the loss test 0.3689918545122039\n",
            "************************************************************** 76\n",
            "what is the tr accuracy:  0.943\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.19015876580903016\n",
            "what is the loss v  0.30895386067935865\n",
            "what is the loss test 0.37175763353133895\n",
            "************************************************************** 76\n",
            "what is the tr accuracy:  0.943\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.19015876580903016\n",
            "what is the loss v  0.30895386067935865\n",
            "what is the loss test 0.37175763353133895\n",
            "************************************************************** 77\n",
            "what is the tr accuracy:  0.9436666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.18681060894691714\n",
            "what is the loss v  0.3093756446461792\n",
            "what is the loss test 0.3708066457038451\n",
            "************************************************************** 77\n",
            "what is the tr accuracy:  0.9436666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.18681060894691714\n",
            "what is the loss v  0.3093756446461792\n",
            "what is the loss test 0.3708066457038451\n",
            "************************************************************** 78\n",
            "what is the tr accuracy:  0.9460666666666666\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.18039123967820644\n",
            "what is the loss v  0.3088308862248643\n",
            "what is the loss test 0.3657316793154392\n",
            "************************************************************** 78\n",
            "what is the tr accuracy:  0.9460666666666666\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.18039123967820644\n",
            "what is the loss v  0.3088308862248643\n",
            "what is the loss test 0.3657316793154392\n",
            "************************************************************** 79\n",
            "what is the tr accuracy:  0.9474666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.17379331316735105\n",
            "what is the loss v  0.30966234251014974\n",
            "what is the loss test 0.35977020856183395\n",
            "************************************************************** 79\n",
            "what is the tr accuracy:  0.9474666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.17379331316735105\n",
            "what is the loss v  0.30966234251014974\n",
            "what is the loss test 0.35977020856183395\n",
            "************************************************************** 80\n",
            "what is the tr accuracy:  0.9484\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9144640234948604\n",
            "what is the loss  0.1699072445071711\n",
            "what is the loss v  0.3142921632640675\n",
            "what is the loss test 0.3562214143211482\n",
            "************************************************************** 80\n",
            "what is the tr accuracy:  0.9484\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9144640234948604\n",
            "what is the loss  0.1699072445071711\n",
            "what is the loss v  0.3142921632640675\n",
            "what is the loss test 0.3562214143211482\n",
            "************************************************************** 81\n",
            "what is the tr accuracy:  0.949\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.16861769874163463\n",
            "what is the loss v  0.32280380837314043\n",
            "what is the loss test 0.3564303393153677\n",
            "************************************************************** 81\n",
            "what is the tr accuracy:  0.949\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9129955947136564\n",
            "what is the loss  0.16861769874163463\n",
            "what is the loss v  0.32280380837314043\n",
            "what is the loss test 0.3564303393153677\n",
            "************************************************************** 82\n",
            "what is the tr accuracy:  0.9485333333333333\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.1678727473497818\n",
            "what is the loss v  0.3319038112832234\n",
            "what is the loss test 0.3588119424275729\n",
            "************************************************************** 82\n",
            "what is the tr accuracy:  0.9485333333333333\n",
            "what is the v accuracy:  0.921\n",
            "what is the t accuracy:  0.9115271659324523\n",
            "what is the loss  0.1678727473497818\n",
            "what is the loss v  0.3319038112832234\n",
            "what is the loss test 0.3588119424275729\n",
            "************************************************************** 83\n",
            "what is the tr accuracy:  0.9496\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.1657494791033939\n",
            "what is the loss v  0.3381367498629215\n",
            "what is the loss test 0.3608521475278808\n",
            "************************************************************** 83\n",
            "what is the tr accuracy:  0.9496\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.1657494791033939\n",
            "what is the loss v  0.3381367498629215\n",
            "what is the loss test 0.3608521475278808\n",
            "************************************************************** 84\n",
            "what is the tr accuracy:  0.9512\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.1617543552857978\n",
            "what is the loss v  0.33999023745153173\n",
            "what is the loss test 0.3616884484815594\n",
            "************************************************************** 84\n",
            "what is the tr accuracy:  0.9512\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.1617543552857978\n",
            "what is the loss v  0.33999023745153173\n",
            "what is the loss test 0.3616884484815594\n",
            "************************************************************** 85\n",
            "what is the tr accuracy:  0.9525333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.15661763050768768\n",
            "what is the loss v  0.33810113621351306\n",
            "what is the loss test 0.36174267960080586\n",
            "************************************************************** 85\n",
            "what is the tr accuracy:  0.9525333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.15661763050768768\n",
            "what is the loss v  0.33810113621351306\n",
            "what is the loss test 0.36174267960080586\n",
            "************************************************************** 86\n",
            "what is the tr accuracy:  0.9529333333333333\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.1517638469373062\n",
            "what is the loss v  0.3338615685951925\n",
            "what is the loss test 0.3620815782348099\n",
            "************************************************************** 86\n",
            "what is the tr accuracy:  0.9529333333333333\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.1517638469373062\n",
            "what is the loss v  0.3338615685951925\n",
            "what is the loss test 0.3620815782348099\n",
            "************************************************************** 87\n",
            "what is the tr accuracy:  0.9540666666666666\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.14834455219909118\n",
            "what is the loss v  0.329765562970608\n",
            "what is the loss test 0.36419349641101223\n",
            "************************************************************** 87\n",
            "what is the tr accuracy:  0.9540666666666666\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9118942731277533\n",
            "what is the loss  0.14834455219909118\n",
            "what is the loss v  0.329765562970608\n",
            "what is the loss test 0.36419349641101223\n",
            "************************************************************** 88\n",
            "what is the tr accuracy:  0.9537333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.14660440450533258\n",
            "what is the loss v  0.3271724924642172\n",
            "what is the loss test 0.3677402721233185\n",
            "************************************************************** 88\n",
            "what is the tr accuracy:  0.9537333333333333\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9107929515418502\n",
            "what is the loss  0.14660440450533258\n",
            "what is the loss v  0.3271724924642172\n",
            "what is the loss test 0.3677402721233185\n",
            "************************************************************** 89\n",
            "what is the tr accuracy:  0.955\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.14562088926755334\n",
            "what is the loss v  0.3249878535451313\n",
            "what is the loss test 0.37111791131944133\n",
            "************************************************************** 89\n",
            "what is the tr accuracy:  0.955\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.14562088926755334\n",
            "what is the loss v  0.3249878535451313\n",
            "what is the loss test 0.37111791131944133\n",
            "************************************************************** 90\n",
            "what is the tr accuracy:  0.9556666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.14393686011768014\n",
            "what is the loss v  0.32211533455986796\n",
            "what is the loss test 0.3726985915565078\n",
            "************************************************************** 90\n",
            "what is the tr accuracy:  0.9556666666666667\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.14393686011768014\n",
            "what is the loss v  0.32211533455986796\n",
            "what is the loss test 0.3726985915565078\n",
            "************************************************************** 91\n",
            "what is the tr accuracy:  0.9570666666666666\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.14108890290242987\n",
            "what is the loss v  0.3184146342467068\n",
            "what is the loss test 0.37203043429720106\n",
            "************************************************************** 91\n",
            "what is the tr accuracy:  0.9570666666666666\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9126284875183553\n",
            "what is the loss  0.14108890290242987\n",
            "what is the loss v  0.3184146342467068\n",
            "what is the loss test 0.37203043429720106\n",
            "************************************************************** 92\n",
            "what is the tr accuracy:  0.9582\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.13762594932459749\n",
            "what is the loss v  0.3147293150258744\n",
            "what is the loss test 0.37016942354344956\n",
            "************************************************************** 92\n",
            "what is the tr accuracy:  0.9582\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9122613803230544\n",
            "what is the loss  0.13762594932459749\n",
            "what is the loss v  0.3147293150258744\n",
            "what is the loss test 0.37016942354344956\n",
            "************************************************************** 93\n",
            "what is the tr accuracy:  0.9594\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.134329333370293\n",
            "what is the loss v  0.3125816715163233\n",
            "what is the loss test 0.3684071388875558\n",
            "************************************************************** 93\n",
            "what is the tr accuracy:  0.9594\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.134329333370293\n",
            "what is the loss v  0.3125816715163233\n",
            "what is the loss test 0.3684071388875558\n",
            "************************************************************** 94\n",
            "what is the tr accuracy:  0.9603333333333334\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.13140800589794924\n",
            "what is the loss v  0.31250248743669595\n",
            "what is the loss test 0.3675105517289129\n",
            "************************************************************** 94\n",
            "what is the tr accuracy:  0.9603333333333334\n",
            "what is the v accuracy:  0.924\n",
            "what is the t accuracy:  0.9133627019089574\n",
            "what is the loss  0.13140800589794924\n",
            "what is the loss v  0.31250248743669595\n",
            "what is the loss test 0.3675105517289129\n",
            "************************************************************** 95\n",
            "what is the tr accuracy:  0.9610666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.1285076294066616\n",
            "what is the loss v  0.31412753828563733\n",
            "what is the loss test 0.3674749040814315\n",
            "************************************************************** 95\n",
            "what is the tr accuracy:  0.9610666666666666\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.1285076294066616\n",
            "what is the loss v  0.31412753828563733\n",
            "what is the loss test 0.3674749040814315\n",
            "************************************************************** 96\n",
            "what is the tr accuracy:  0.9609333333333333\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.12527610136468006\n",
            "what is the loss v  0.3163453967824814\n",
            "what is the loss test 0.3682051991022785\n",
            "************************************************************** 96\n",
            "what is the tr accuracy:  0.9609333333333333\n",
            "what is the v accuracy:  0.922\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.12527610136468006\n",
            "what is the loss v  0.3163453967824814\n",
            "what is the loss test 0.3682051991022785\n",
            "************************************************************** 97\n",
            "what is the tr accuracy:  0.9624666666666667\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.12146305652359679\n",
            "what is the loss v  0.31856500957244\n",
            "what is the loss test 0.36959698224155507\n",
            "************************************************************** 97\n",
            "what is the tr accuracy:  0.9624666666666667\n",
            "what is the v accuracy:  0.923\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.12146305652359679\n",
            "what is the loss v  0.31856500957244\n",
            "what is the loss test 0.36959698224155507\n",
            "************************************************************** 98\n",
            "what is the tr accuracy:  0.9632666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.11743597649519243\n",
            "what is the loss v  0.3206574750722999\n",
            "what is the loss test 0.37147594740840983\n",
            "************************************************************** 98\n",
            "what is the tr accuracy:  0.9632666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.11743597649519243\n",
            "what is the loss v  0.3206574750722999\n",
            "what is the loss test 0.37147594740840983\n",
            "************************************************************** 99\n",
            "what is the tr accuracy:  0.9653333333333334\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.11376536525609934\n",
            "what is the loss v  0.323201021747362\n",
            "what is the loss test 0.3742108483360165\n",
            "************************************************************** 99\n",
            "what is the tr accuracy:  0.9653333333333334\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.11376536525609934\n",
            "what is the loss v  0.323201021747362\n",
            "what is the loss test 0.3742108483360165\n",
            "************************************************************** 100\n",
            "what is the tr accuracy:  0.9664\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.11097575866430698\n",
            "what is the loss v  0.32661762873751415\n",
            "what is the loss test 0.3778988143109446\n",
            "************************************************************** 100\n",
            "what is the tr accuracy:  0.9664\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.11097575866430698\n",
            "what is the loss v  0.32661762873751415\n",
            "what is the loss test 0.3778988143109446\n",
            "************************************************************** 101\n",
            "what is the tr accuracy:  0.9660666666666666\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.1091677368384046\n",
            "what is the loss v  0.330942220408917\n",
            "what is the loss test 0.382217535980521\n",
            "************************************************************** 101\n",
            "what is the tr accuracy:  0.9660666666666666\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.1091677368384046\n",
            "what is the loss v  0.330942220408917\n",
            "what is the loss test 0.382217535980521\n",
            "************************************************************** 102\n",
            "what is the tr accuracy:  0.9672\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9151982378854625\n",
            "what is the loss  0.10774832209595533\n",
            "what is the loss v  0.33525395654551743\n",
            "what is the loss test 0.38646878084016917\n",
            "************************************************************** 102\n",
            "what is the tr accuracy:  0.9672\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9151982378854625\n",
            "what is the loss  0.10774832209595533\n",
            "what is the loss v  0.33525395654551743\n",
            "what is the loss test 0.38646878084016917\n",
            "************************************************************** 103\n",
            "what is the tr accuracy:  0.9669333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.1059546372344326\n",
            "what is the loss v  0.3384734582524789\n",
            "what is the loss test 0.3896695081098097\n",
            "************************************************************** 103\n",
            "what is the tr accuracy:  0.9669333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.1059546372344326\n",
            "what is the loss v  0.3384734582524789\n",
            "what is the loss test 0.3896695081098097\n",
            "************************************************************** 104\n",
            "what is the tr accuracy:  0.9678666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.10338685652778136\n",
            "what is the loss v  0.3401833903514976\n",
            "what is the loss test 0.39135548003921955\n",
            "************************************************************** 104\n",
            "what is the tr accuracy:  0.9678666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.10338685652778136\n",
            "what is the loss v  0.3401833903514976\n",
            "what is the loss test 0.39135548003921955\n",
            "************************************************************** 105\n",
            "what is the tr accuracy:  0.9691333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.10036777015200059\n",
            "what is the loss v  0.340757788886052\n",
            "what is the loss test 0.39192804895013444\n",
            "************************************************************** 105\n",
            "what is the tr accuracy:  0.9691333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.10036777015200059\n",
            "what is the loss v  0.340757788886052\n",
            "what is the loss test 0.39192804895013444\n",
            "************************************************************** 106\n",
            "what is the tr accuracy:  0.9698666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.09745329935581601\n",
            "what is the loss v  0.34084070643896747\n",
            "what is the loss test 0.39219117350283\n",
            "************************************************************** 106\n",
            "what is the tr accuracy:  0.9698666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.09745329935581601\n",
            "what is the loss v  0.34084070643896747\n",
            "what is the loss test 0.39219117350283\n",
            "************************************************************** 107\n",
            "what is the tr accuracy:  0.9712666666666666\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9144640234948604\n",
            "what is the loss  0.09502753162934041\n",
            "what is the loss v  0.3403687236557495\n",
            "what is the loss test 0.3926981436276523\n",
            "************************************************************** 107\n",
            "what is the tr accuracy:  0.9712666666666666\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9144640234948604\n",
            "what is the loss  0.09502753162934041\n",
            "what is the loss v  0.3403687236557495\n",
            "what is the loss test 0.3926981436276523\n",
            "************************************************************** 108\n",
            "what is the tr accuracy:  0.9721333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.0932503404352787\n",
            "what is the loss v  0.339727696722731\n",
            "what is the loss test 0.39376638853604806\n",
            "************************************************************** 108\n",
            "what is the tr accuracy:  0.9721333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.0932503404352787\n",
            "what is the loss v  0.339727696722731\n",
            "what is the loss test 0.39376638853604806\n",
            "************************************************************** 109\n",
            "what is the tr accuracy:  0.9726\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.09167870851032021\n",
            "what is the loss v  0.33942129470071464\n",
            "what is the loss test 0.39523238437683056\n",
            "************************************************************** 109\n",
            "what is the tr accuracy:  0.9726\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9140969162995595\n",
            "what is the loss  0.09167870851032021\n",
            "what is the loss v  0.33942129470071464\n",
            "what is the loss test 0.39523238437683056\n",
            "************************************************************** 110\n",
            "what is the tr accuracy:  0.9729333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.08952268619255277\n",
            "what is the loss v  0.3393327885629776\n",
            "what is the loss test 0.39646506166081064\n",
            "************************************************************** 110\n",
            "what is the tr accuracy:  0.9729333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9137298091042585\n",
            "what is the loss  0.08952268619255277\n",
            "what is the loss v  0.3393327885629776\n",
            "what is the loss test 0.39646506166081064\n",
            "************************************************************** 111\n",
            "what is the tr accuracy:  0.9738666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.08648260850518012\n",
            "what is the loss v  0.3400087703161767\n",
            "what is the loss test 0.39747874327640076\n",
            "************************************************************** 111\n",
            "what is the tr accuracy:  0.9738666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.08648260850518012\n",
            "what is the loss v  0.3400087703161767\n",
            "what is the loss test 0.39747874327640076\n",
            "************************************************************** 112\n",
            "what is the tr accuracy:  0.9747333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.08321299933203069\n",
            "what is the loss v  0.34268163636531224\n",
            "what is the loss test 0.3990600884477397\n",
            "************************************************************** 112\n",
            "what is the tr accuracy:  0.9747333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.08321299933203069\n",
            "what is the loss v  0.34268163636531224\n",
            "what is the loss test 0.3990600884477397\n",
            "************************************************************** 113\n",
            "what is the tr accuracy:  0.9758\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.08036014583918866\n",
            "what is the loss v  0.34790721036076877\n",
            "what is the loss test 0.401991168010219\n",
            "************************************************************** 113\n",
            "what is the tr accuracy:  0.9758\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.08036014583918866\n",
            "what is the loss v  0.34790721036076877\n",
            "what is the loss test 0.401991168010219\n",
            "************************************************************** 114\n",
            "what is the tr accuracy:  0.976\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.07803929960001126\n",
            "what is the loss v  0.3549697930460669\n",
            "what is the loss test 0.4059488945667985\n",
            "************************************************************** 114\n",
            "what is the tr accuracy:  0.976\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.07803929960001126\n",
            "what is the loss v  0.3549697930460669\n",
            "what is the loss test 0.4059488945667985\n",
            "************************************************************** 115\n",
            "what is the tr accuracy:  0.9768\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.0761149950651921\n",
            "what is the loss v  0.36240014844798135\n",
            "what is the loss test 0.410424550741587\n",
            "************************************************************** 115\n",
            "what is the tr accuracy:  0.9768\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.0761149950651921\n",
            "what is the loss v  0.36240014844798135\n",
            "what is the loss test 0.410424550741587\n",
            "************************************************************** 116\n",
            "what is the tr accuracy:  0.9766\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9185022026431718\n",
            "what is the loss  0.07440555354637529\n",
            "what is the loss v  0.369679230772606\n",
            "what is the loss test 0.4152667723100341\n",
            "************************************************************** 116\n",
            "what is the tr accuracy:  0.9766\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9185022026431718\n",
            "what is the loss  0.07440555354637529\n",
            "what is the loss v  0.369679230772606\n",
            "what is the loss test 0.4152667723100341\n",
            "************************************************************** 117\n",
            "what is the tr accuracy:  0.9769333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.07288857594909465\n",
            "what is the loss v  0.3759308444045769\n",
            "what is the loss test 0.41997420821772874\n",
            "************************************************************** 117\n",
            "what is the tr accuracy:  0.9769333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.07288857594909465\n",
            "what is the loss v  0.3759308444045769\n",
            "what is the loss test 0.41997420821772874\n",
            "************************************************************** 118\n",
            "what is the tr accuracy:  0.9779333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.07138478754826702\n",
            "what is the loss v  0.38011526806423357\n",
            "what is the loss test 0.4235924609164672\n",
            "************************************************************** 118\n",
            "what is the tr accuracy:  0.9779333333333333\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.07138478754826702\n",
            "what is the loss v  0.38011526806423357\n",
            "what is the loss test 0.4235924609164672\n",
            "************************************************************** 119\n",
            "what is the tr accuracy:  0.9793333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.06969642731048024\n",
            "what is the loss v  0.381798940590327\n",
            "what is the loss test 0.42563086940228634\n",
            "************************************************************** 119\n",
            "what is the tr accuracy:  0.9793333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.06969642731048024\n",
            "what is the loss v  0.381798940590327\n",
            "what is the loss test 0.42563086940228634\n",
            "************************************************************** 120\n",
            "what is the tr accuracy:  0.98\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.06769388416079819\n",
            "what is the loss v  0.3808580328241178\n",
            "what is the loss test 0.4258336944054021\n",
            "************************************************************** 120\n",
            "what is the tr accuracy:  0.98\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.06769388416079819\n",
            "what is the loss v  0.3808580328241178\n",
            "what is the loss test 0.4258336944054021\n",
            "************************************************************** 121\n",
            "what is the tr accuracy:  0.9811333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.06535132617007539\n",
            "what is the loss v  0.3773968331939683\n",
            "what is the loss test 0.4243922790883475\n",
            "************************************************************** 121\n",
            "what is the tr accuracy:  0.9811333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.06535132617007539\n",
            "what is the loss v  0.3773968331939683\n",
            "what is the loss test 0.4243922790883475\n",
            "************************************************************** 122\n",
            "what is the tr accuracy:  0.9812\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.06278120482193547\n",
            "what is the loss v  0.37245827983973906\n",
            "what is the loss test 0.4219221736347306\n",
            "************************************************************** 122\n",
            "what is the tr accuracy:  0.9812\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.06278120482193547\n",
            "what is the loss v  0.37245827983973906\n",
            "what is the loss test 0.4219221736347306\n",
            "************************************************************** 123\n",
            "what is the tr accuracy:  0.9823333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.06036603935269794\n",
            "what is the loss v  0.36742304914056184\n",
            "what is the loss test 0.41923479266917596\n",
            "************************************************************** 123\n",
            "what is the tr accuracy:  0.9823333333333333\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.06036603935269794\n",
            "what is the loss v  0.36742304914056184\n",
            "what is the loss test 0.41923479266917596\n",
            "************************************************************** 124\n",
            "what is the tr accuracy:  0.9824666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.05854573120914168\n",
            "what is the loss v  0.36354350513093786\n",
            "what is the loss test 0.41750131679389463\n",
            "************************************************************** 124\n",
            "what is the tr accuracy:  0.9824666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.05854573120914168\n",
            "what is the loss v  0.36354350513093786\n",
            "what is the loss test 0.41750131679389463\n",
            "************************************************************** 125\n",
            "what is the tr accuracy:  0.9832\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.05747319689494396\n",
            "what is the loss v  0.3616334964654789\n",
            "what is the loss test 0.4171786368670577\n",
            "************************************************************** 125\n",
            "what is the tr accuracy:  0.9832\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.05747319689494396\n",
            "what is the loss v  0.3616334964654789\n",
            "what is the loss test 0.4171786368670577\n",
            "************************************************************** 126\n",
            "what is the tr accuracy:  0.9828\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.056837510021693925\n",
            "what is the loss v  0.3615154932091719\n",
            "what is the loss test 0.4179248714542641\n",
            "************************************************************** 126\n",
            "what is the tr accuracy:  0.9828\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.056837510021693925\n",
            "what is the loss v  0.3615154932091719\n",
            "what is the loss test 0.4179248714542641\n",
            "************************************************************** 127\n",
            "what is the tr accuracy:  0.9829333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.05602964479584691\n",
            "what is the loss v  0.3628557692145299\n",
            "what is the loss test 0.4191201757890031\n",
            "************************************************************** 127\n",
            "what is the tr accuracy:  0.9829333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.05602964479584691\n",
            "what is the loss v  0.3628557692145299\n",
            "what is the loss test 0.4191201757890031\n",
            "************************************************************** 128\n",
            "what is the tr accuracy:  0.9834\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.05472072898081379\n",
            "what is the loss v  0.3654861232444233\n",
            "what is the loss test 0.42040570037210406\n",
            "************************************************************** 128\n",
            "what is the tr accuracy:  0.9834\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.05472072898081379\n",
            "what is the loss v  0.3654861232444233\n",
            "what is the loss test 0.42040570037210406\n",
            "************************************************************** 129\n",
            "what is the tr accuracy:  0.9841333333333333\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.0528741207088486\n",
            "what is the loss v  0.36895438023706467\n",
            "what is the loss test 0.4218150378256772\n",
            "************************************************************** 129\n",
            "what is the tr accuracy:  0.9841333333333333\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.0528741207088486\n",
            "what is the loss v  0.36895438023706467\n",
            "what is the loss test 0.4218150378256772\n",
            "************************************************************** 130\n",
            "what is the tr accuracy:  0.9848\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.05080373213604999\n",
            "what is the loss v  0.3734179361092738\n",
            "what is the loss test 0.42371727882722443\n",
            "************************************************************** 130\n",
            "what is the tr accuracy:  0.9848\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.05080373213604999\n",
            "what is the loss v  0.3734179361092738\n",
            "what is the loss test 0.42371727882722443\n",
            "************************************************************** 131\n",
            "what is the tr accuracy:  0.9853333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.0488598346171198\n",
            "what is the loss v  0.3787854683843709\n",
            "what is the loss test 0.4265234135522397\n",
            "************************************************************** 131\n",
            "what is the tr accuracy:  0.9853333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.0488598346171198\n",
            "what is the loss v  0.3787854683843709\n",
            "what is the loss test 0.4265234135522397\n",
            "************************************************************** 132\n",
            "what is the tr accuracy:  0.9859333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.047198462353231344\n",
            "what is the loss v  0.38509576410901186\n",
            "what is the loss test 0.4306773734311138\n",
            "************************************************************** 132\n",
            "what is the tr accuracy:  0.9859333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.047198462353231344\n",
            "what is the loss v  0.38509576410901186\n",
            "what is the loss test 0.4306773734311138\n",
            "************************************************************** 133\n",
            "what is the tr accuracy:  0.9866\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.04592783658445537\n",
            "what is the loss v  0.3918442644135144\n",
            "what is the loss test 0.4362016700728242\n",
            "************************************************************** 133\n",
            "what is the tr accuracy:  0.9866\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.04592783658445537\n",
            "what is the loss v  0.3918442644135144\n",
            "what is the loss test 0.4362016700728242\n",
            "************************************************************** 134\n",
            "what is the tr accuracy:  0.9865333333333334\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.04498493198015755\n",
            "what is the loss v  0.3989542924478636\n",
            "what is the loss test 0.44259001840005985\n",
            "************************************************************** 134\n",
            "what is the tr accuracy:  0.9865333333333334\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.04498493198015755\n",
            "what is the loss v  0.3989542924478636\n",
            "what is the loss test 0.44259001840005985\n",
            "************************************************************** 135\n",
            "what is the tr accuracy:  0.9864\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.04422438059570255\n",
            "what is the loss v  0.40610624938901624\n",
            "what is the loss test 0.4492542748519655\n",
            "************************************************************** 135\n",
            "what is the tr accuracy:  0.9864\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.04422438059570255\n",
            "what is the loss v  0.40610624938901624\n",
            "what is the loss test 0.4492542748519655\n",
            "************************************************************** 136\n",
            "what is the tr accuracy:  0.9864\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.04346987601643168\n",
            "what is the loss v  0.41231200800946544\n",
            "what is the loss test 0.4555675542622365\n",
            "************************************************************** 136\n",
            "what is the tr accuracy:  0.9864\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.04346987601643168\n",
            "what is the loss v  0.41231200800946544\n",
            "what is the loss test 0.4555675542622365\n",
            "************************************************************** 137\n",
            "what is the tr accuracy:  0.9864666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9151982378854625\n",
            "what is the loss  0.04254398945890653\n",
            "what is the loss v  0.41701658101350436\n",
            "what is the loss test 0.4610812441442827\n",
            "************************************************************** 137\n",
            "what is the tr accuracy:  0.9864666666666667\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9151982378854625\n",
            "what is the loss  0.04254398945890653\n",
            "what is the loss v  0.41701658101350436\n",
            "what is the loss test 0.4610812441442827\n",
            "************************************************************** 138\n",
            "what is the tr accuracy:  0.9870666666666666\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.04135676080462222\n",
            "what is the loss v  0.4203816425655574\n",
            "what is the loss test 0.46535879251261875\n",
            "************************************************************** 138\n",
            "what is the tr accuracy:  0.9870666666666666\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.04135676080462222\n",
            "what is the loss v  0.4203816425655574\n",
            "what is the loss test 0.46535879251261875\n",
            "************************************************************** 139\n",
            "what is the tr accuracy:  0.9878\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.039939729958010435\n",
            "what is the loss v  0.42229635650850106\n",
            "what is the loss test 0.4681694174799715\n",
            "************************************************************** 139\n",
            "what is the tr accuracy:  0.9878\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.039939729958010435\n",
            "what is the loss v  0.42229635650850106\n",
            "what is the loss test 0.4681694174799715\n",
            "************************************************************** 140\n",
            "what is the tr accuracy:  0.9886666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.03842580602485541\n",
            "what is the loss v  0.42275631239239436\n",
            "what is the loss test 0.4697229754587667\n",
            "************************************************************** 140\n",
            "what is the tr accuracy:  0.9886666666666667\n",
            "what is the v accuracy:  0.925\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.03842580602485541\n",
            "what is the loss v  0.42275631239239436\n",
            "what is the loss test 0.4697229754587667\n",
            "************************************************************** 141\n",
            "what is the tr accuracy:  0.989\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.03698078864896173\n",
            "what is the loss v  0.42232737512224683\n",
            "what is the loss test 0.4702141728417883\n",
            "************************************************************** 141\n",
            "what is the tr accuracy:  0.989\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9148311306901615\n",
            "what is the loss  0.03698078864896173\n",
            "what is the loss v  0.42232737512224683\n",
            "what is the loss test 0.4702141728417883\n",
            "************************************************************** 142\n",
            "what is the tr accuracy:  0.9896\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.03571329253871926\n",
            "what is the loss v  0.4214182342275743\n",
            "what is the loss test 0.4699116515718617\n",
            "************************************************************** 142\n",
            "what is the tr accuracy:  0.9896\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.03571329253871926\n",
            "what is the loss v  0.4214182342275743\n",
            "what is the loss test 0.4699116515718617\n",
            "************************************************************** 143\n",
            "what is the tr accuracy:  0.9900666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9151982378854625\n",
            "what is the loss  0.034676392901226365\n",
            "what is the loss v  0.42064684013793163\n",
            "what is the loss test 0.4694624143050675\n",
            "************************************************************** 143\n",
            "what is the tr accuracy:  0.9900666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9151982378854625\n",
            "what is the loss  0.034676392901226365\n",
            "what is the loss v  0.42064684013793163\n",
            "what is the loss test 0.4694624143050675\n",
            "************************************************************** 144\n",
            "what is the tr accuracy:  0.99\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.0338895160109804\n",
            "what is the loss v  0.42058875016464115\n",
            "what is the loss test 0.46954186442023826\n",
            "************************************************************** 144\n",
            "what is the tr accuracy:  0.99\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.0338895160109804\n",
            "what is the loss v  0.42058875016464115\n",
            "what is the loss test 0.46954186442023826\n",
            "************************************************************** 145\n",
            "what is the tr accuracy:  0.9904666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.03333909834927996\n",
            "what is the loss v  0.42152091893383214\n",
            "what is the loss test 0.4705544595077033\n",
            "************************************************************** 145\n",
            "what is the tr accuracy:  0.9904666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.03333909834927996\n",
            "what is the loss v  0.42152091893383214\n",
            "what is the loss test 0.4705544595077033\n",
            "************************************************************** 146\n",
            "what is the tr accuracy:  0.9905333333333334\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.032922465398806076\n",
            "what is the loss v  0.4234931251919653\n",
            "what is the loss test 0.47262797626852604\n",
            "************************************************************** 146\n",
            "what is the tr accuracy:  0.9905333333333334\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.032922465398806076\n",
            "what is the loss v  0.4234931251919653\n",
            "what is the loss test 0.47262797626852604\n",
            "************************************************************** 147\n",
            "what is the tr accuracy:  0.9904\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.03240025304321366\n",
            "what is the loss v  0.42606016940611585\n",
            "what is the loss test 0.4752923343011105\n",
            "************************************************************** 147\n",
            "what is the tr accuracy:  0.9904\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.03240025304321366\n",
            "what is the loss v  0.42606016940611585\n",
            "what is the loss test 0.4752923343011105\n",
            "************************************************************** 148\n",
            "what is the tr accuracy:  0.9906666666666667\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.031516517675871214\n",
            "what is the loss v  0.42871034711252237\n",
            "what is the loss test 0.4779962334263808\n",
            "************************************************************** 148\n",
            "what is the tr accuracy:  0.9906666666666667\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.031516517675871214\n",
            "what is the loss v  0.42871034711252237\n",
            "what is the loss test 0.4779962334263808\n",
            "************************************************************** 149\n",
            "what is the tr accuracy:  0.9912666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.030206211083367122\n",
            "what is the loss v  0.4313716735760959\n",
            "what is the loss test 0.4804296256072851\n",
            "************************************************************** 149\n",
            "what is the tr accuracy:  0.9912666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.030206211083367122\n",
            "what is the loss v  0.4313716735760959\n",
            "what is the loss test 0.4804296256072851\n",
            "************************************************************** 150\n",
            "what is the tr accuracy:  0.9922666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.028777015623112977\n",
            "what is the loss v  0.4346877543710792\n",
            "what is the loss test 0.48288517548519777\n",
            "************************************************************** 150\n",
            "what is the tr accuracy:  0.9922666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.028777015623112977\n",
            "what is the loss v  0.4346877543710792\n",
            "what is the loss test 0.48288517548519777\n",
            "************************************************************** 151\n",
            "what is the tr accuracy:  0.9924666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9192364170337739\n",
            "what is the loss  0.027636143708745495\n",
            "what is the loss v  0.4387506550257163\n",
            "what is the loss test 0.48586619253656055\n",
            "************************************************************** 151\n",
            "what is the tr accuracy:  0.9924666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9192364170337739\n",
            "what is the loss  0.027636143708745495\n",
            "what is the loss v  0.4387506550257163\n",
            "what is the loss test 0.48586619253656055\n",
            "************************************************************** 152\n",
            "what is the tr accuracy:  0.9926\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9185022026431718\n",
            "what is the loss  0.02696114561510384\n",
            "what is the loss v  0.4439271073470781\n",
            "what is the loss test 0.48957793060055915\n",
            "************************************************************** 152\n",
            "what is the tr accuracy:  0.9926\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9185022026431718\n",
            "what is the loss  0.02696114561510384\n",
            "what is the loss v  0.4439271073470781\n",
            "what is the loss test 0.48957793060055915\n",
            "************************************************************** 153\n",
            "what is the tr accuracy:  0.9926\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9192364170337739\n",
            "what is the loss  0.026641732467957167\n",
            "what is the loss v  0.45028612861234946\n",
            "what is the loss test 0.49382519537533437\n",
            "************************************************************** 153\n",
            "what is the tr accuracy:  0.9926\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9192364170337739\n",
            "what is the loss  0.026641732467957167\n",
            "what is the loss v  0.45028612861234946\n",
            "what is the loss test 0.49382519537533437\n",
            "************************************************************** 154\n",
            "what is the tr accuracy:  0.9926666666666667\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9188693098384728\n",
            "what is the loss  0.02638688063090125\n",
            "what is the loss v  0.4572865550886911\n",
            "what is the loss test 0.49822364420544324\n",
            "************************************************************** 154\n",
            "what is the tr accuracy:  0.9926666666666667\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9188693098384728\n",
            "what is the loss  0.02638688063090125\n",
            "what is the loss v  0.4572865550886911\n",
            "what is the loss test 0.49822364420544324\n",
            "************************************************************** 155\n",
            "what is the tr accuracy:  0.9928666666666667\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9185022026431718\n",
            "what is the loss  0.025925766089298957\n",
            "what is the loss v  0.46444968053500446\n",
            "what is the loss test 0.502319494475683\n",
            "************************************************************** 155\n",
            "what is the tr accuracy:  0.9928666666666667\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9185022026431718\n",
            "what is the loss  0.025925766089298957\n",
            "what is the loss v  0.46444968053500446\n",
            "what is the loss test 0.502319494475683\n",
            "************************************************************** 156\n",
            "what is the tr accuracy:  0.9932\n",
            "what is the v accuracy:  0.933\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.025166391742410917\n",
            "what is the loss v  0.471313543437305\n",
            "what is the loss test 0.5058569251052704\n",
            "************************************************************** 156\n",
            "what is the tr accuracy:  0.9932\n",
            "what is the v accuracy:  0.933\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.025166391742410917\n",
            "what is the loss v  0.471313543437305\n",
            "what is the loss test 0.5058569251052704\n",
            "************************************************************** 157\n",
            "what is the tr accuracy:  0.9936666666666667\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9196035242290749\n",
            "what is the loss  0.024235048604071933\n",
            "what is the loss v  0.4777456922643319\n",
            "what is the loss test 0.5088890020747951\n",
            "************************************************************** 157\n",
            "what is the tr accuracy:  0.9936666666666667\n",
            "what is the v accuracy:  0.932\n",
            "what is the t accuracy:  0.9196035242290749\n",
            "what is the loss  0.024235048604071933\n",
            "what is the loss v  0.4777456922643319\n",
            "what is the loss test 0.5088890020747951\n",
            "************************************************************** 158\n",
            "what is the tr accuracy:  0.9938\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.02334952778073936\n",
            "what is the loss v  0.4838126654386772\n",
            "what is the loss test 0.5117320197204401\n",
            "************************************************************** 158\n",
            "what is the tr accuracy:  0.9938\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.02334952778073936\n",
            "what is the loss v  0.4838126654386772\n",
            "what is the loss test 0.5117320197204401\n",
            "************************************************************** 159\n",
            "what is the tr accuracy:  0.9941333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9203377386196769\n",
            "what is the loss  0.022628196950957167\n",
            "what is the loss v  0.48910848714918903\n",
            "what is the loss test 0.5143695266106444\n",
            "************************************************************** 159\n",
            "what is the tr accuracy:  0.9941333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9203377386196769\n",
            "what is the loss  0.022628196950957167\n",
            "what is the loss v  0.48910848714918903\n",
            "what is the loss test 0.5143695266106444\n",
            "************************************************************** 160\n",
            "what is the tr accuracy:  0.9942\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.022086480858319657\n",
            "what is the loss v  0.4935593145533321\n",
            "what is the loss test 0.5167621382733094\n",
            "************************************************************** 160\n",
            "what is the tr accuracy:  0.9942\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.022086480858319657\n",
            "what is the loss v  0.4935593145533321\n",
            "what is the loss test 0.5167621382733094\n",
            "************************************************************** 161\n",
            "what is the tr accuracy:  0.9945333333333334\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.02161055955180839\n",
            "what is the loss v  0.4971532093344697\n",
            "what is the loss test 0.5189893051090783\n",
            "************************************************************** 161\n",
            "what is the tr accuracy:  0.9945333333333334\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.02161055955180839\n",
            "what is the loss v  0.4971532093344697\n",
            "what is the loss test 0.5189893051090783\n",
            "************************************************************** 162\n",
            "what is the tr accuracy:  0.9946666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.021138027317253883\n",
            "what is the loss v  0.49970585844479837\n",
            "what is the loss test 0.5208374957329227\n",
            "************************************************************** 162\n",
            "what is the tr accuracy:  0.9946666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.021138027317253883\n",
            "what is the loss v  0.49970585844479837\n",
            "what is the loss test 0.5208374957329227\n",
            "************************************************************** 163\n",
            "what is the tr accuracy:  0.995\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.020638584673812167\n",
            "what is the loss v  0.5014947084102573\n",
            "what is the loss test 0.5224399413943175\n",
            "************************************************************** 163\n",
            "what is the tr accuracy:  0.995\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.020638584673812167\n",
            "what is the loss v  0.5014947084102573\n",
            "what is the loss test 0.5224399413943175\n",
            "************************************************************** 164\n",
            "what is the tr accuracy:  0.9948666666666667\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.020157531169661774\n",
            "what is the loss v  0.5027511002404519\n",
            "what is the loss test 0.5241325727440231\n",
            "************************************************************** 164\n",
            "what is the tr accuracy:  0.9948666666666667\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.020157531169661774\n",
            "what is the loss v  0.5027511002404519\n",
            "what is the loss test 0.5241325727440231\n",
            "************************************************************** 165\n",
            "what is the tr accuracy:  0.9950666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9188693098384728\n",
            "what is the loss  0.019736426105005755\n",
            "what is the loss v  0.5038694845101512\n",
            "what is the loss test 0.5262312548767012\n",
            "************************************************************** 165\n",
            "what is the tr accuracy:  0.9950666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9188693098384728\n",
            "what is the loss  0.019736426105005755\n",
            "what is the loss v  0.5038694845101512\n",
            "what is the loss test 0.5262312548767012\n",
            "************************************************************** 166\n",
            "what is the tr accuracy:  0.9953333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9192364170337739\n",
            "what is the loss  0.019378846361366397\n",
            "what is the loss v  0.5051932214878426\n",
            "what is the loss test 0.528862936140945\n",
            "************************************************************** 166\n",
            "what is the tr accuracy:  0.9953333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9192364170337739\n",
            "what is the loss  0.019378846361366397\n",
            "what is the loss v  0.5051932214878426\n",
            "what is the loss test 0.528862936140945\n",
            "************************************************************** 167\n",
            "what is the tr accuracy:  0.9954\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.019008938642306072\n",
            "what is the loss v  0.5068440061198526\n",
            "what is the loss test 0.5319464821596441\n",
            "************************************************************** 167\n",
            "what is the tr accuracy:  0.9954\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.019008938642306072\n",
            "what is the loss v  0.5068440061198526\n",
            "what is the loss test 0.5319464821596441\n",
            "************************************************************** 168\n",
            "what is the tr accuracy:  0.9954666666666667\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.921071953010279\n",
            "what is the loss  0.0185395325432899\n",
            "what is the loss v  0.5088082770905898\n",
            "what is the loss test 0.5353481330012894\n",
            "************************************************************** 168\n",
            "what is the tr accuracy:  0.9954666666666667\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.921071953010279\n",
            "what is the loss  0.0185395325432899\n",
            "what is the loss v  0.5088082770905898\n",
            "what is the loss test 0.5353481330012894\n",
            "************************************************************** 169\n",
            "what is the tr accuracy:  0.9958\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.921071953010279\n",
            "what is the loss  0.017967502501672887\n",
            "what is the loss v  0.5109817876577062\n",
            "what is the loss test 0.5390029012910713\n",
            "************************************************************** 169\n",
            "what is the tr accuracy:  0.9958\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.921071953010279\n",
            "what is the loss  0.017967502501672887\n",
            "what is the loss v  0.5109817876577062\n",
            "what is the loss test 0.5390029012910713\n",
            "************************************************************** 170\n",
            "what is the tr accuracy:  0.9958\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.017352677589199682\n",
            "what is the loss v  0.5134398691859444\n",
            "what is the loss test 0.5427392126537779\n",
            "************************************************************** 170\n",
            "what is the tr accuracy:  0.9958\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.920704845814978\n",
            "what is the loss  0.017352677589199682\n",
            "what is the loss v  0.5134398691859444\n",
            "what is the loss test 0.5427392126537779\n",
            "************************************************************** 171\n",
            "what is the tr accuracy:  0.9960666666666667\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9203377386196769\n",
            "what is the loss  0.01678845012512944\n",
            "what is the loss v  0.516226852426452\n",
            "what is the loss test 0.5466224736170456\n",
            "************************************************************** 171\n",
            "what is the tr accuracy:  0.9960666666666667\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9203377386196769\n",
            "what is the loss  0.01678845012512944\n",
            "what is the loss v  0.516226852426452\n",
            "what is the loss test 0.5466224736170456\n",
            "************************************************************** 172\n",
            "what is the tr accuracy:  0.9958\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9203377386196769\n",
            "what is the loss  0.01631437872537101\n",
            "what is the loss v  0.5192719098916798\n",
            "what is the loss test 0.5505568115284383\n",
            "************************************************************** 172\n",
            "what is the tr accuracy:  0.9958\n",
            "what is the v accuracy:  0.931\n",
            "what is the t accuracy:  0.9203377386196769\n",
            "what is the loss  0.01631437872537101\n",
            "what is the loss v  0.5192719098916798\n",
            "what is the loss test 0.5505568115284383\n",
            "************************************************************** 173\n",
            "what is the tr accuracy:  0.996\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9196035242290749\n",
            "what is the loss  0.015924478664136675\n",
            "what is the loss v  0.5224086241236272\n",
            "what is the loss test 0.5544126041211948\n",
            "************************************************************** 173\n",
            "what is the tr accuracy:  0.996\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.9196035242290749\n",
            "what is the loss  0.015924478664136675\n",
            "what is the loss v  0.5224086241236272\n",
            "what is the loss test 0.5544126041211948\n",
            "************************************************************** 174\n",
            "what is the tr accuracy:  0.9959333333333333\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.01558755278707535\n",
            "what is the loss v  0.5256583331297447\n",
            "what is the loss test 0.5579938428232684\n",
            "************************************************************** 174\n",
            "what is the tr accuracy:  0.9959333333333333\n",
            "what is the v accuracy:  0.93\n",
            "what is the t accuracy:  0.919970631424376\n",
            "what is the loss  0.01558755278707535\n",
            "what is the loss v  0.5256583331297447\n",
            "what is the loss test 0.5579938428232684\n",
            "************************************************************** 175\n",
            "what is the tr accuracy:  0.9959333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.01527130045520047\n",
            "what is the loss v  0.528937815747919\n",
            "what is the loss test 0.5614250358998407\n",
            "************************************************************** 175\n",
            "what is the tr accuracy:  0.9959333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9181350954478708\n",
            "what is the loss  0.01527130045520047\n",
            "what is the loss v  0.528937815747919\n",
            "what is the loss test 0.5614250358998407\n",
            "************************************************************** 176\n",
            "what is the tr accuracy:  0.9960666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.014967144529392174\n",
            "what is the loss v  0.5323511058578846\n",
            "what is the loss test 0.5646230109283624\n",
            "************************************************************** 176\n",
            "what is the tr accuracy:  0.9960666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.014967144529392174\n",
            "what is the loss v  0.5323511058578846\n",
            "what is the loss test 0.5646230109283624\n",
            "************************************************************** 177\n",
            "what is the tr accuracy:  0.9961333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.014696380751095334\n",
            "what is the loss v  0.5360655785126933\n",
            "what is the loss test 0.5677207684621847\n",
            "************************************************************** 177\n",
            "what is the tr accuracy:  0.9961333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.014696380751095334\n",
            "what is the loss v  0.5360655785126933\n",
            "what is the loss test 0.5677207684621847\n",
            "************************************************************** 178\n",
            "what is the tr accuracy:  0.996\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.014478806462887782\n",
            "what is the loss v  0.5403014630117459\n",
            "what is the loss test 0.570799037298121\n",
            "************************************************************** 178\n",
            "what is the tr accuracy:  0.996\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.014478806462887782\n",
            "what is the loss v  0.5403014630117459\n",
            "what is the loss test 0.570799037298121\n",
            "************************************************************** 179\n",
            "what is the tr accuracy:  0.9962\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.014284050651120444\n",
            "what is the loss v  0.5450945061163407\n",
            "what is the loss test 0.5738530658850951\n",
            "************************************************************** 179\n",
            "what is the tr accuracy:  0.9962\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.014284050651120444\n",
            "what is the loss v  0.5450945061163407\n",
            "what is the loss test 0.5738530658850951\n",
            "************************************************************** 180\n",
            "what is the tr accuracy:  0.9962\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.014056246535343469\n",
            "what is the loss v  0.5500595196642217\n",
            "what is the loss test 0.5768358256555397\n",
            "************************************************************** 180\n",
            "what is the tr accuracy:  0.9962\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.014056246535343469\n",
            "what is the loss v  0.5500595196642217\n",
            "what is the loss test 0.5768358256555397\n",
            "************************************************************** 181\n",
            "what is the tr accuracy:  0.9961333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.013741219358648152\n",
            "what is the loss v  0.5550238518317984\n",
            "what is the loss test 0.5795795658839942\n",
            "************************************************************** 181\n",
            "what is the tr accuracy:  0.9961333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.013741219358648152\n",
            "what is the loss v  0.5550238518317984\n",
            "what is the loss test 0.5795795658839942\n",
            "************************************************************** 182\n",
            "what is the tr accuracy:  0.9964\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.013337814925702412\n",
            "what is the loss v  0.5599224976268175\n",
            "what is the loss test 0.5821268793336937\n",
            "************************************************************** 182\n",
            "what is the tr accuracy:  0.9964\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.013337814925702412\n",
            "what is the loss v  0.5599224976268175\n",
            "what is the loss test 0.5821268793336937\n",
            "************************************************************** 183\n",
            "what is the tr accuracy:  0.9963333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.012920984983546881\n",
            "what is the loss v  0.564699151487154\n",
            "what is the loss test 0.5846605917650309\n",
            "************************************************************** 183\n",
            "what is the tr accuracy:  0.9963333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.012920984983546881\n",
            "what is the loss v  0.564699151487154\n",
            "what is the loss test 0.5846605917650309\n",
            "************************************************************** 184\n",
            "what is the tr accuracy:  0.9966666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.012570855764750161\n",
            "what is the loss v  0.5692878037645279\n",
            "what is the loss test 0.5872932835394313\n",
            "************************************************************** 184\n",
            "what is the tr accuracy:  0.9966666666666667\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.012570855764750161\n",
            "what is the loss v  0.5692878037645279\n",
            "what is the loss test 0.5872932835394313\n",
            "************************************************************** 185\n",
            "what is the tr accuracy:  0.997\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.012274787434793768\n",
            "what is the loss v  0.5735167111909345\n",
            "what is the loss test 0.5900321752953838\n",
            "************************************************************** 185\n",
            "what is the tr accuracy:  0.997\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.012274787434793768\n",
            "what is the loss v  0.5735167111909345\n",
            "what is the loss test 0.5900321752953838\n",
            "************************************************************** 186\n",
            "what is the tr accuracy:  0.997\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.011988817617617073\n",
            "what is the loss v  0.5772167977852366\n",
            "what is the loss test 0.5927743798917445\n",
            "************************************************************** 186\n",
            "what is the tr accuracy:  0.997\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.011988817617617073\n",
            "what is the loss v  0.5772167977852366\n",
            "what is the loss test 0.5927743798917445\n",
            "************************************************************** 187\n",
            "what is the tr accuracy:  0.997\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.011677907729698296\n",
            "what is the loss v  0.5804673969077223\n",
            "what is the loss test 0.5955035672919536\n",
            "************************************************************** 187\n",
            "what is the tr accuracy:  0.997\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.011677907729698296\n",
            "what is the loss v  0.5804673969077223\n",
            "what is the loss test 0.5955035672919536\n",
            "************************************************************** 188\n",
            "what is the tr accuracy:  0.9969333333333333\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.011366573845059808\n",
            "what is the loss v  0.5834261312711501\n",
            "what is the loss test 0.5982734447901977\n",
            "************************************************************** 188\n",
            "what is the tr accuracy:  0.9969333333333333\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.011366573845059808\n",
            "what is the loss v  0.5834261312711501\n",
            "what is the loss test 0.5982734447901977\n",
            "************************************************************** 189\n",
            "what is the tr accuracy:  0.9966666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.011115296717742421\n",
            "what is the loss v  0.5863355734315282\n",
            "what is the loss test 0.6013420785379215\n",
            "************************************************************** 189\n",
            "what is the tr accuracy:  0.9966666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.011115296717742421\n",
            "what is the loss v  0.5863355734315282\n",
            "what is the loss test 0.6013420785379215\n",
            "************************************************************** 190\n",
            "what is the tr accuracy:  0.9968666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.010949040056213478\n",
            "what is the loss v  0.58933493189967\n",
            "what is the loss test 0.6049175840565274\n",
            "************************************************************** 190\n",
            "what is the tr accuracy:  0.9968666666666667\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.010949040056213478\n",
            "what is the loss v  0.58933493189967\n",
            "what is the loss test 0.6049175840565274\n",
            "************************************************************** 191\n",
            "what is the tr accuracy:  0.9971333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.010823428784604036\n",
            "what is the loss v  0.5926268353564187\n",
            "what is the loss test 0.6090043843947196\n",
            "************************************************************** 191\n",
            "what is the tr accuracy:  0.9971333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.010823428784604036\n",
            "what is the loss v  0.5926268353564187\n",
            "what is the loss test 0.6090043843947196\n",
            "************************************************************** 192\n",
            "what is the tr accuracy:  0.9971333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.0106618378223234\n",
            "what is the loss v  0.5963082176872172\n",
            "what is the loss test 0.6134298495324079\n",
            "************************************************************** 192\n",
            "what is the tr accuracy:  0.9971333333333333\n",
            "what is the v accuracy:  0.929\n",
            "what is the t accuracy:  0.9159324522760646\n",
            "what is the loss  0.0106618378223234\n",
            "what is the loss v  0.5963082176872172\n",
            "what is the loss test 0.6134298495324079\n",
            "************************************************************** 193\n",
            "what is the tr accuracy:  0.9972666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.010394421982297833\n",
            "what is the loss v  0.6003251281324563\n",
            "what is the loss test 0.6180417747779006\n",
            "************************************************************** 193\n",
            "what is the tr accuracy:  0.9972666666666666\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.010394421982297833\n",
            "what is the loss v  0.6003251281324563\n",
            "what is the loss test 0.6180417747779006\n",
            "************************************************************** 194\n",
            "what is the tr accuracy:  0.9973333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.010034755592648428\n",
            "what is the loss v  0.6047782226425713\n",
            "what is the loss test 0.6228098391854684\n",
            "************************************************************** 194\n",
            "what is the tr accuracy:  0.9973333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9155653450807636\n",
            "what is the loss  0.010034755592648428\n",
            "what is the loss v  0.6047782226425713\n",
            "what is the loss test 0.6228098391854684\n",
            "************************************************************** 195\n",
            "what is the tr accuracy:  0.9978\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.009667865940450942\n",
            "what is the loss v  0.6094100619169492\n",
            "what is the loss test 0.6277129776465027\n",
            "************************************************************** 195\n",
            "what is the tr accuracy:  0.9978\n",
            "what is the v accuracy:  0.926\n",
            "what is the t accuracy:  0.9162995594713657\n",
            "what is the loss  0.009667865940450942\n",
            "what is the loss v  0.6094100619169492\n",
            "what is the loss test 0.6277129776465027\n",
            "************************************************************** 196\n",
            "what is the tr accuracy:  0.9978666666666667\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.009392017136967577\n",
            "what is the loss v  0.6143842263265433\n",
            "what is the loss test 0.6327692831249115\n",
            "************************************************************** 196\n",
            "what is the tr accuracy:  0.9978666666666667\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9166666666666666\n",
            "what is the loss  0.009392017136967577\n",
            "what is the loss v  0.6143842263265433\n",
            "what is the loss test 0.6327692831249115\n",
            "************************************************************** 197\n",
            "what is the tr accuracy:  0.9978\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.009242728999423662\n",
            "what is the loss v  0.6195673303659082\n",
            "what is the loss test 0.6378073573707587\n",
            "************************************************************** 197\n",
            "what is the tr accuracy:  0.9978\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9177679882525698\n",
            "what is the loss  0.009242728999423662\n",
            "what is the loss v  0.6195673303659082\n",
            "what is the loss test 0.6378073573707587\n",
            "************************************************************** 198\n",
            "what is the tr accuracy:  0.9979333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.009167852491703475\n",
            "what is the loss v  0.6246501582417288\n",
            "what is the loss test 0.6426706337643128\n",
            "************************************************************** 198\n",
            "what is the tr accuracy:  0.9979333333333333\n",
            "what is the v accuracy:  0.928\n",
            "what is the t accuracy:  0.9170337738619677\n",
            "what is the loss  0.009167852491703475\n",
            "what is the loss v  0.6246501582417288\n",
            "what is the loss test 0.6426706337643128\n",
            "************************************************************** 199\n",
            "what is the tr accuracy:  0.998\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.009087733988024004\n",
            "what is the loss v  0.6292624738079005\n",
            "what is the loss test 0.6470422068858057\n",
            "************************************************************** 199\n",
            "what is the tr accuracy:  0.998\n",
            "what is the v accuracy:  0.927\n",
            "what is the t accuracy:  0.9174008810572687\n",
            "what is the loss  0.009087733988024004\n",
            "what is the loss v  0.6292624738079005\n",
            "what is the loss test 0.6470422068858057\n",
            "min loss is 0.352814418806089 at epoch 59\n",
            "min loss is 0.352814418806089 at epoch 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xOY8ncgPIKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateTestPlot(x_val, y_val_train, y_val_valid, y_val_test, title, xlabel, ylabel, yupper):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "   \n",
        "    plt.plot(x_val,y_val_train, label=\"100 hidden units\")\n",
        "    plt.plot(x_val,y_val_valid, label=\"500 hidden units\")\n",
        "    plt.plot(x_val,y_val_test, label=\"2000 hidden units\")\n",
        "    plt.ylim(0, yupper)    \n",
        "    plt.legend(loc='bottom right')        \n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0TP-5wlPQHh",
        "colab_type": "code",
        "outputId": "fbb3228e-1aee-438f-a49e-ffa42dab405a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        }
      },
      "source": [
        "generateTestPlot(range(200), acc_test_100, acc_test_500, acc_test_2000, \"Test accuracy for N hidden units\", \"Number of epochs\", \"Accuracy\", 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'bottom right'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIau20ivQshW",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Neural Networks in Tensorflow \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jISQ6RC88mKO",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 and 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjkWdzd5zBKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertOneHot(trainTarget, validTarget, testTarget):\n",
        "  newtrain = np.zeros((trainTarget.shape[0], 10))\n",
        "  newvalid = np.zeros((validTarget.shape[0], 10))\n",
        "  newtest = np.zeros((testTarget.shape[0], 10))\n",
        "  for item in range(0, trainTarget.shape[0]):\n",
        "    newtrain[item][trainTarget[item]] = 1\n",
        "  for item in range(0, validTarget.shape[0]):\n",
        "    newvalid[item][validTarget[item]] = 1\n",
        "  for item in range(0, testTarget.shape[0]):\n",
        "    newtest[item][testTarget[item]] = 1\n",
        "  return newtrain, newvalid, newtest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqE_R4lQzCoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY, validY, testY = convertOneHot(trainY, validY, testY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIt71YBJzPT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = np.reshape(trainX, (-1, 28, 28, 1))\n",
        "validX = np.reshape(validX, (-1, 28, 28, 1))\n",
        "testX = np.reshape(testX, (-1, 28, 28, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oFl2y-oEvcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_data, testLoss):\n",
        "        self.test_data = test_data\n",
        "        self.testLoss = testLoss\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "        self.testLoss.append((epoch, loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNDa-D_yEyKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## KERAS\n",
        "def buildModel(beta, dropout_rate):\n",
        "    model = tf.keras.Sequential()\n",
        "    glorot_normal_init = tf.keras.initializers.glorot_normal(seed=421)\n",
        "          \n",
        "    # Conv Layer + Relu\n",
        "    conv_layer = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same',\n",
        "                                        kernel_initializer=glorot_normal_init, activation='relu')\n",
        "    model.add(conv_layer)\n",
        "\n",
        "    # Batch Normalizatoin\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    \n",
        "    # Max Pooling\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # FC1\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(784, kernel_initializer=glorot_normal_init, \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(beta)))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "\n",
        "    # FC2\n",
        "    model.add(tf.keras.layers.Dense(10, kernel_initializer=glorot_normal_init, \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(beta), activation='softmax'))\n",
        "\n",
        "    #LOSS\n",
        "    adam= tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQCNFS-pE0Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(beta, dropout_rate):\n",
        "  testLoss = []\n",
        "  model = buildModel(beta, dropout_rate)\n",
        "  model_history = model.fit(trainX, trainY, 32, 50, shuffle=True, validation_data=(validX, validY), callbacks=[TestCallback((testX, testY), testLoss)])\n",
        "  return model_history, testLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16jG6qUHE2aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_plot(history, testLoss):\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  testAcc = [testData[2] for testData in testLoss]\n",
        "  plt.plot(testAcc)\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  testLoss = [testData[1] for testData in testLoss]\n",
        "  plt.plot(testLoss)\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation', 'test'], loc='upper left')\n",
        "  plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xxR2R-NE4NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0,0)\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNEVix-cFC04",
        "colab_type": "text"
      },
      "source": [
        "## 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2guK4Mb78oZ",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1 L2 *Regularization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3EMg5BJE-dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0.01, 0) # reg = 0.01\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP3Fqx1cFGt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0.1, 0) # reg = 0.1\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PORXV0EFHWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0.5, 0) # reg = 0.5\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OufDNYcY8BTp",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2 Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMpH8VMjFJRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0, 0.9) # dropout_rate = 0.9\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U81GB_j4FLv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0, 0.75) # dropout_rate = 0.75\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B42_t1NHFOA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history, testLoss = train(0, 0.5) # dropout_rate = 0.5\n",
        "generate_plot(history, testLoss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoaVp05fFPtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}